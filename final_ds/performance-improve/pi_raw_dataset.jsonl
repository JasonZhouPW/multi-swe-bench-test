{"org": "syncthing", "repo": "syncthing", "number": 10318, "state": "closed", "title": "refactor(db): slightly improve insert performance", "body": "This just removes an unnecessary foreign key constraint, where we already do the garbage collection manually in the database service. However, as part of getting here I tried a couple of other variants along the way:\r\n\r\n- Changing the order of the primary key from `(hash, blocklist_hash, idx)` to `(blocklist_hash, idx, hash)` so that inserts would be naturally ordered. However this requires a new index `on blocks (hash)` so that we can still look up blocks by hash, and turns out to be strictly worse than what we already have.\r\n- Removing the primary key entirely and the `WITHOUT ROWID` to make it a rowid table without any required order, and an index as above. This is faster when the table is small, but becomes slower when it's large (due to dual indexes I guess).\r\n\r\nThese are the benchmark results from current `main`, the second alternative below (\"Index(hash)\") and this proposal that retains the combined primary key (\"combined\"). Overall it ends up being about 65% faster.\r\n\r\n<img width=\"764\" height=\"452\" alt=\"Screenshot 2025-08-29 at 14 36 28\" src=\"https://github.com/user-attachments/assets/bff3f9d1-916a-485f-91b7-b54b477f5aac\" />\r\n\r\nRef #10264 ", "url": "https://api.github.com/repos/syncthing/syncthing/pulls/10318", "id": 2784775822, "node_id": "PR_kwDOAOCAEs6l_E6O", "html_url": "https://github.com/syncthing/syncthing/pull/10318", "diff_url": "https://github.com/syncthing/syncthing/pull/10318.diff", "patch_url": "https://github.com/syncthing/syncthing/pull/10318.patch", "issue_url": "https://api.github.com/repos/syncthing/syncthing/issues/10318", "created_at": "2025-08-29T12:37:02+00:00", "updated_at": "2025-08-31T11:19:37+00:00", "closed_at": "2025-08-29T13:26:24+00:00", "merged_at": "2025-08-29T13:26:24+00:00", "merge_commit_sha": "c918299eab1a8fae2299bb64bb3cf471f2defb5f", "labels": ["chore"], "draft": false, "commits_url": "https://api.github.com/repos/syncthing/syncthing/pulls/10318/commits", "review_comments_url": "https://api.github.com/repos/syncthing/syncthing/pulls/10318/comments", "review_comment_url": "https://api.github.com/repos/syncthing/syncthing/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/syncthing/syncthing/issues/10318/comments", "base": {"label": "syncthing:main", "ref": "main", "sha": "b59443f1360a756ea70d52f07b86eff47ed82671", "user": {"login": "syncthing", "id": 7628018, "node_id": "MDEyOk9yZ2FuaXphdGlvbjc2MjgwMTg=", "avatar_url": "https://avatars.githubusercontent.com/u/7628018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syncthing", "html_url": "https://github.com/syncthing", "followers_url": "https://api.github.com/users/syncthing/followers", "following_url": "https://api.github.com/users/syncthing/following{/other_user}", "gists_url": "https://api.github.com/users/syncthing/gists{/gist_id}", "starred_url": "https://api.github.com/users/syncthing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syncthing/subscriptions", "organizations_url": "https://api.github.com/users/syncthing/orgs", "repos_url": "https://api.github.com/users/syncthing/repos", "events_url": "https://api.github.com/users/syncthing/events{/privacy}", "received_events_url": "https://api.github.com/users/syncthing/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 14712850, "node_id": "MDEwOlJlcG9zaXRvcnkxNDcxMjg1MA==", "name": "syncthing", "full_name": "syncthing/syncthing", "private": false, "owner": {"login": "syncthing", "id": 7628018, "node_id": "MDEyOk9yZ2FuaXphdGlvbjc2MjgwMTg=", "avatar_url": "https://avatars.githubusercontent.com/u/7628018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syncthing", "html_url": "https://github.com/syncthing", "followers_url": "https://api.github.com/users/syncthing/followers", "following_url": "https://api.github.com/users/syncthing/following{/other_user}", "gists_url": "https://api.github.com/users/syncthing/gists{/gist_id}", "starred_url": "https://api.github.com/users/syncthing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syncthing/subscriptions", "organizations_url": "https://api.github.com/users/syncthing/orgs", "repos_url": "https://api.github.com/users/syncthing/repos", "events_url": "https://api.github.com/users/syncthing/events{/privacy}", "received_events_url": "https://api.github.com/users/syncthing/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/syncthing/syncthing", "description": "Open Source Continuous File Synchronization", "fork": false, "url": "https://api.github.com/repos/syncthing/syncthing", "forks_url": "https://api.github.com/repos/syncthing/syncthing/forks", "keys_url": "https://api.github.com/repos/syncthing/syncthing/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/syncthing/syncthing/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/syncthing/syncthing/teams", "hooks_url": "https://api.github.com/repos/syncthing/syncthing/hooks", "issue_events_url": "https://api.github.com/repos/syncthing/syncthing/issues/events{/number}", "events_url": "https://api.github.com/repos/syncthing/syncthing/events", "assignees_url": "https://api.github.com/repos/syncthing/syncthing/assignees{/user}", "branches_url": "https://api.github.com/repos/syncthing/syncthing/branches{/branch}", "tags_url": "https://api.github.com/repos/syncthing/syncthing/tags", "blobs_url": "https://api.github.com/repos/syncthing/syncthing/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/syncthing/syncthing/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/syncthing/syncthing/git/refs{/sha}", "trees_url": "https://api.github.com/repos/syncthing/syncthing/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/syncthing/syncthing/statuses/{sha}", "languages_url": "https://api.github.com/repos/syncthing/syncthing/languages", "stargazers_url": "https://api.github.com/repos/syncthing/syncthing/stargazers", "contributors_url": "https://api.github.com/repos/syncthing/syncthing/contributors", "subscribers_url": "https://api.github.com/repos/syncthing/syncthing/subscribers", "subscription_url": "https://api.github.com/repos/syncthing/syncthing/subscription", "commits_url": "https://api.github.com/repos/syncthing/syncthing/commits{/sha}", "git_commits_url": "https://api.github.com/repos/syncthing/syncthing/git/commits{/sha}", "comments_url": "https://api.github.com/repos/syncthing/syncthing/comments{/number}", "issue_comment_url": "https://api.github.com/repos/syncthing/syncthing/issues/comments{/number}", "contents_url": "https://api.github.com/repos/syncthing/syncthing/contents/{+path}", "compare_url": "https://api.github.com/repos/syncthing/syncthing/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/syncthing/syncthing/merges", "archive_url": "https://api.github.com/repos/syncthing/syncthing/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/syncthing/syncthing/downloads", "issues_url": "https://api.github.com/repos/syncthing/syncthing/issues{/number}", "pulls_url": "https://api.github.com/repos/syncthing/syncthing/pulls{/number}", "milestones_url": "https://api.github.com/repos/syncthing/syncthing/milestones{/number}", "notifications_url": "https://api.github.com/repos/syncthing/syncthing/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/syncthing/syncthing/labels{/name}", "releases_url": "https://api.github.com/repos/syncthing/syncthing/releases{/id}", "deployments_url": "https://api.github.com/repos/syncthing/syncthing/deployments", "created_at": "2013-11-26T09:48:21Z", "updated_at": "2026-01-07T08:57:15Z", "pushed_at": "2026-01-06T11:43:53Z", "git_url": "git://github.com/syncthing/syncthing.git", "ssh_url": "git@github.com:syncthing/syncthing.git", "clone_url": "https://github.com/syncthing/syncthing.git", "svn_url": "https://github.com/syncthing/syncthing", "homepage": "https://syncthing.net/", "size": 124231, "stargazers_count": 78824, "watchers_count": 78824, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 4877, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 387, "license": {"key": "mpl-2.0", "name": "Mozilla Public License 2.0", "spdx_id": "MPL-2.0", "url": "https://api.github.com/licenses/mpl-2.0", "node_id": "MDc6TGljZW5zZTE0"}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": true, "topics": ["go", "p2p", "peer-to-peer", "synchronization"], "visibility": "public", "forks": 4877, "open_issues": 387, "watchers": 78824, "default_branch": "main"}}, "commits": [{"sha": "9dd07d93d3f3a586b7e17bfaae32460ae0530deb", "parents": ["b59443f1360a756ea70d52f07b86eff47ed82671"], "message": "wip"}, {"sha": "01b6f5beed19034e92f4439cc26d95d1921c0822", "parents": ["9dd07d93d3f3a586b7e17bfaae32460ae0530deb"], "message": "wip"}, {"sha": "2dd199cddb183b7b1a7489808cc8f14cea10496f", "parents": ["01b6f5beed19034e92f4439cc26d95d1921c0822"], "message": "wip"}, {"sha": "66862c2fc0d0961edb2469a605dc757eeb85a3c4", "parents": ["2dd199cddb183b7b1a7489808cc8f14cea10496f"], "message": "wip"}, {"sha": "c1fd581a1ff0f15af095fcd3625e2e32380641d4", "parents": ["66862c2fc0d0961edb2469a605dc757eeb85a3c4"], "message": "wip"}, {"sha": "99a934c9e41f8d3356c17edcd1a2a34d9efd4ffc", "parents": ["c1fd581a1ff0f15af095fcd3625e2e32380641d4"], "message": "wip"}], "resolved_issues": [{"org": "syncthing", "repo": "syncthing", "number": -1, "state": "unknown", "title": "refactor(db): slightly improve insert performance", "body": "This just removes an unnecessary foreign key constraint, where we already do the garbage collection manually in the database service. However, as part of getting here I tried a couple of other variants along the way:\r\n\r\n- Changing the order of the primary key from `(hash, blocklist_hash, idx)` to `(blocklist_hash, idx, hash)` so that inserts would be naturally ordered. However this requires a new index `on blocks (hash)` so that we can still look up blocks by hash, and turns out to be strictly worse than what we already have.\r\n- Removing the primary key entirely and the `WITHOUT ROWID` to make it a rowid table without any required order, and an index as above. This is faster when the table is small, but becomes slower when it's large (due to dual indexes I guess).\r\n\r\nThese are the benchmark results from current `main`, the second alternative below (\"Index(hash)\") and this proposal that retains the combined primary key (\"combined\"). Overall it ends up being about 65% faster.\r\n\r\n<img width=\"764\" height=\"452\" alt=\"Screenshot 2025-08-29 at 14 36 28\" src=\"https://github.com/user-attachments/assets/bff3f9d1-916a-485f-91b7-b54b477f5aac\" />\r\n\r\nRef #10264 "}], "fix_patch": "diff --git a/internal/db/sqlite/folderdb_update.go b/internal/db/sqlite/folderdb_update.go\nindex 76f54d1d36e..53ef4456993 100644\n--- a/internal/db/sqlite/folderdb_update.go\n+++ b/internal/db/sqlite/folderdb_update.go\n@@ -114,11 +114,9 @@ func (s *folderDB) Update(device protocol.DeviceID, fs []protocol.FileInfo) erro\n \t\t\tif err != nil {\n \t\t\t\treturn wrap(err, \"marshal blocklist\")\n \t\t\t}\n-\t\t\tif _, err := insertBlockListStmt.Exec(f.BlocksHash, bs); err != nil {\n+\t\t\tif res, err := insertBlockListStmt.Exec(f.BlocksHash, bs); err != nil {\n \t\t\t\treturn wrap(err, \"insert blocklist\")\n-\t\t\t}\n-\n-\t\t\tif device == protocol.LocalDeviceID {\n+\t\t\t} else if aff, _ := res.RowsAffected(); aff != 0 && device == protocol.LocalDeviceID {\n \t\t\t\t// Insert all blocks\n \t\t\t\tif err := s.insertBlocksLocked(txp, f.BlocksHash, f.Blocks); err != nil {\n \t\t\t\t\treturn wrap(err, \"insert blocks\")\ndiff --git a/internal/db/sqlite/sql/schema/folder/50-blocks.sql b/internal/db/sqlite/sql/schema/folder/50-blocks.sql\nindex 3c2ca8bc19d..18c3817fec5 100644\n--- a/internal/db/sqlite/sql/schema/folder/50-blocks.sql\n+++ b/internal/db/sqlite/sql/schema/folder/50-blocks.sql\n@@ -21,14 +21,14 @@ CREATE TABLE IF NOT EXISTS blocklists (\n --\n -- For all local files we store the blocks individually for quick lookup. A\n -- given block can exist in multiple blocklists and at multiple offsets in a\n--- blocklist.\n+-- blocklist. We eschew most indexes here as inserting millions of blocks is\n+-- common and performance is critical.\n CREATE TABLE IF NOT EXISTS blocks (\n     hash BLOB NOT NULL,\n     blocklist_hash BLOB NOT NULL,\n     idx INTEGER NOT NULL,\n     offset INTEGER NOT NULL,\n     size INTEGER NOT NULL,\n-    PRIMARY KEY (hash, blocklist_hash, idx),\n-    FOREIGN KEY(blocklist_hash) REFERENCES blocklists(blocklist_hash) ON DELETE CASCADE DEFERRABLE INITIALLY DEFERRED\n+    PRIMARY KEY(hash, blocklist_hash, idx)\n ) STRICT, WITHOUT ROWID\n ;\n", "test_patch": "diff --git a/internal/db/sqlite/db_bench_test.go b/internal/db/sqlite/db_bench_test.go\nindex 7af6643738f..8478c0cba9e 100644\n--- a/internal/db/sqlite/db_bench_test.go\n+++ b/internal/db/sqlite/db_bench_test.go\n@@ -7,7 +7,6 @@\n package sqlite\n \n import (\n-\t\"context\"\n \t\"fmt\"\n \t\"testing\"\n \t\"time\"\n@@ -30,19 +29,14 @@ func BenchmarkUpdate(b *testing.B) {\n \t\t\tb.Fatal(err)\n \t\t}\n \t})\n-\tsvc := db.Service(time.Hour).(*Service)\n \n \tfs := make([]protocol.FileInfo, 100)\n+\n \tseed := 0\n+\tsize := 1000\n+\tconst numBlocks = 1000\n \n-\tsize := 10000\n \tfor size < 200_000 {\n-\t\tt0 := time.Now()\n-\t\tif err := svc.periodic(context.Background()); err != nil {\n-\t\t\tb.Fatal(err)\n-\t\t}\n-\t\tb.Log(\"garbage collect in\", time.Since(t0))\n-\n \t\tfor {\n \t\t\tlocal, err := db.CountLocal(folderID, protocol.LocalDeviceID)\n \t\t\tif err != nil {\n@@ -53,7 +47,7 @@ func BenchmarkUpdate(b *testing.B) {\n \t\t\t}\n \t\t\tfs := make([]protocol.FileInfo, 1000)\n \t\t\tfor i := range fs {\n-\t\t\t\tfs[i] = genFile(rand.String(24), 64, 0)\n+\t\t\t\tfs[i] = genFile(rand.String(24), numBlocks, 0)\n \t\t\t}\n \t\t\tif err := db.Update(folderID, protocol.LocalDeviceID, fs); err != nil {\n \t\t\t\tb.Fatal(err)\n@@ -63,7 +57,7 @@ func BenchmarkUpdate(b *testing.B) {\n \t\tb.Run(fmt.Sprintf(\"n=Insert100Loc/size=%d\", size), func(b *testing.B) {\n \t\t\tfor range b.N {\n \t\t\t\tfor i := range fs {\n-\t\t\t\t\tfs[i] = genFile(rand.String(24), 64, 0)\n+\t\t\t\t\tfs[i] = genFile(rand.String(24), numBlocks, 0)\n \t\t\t\t}\n \t\t\t\tif err := db.Update(folderID, protocol.LocalDeviceID, fs); err != nil {\n \t\t\t\t\tb.Fatal(err)\n@@ -146,6 +140,20 @@ func BenchmarkUpdate(b *testing.B) {\n \t\t\tb.ReportMetric(float64(count)/b.Elapsed().Seconds(), \"files/s\")\n \t\t})\n \n+\t\tb.Run(fmt.Sprintf(\"n=AllLocalBlocksWithHash/size=%d\", size), func(b *testing.B) {\n+\t\t\tcount := 0\n+\t\t\tfor range b.N {\n+\t\t\t\tit, errFn := db.AllLocalBlocksWithHash(folderID, globalFi.Blocks[0].Hash)\n+\t\t\t\tfor range it {\n+\t\t\t\t\tcount++\n+\t\t\t\t}\n+\t\t\t\tif err := errFn(); err != nil {\n+\t\t\t\t\tb.Fatal(err)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tb.ReportMetric(float64(count)/b.Elapsed().Seconds(), \"blocks/s\")\n+\t\t})\n+\n \t\tb.Run(fmt.Sprintf(\"n=GetDeviceSequenceLoc/size=%d\", size), func(b *testing.B) {\n \t\t\tfor range b.N {\n \t\t\t\t_, err := db.GetDeviceSequence(folderID, protocol.LocalDeviceID)\n@@ -193,7 +201,7 @@ func BenchmarkUpdate(b *testing.B) {\n \t\t\tb.ReportMetric(float64(count)/b.Elapsed().Seconds(), \"files/s\")\n \t\t})\n \n-\t\tsize <<= 1\n+\t\tsize += 1000\n \t}\n }\n \n"}
{"org": "prometheus", "repo": "prometheus", "number": 16365, "state": "closed", "title": "refactor(endpointslice): use service cache.Indexer to achieve better iteration performance", "body": "try to solve the TODO:\r\n\r\n```\r\n// TODO(brancz): use cache.Indexer to index endpointslices by\r\n// LabelServiceName so this operation doesn't have to iterate over all\r\n// endpoint objects.\r\nfor _, obj := range e.endpointSliceStore.List() {\r\n\tesa, err := e.getEndpointSliceAdaptor(obj)\r\n\tif err != nil {\r\n\t\te.logger.Error(\"converting to EndpointSlice object failed\", \"err\", err)\r\n\t\tcontinue\r\n\t}\r\n\tif lv, exists := esa.labels()[esa.labelServiceName()]; exists && lv == svc.Name {\r\n\t\te.enqueue(esa.get())\r\n\t}\r\n}\r\n```\r\n\r\n\r\n<!--\r\n    Please give your PR a title in the form \"area: short description\".  For example \"tsdb: reduce disk usage by 95%\"\r\n\r\n    If your PR is to fix an issue, put \"Fixes #issue-number\" in the description.\r\n\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s / --signoff flag to `git commit`. See https://github.com/apps/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n", "url": "https://api.github.com/repos/prometheus/prometheus/pulls/16365", "id": 2430862730, "node_id": "PR_kwDOAGhaic6Q5AWK", "html_url": "https://github.com/prometheus/prometheus/pull/16365", "diff_url": "https://github.com/prometheus/prometheus/pull/16365.diff", "patch_url": "https://github.com/prometheus/prometheus/pull/16365.patch", "issue_url": "https://api.github.com/repos/prometheus/prometheus/issues/16365", "created_at": "2025-04-01T08:03:58+00:00", "updated_at": "2025-05-20T18:33:25+00:00", "closed_at": "2025-05-20T18:33:25+00:00", "merged_at": "2025-05-20T18:33:25+00:00", "merge_commit_sha": "091e662f4d2e7b0e0310844fe49d7297d17f1bc4", "labels": [], "draft": false, "commits_url": "https://api.github.com/repos/prometheus/prometheus/pulls/16365/commits", "review_comments_url": "https://api.github.com/repos/prometheus/prometheus/pulls/16365/comments", "review_comment_url": "https://api.github.com/repos/prometheus/prometheus/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/prometheus/prometheus/issues/16365/comments", "base": {"label": "prometheus:main", "ref": "main", "sha": "ba4b058b7ab60105e03f83380cc3200a8a66e52f", "user": {"login": "prometheus", "id": 3380462, "node_id": "MDEyOk9yZ2FuaXphdGlvbjMzODA0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3380462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prometheus", "html_url": "https://github.com/prometheus", "followers_url": "https://api.github.com/users/prometheus/followers", "following_url": "https://api.github.com/users/prometheus/following{/other_user}", "gists_url": "https://api.github.com/users/prometheus/gists{/gist_id}", "starred_url": "https://api.github.com/users/prometheus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prometheus/subscriptions", "organizations_url": "https://api.github.com/users/prometheus/orgs", "repos_url": "https://api.github.com/users/prometheus/repos", "events_url": "https://api.github.com/users/prometheus/events{/privacy}", "received_events_url": "https://api.github.com/users/prometheus/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 6838921, "node_id": "MDEwOlJlcG9zaXRvcnk2ODM4OTIx", "name": "prometheus", "full_name": "prometheus/prometheus", "private": false, "owner": {"login": "prometheus", "id": 3380462, "node_id": "MDEyOk9yZ2FuaXphdGlvbjMzODA0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3380462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prometheus", "html_url": "https://github.com/prometheus", "followers_url": "https://api.github.com/users/prometheus/followers", "following_url": "https://api.github.com/users/prometheus/following{/other_user}", "gists_url": "https://api.github.com/users/prometheus/gists{/gist_id}", "starred_url": "https://api.github.com/users/prometheus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prometheus/subscriptions", "organizations_url": "https://api.github.com/users/prometheus/orgs", "repos_url": "https://api.github.com/users/prometheus/repos", "events_url": "https://api.github.com/users/prometheus/events{/privacy}", "received_events_url": "https://api.github.com/users/prometheus/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/prometheus/prometheus", "description": "The Prometheus monitoring system and time series database.", "fork": false, "url": "https://api.github.com/repos/prometheus/prometheus", "forks_url": "https://api.github.com/repos/prometheus/prometheus/forks", "keys_url": "https://api.github.com/repos/prometheus/prometheus/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/prometheus/prometheus/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/prometheus/prometheus/teams", "hooks_url": "https://api.github.com/repos/prometheus/prometheus/hooks", "issue_events_url": "https://api.github.com/repos/prometheus/prometheus/issues/events{/number}", "events_url": "https://api.github.com/repos/prometheus/prometheus/events", "assignees_url": "https://api.github.com/repos/prometheus/prometheus/assignees{/user}", "branches_url": "https://api.github.com/repos/prometheus/prometheus/branches{/branch}", "tags_url": "https://api.github.com/repos/prometheus/prometheus/tags", "blobs_url": "https://api.github.com/repos/prometheus/prometheus/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/prometheus/prometheus/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/prometheus/prometheus/git/refs{/sha}", "trees_url": "https://api.github.com/repos/prometheus/prometheus/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/prometheus/prometheus/statuses/{sha}", "languages_url": "https://api.github.com/repos/prometheus/prometheus/languages", "stargazers_url": "https://api.github.com/repos/prometheus/prometheus/stargazers", "contributors_url": "https://api.github.com/repos/prometheus/prometheus/contributors", "subscribers_url": "https://api.github.com/repos/prometheus/prometheus/subscribers", "subscription_url": "https://api.github.com/repos/prometheus/prometheus/subscription", "commits_url": "https://api.github.com/repos/prometheus/prometheus/commits{/sha}", "git_commits_url": "https://api.github.com/repos/prometheus/prometheus/git/commits{/sha}", "comments_url": "https://api.github.com/repos/prometheus/prometheus/comments{/number}", "issue_comment_url": "https://api.github.com/repos/prometheus/prometheus/issues/comments{/number}", "contents_url": "https://api.github.com/repos/prometheus/prometheus/contents/{+path}", "compare_url": "https://api.github.com/repos/prometheus/prometheus/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/prometheus/prometheus/merges", "archive_url": "https://api.github.com/repos/prometheus/prometheus/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/prometheus/prometheus/downloads", "issues_url": "https://api.github.com/repos/prometheus/prometheus/issues{/number}", "pulls_url": "https://api.github.com/repos/prometheus/prometheus/pulls{/number}", "milestones_url": "https://api.github.com/repos/prometheus/prometheus/milestones{/number}", "notifications_url": "https://api.github.com/repos/prometheus/prometheus/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/prometheus/prometheus/labels{/name}", "releases_url": "https://api.github.com/repos/prometheus/prometheus/releases{/id}", "deployments_url": "https://api.github.com/repos/prometheus/prometheus/deployments", "created_at": "2012-11-24T11:14:12Z", "updated_at": "2026-01-07T09:07:54Z", "pushed_at": "2026-01-07T07:44:59Z", "git_url": "git://github.com/prometheus/prometheus.git", "ssh_url": "git@github.com:prometheus/prometheus.git", "clone_url": "https://github.com/prometheus/prometheus.git", "svn_url": "https://github.com/prometheus/prometheus", "homepage": "https://prometheus.io/", "size": 274163, "stargazers_count": 62089, "watchers_count": 62089, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": true, "forks_count": 10058, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 737, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": true, "topics": ["alerting", "graphing", "hacktoberfest", "metrics", "monitoring", "prometheus", "time-series"], "visibility": "public", "forks": 10058, "open_issues": 737, "watchers": 62089, "default_branch": "main"}}, "commits": [{"sha": "210b368abc210d4be2f1a47c8086ddd83d9ecf92", "parents": ["ba4b058b7ab60105e03f83380cc3200a8a66e52f"], "message": "refactor(endpointslice): use cache.Indexer to index endpointslices by LabelServiceName so not have to iterate over all endpoint objects.\n\nSigned-off-by: Ryan Wu <rongjun0821@gmail.com>"}, {"sha": "6a079925c9d9cf913f8d0bc15d28b8dca693165c", "parents": ["210b368abc210d4be2f1a47c8086ddd83d9ecf92"], "message": "check the type and error early and add 'TestEndpointSliceDiscoveryWithUnrelatedServiceUpdate' unit test to give a regression test\n\nSigned-off-by: Ryan Wu <rongjun0821@gmail.com>"}, {"sha": "c19a99ce4a2b01828b9d457d9a15589dcb4397b6", "parents": ["6a079925c9d9cf913f8d0bc15d28b8dca693165c"], "message": "make service indexer namespaced\n\nSigned-off-by: Ryan Wu <rongjun0821@gmail.com>"}, {"sha": "a580920a1119e241fe9665786ae5c68d3d1b8fff", "parents": ["c19a99ce4a2b01828b9d457d9a15589dcb4397b6"], "message": "remove unneeded test func\n\nSigned-off-by: Ryan Wu <rongjun0821@gmail.com>"}, {"sha": "615ca67b268a71df1c0c245640908b4aead47197", "parents": ["a580920a1119e241fe9665786ae5c68d3d1b8fff"], "message": "Apply suggestions from code review\n\nCo-authored-by: Ayoub Mrini <ayoubmrini424@gmail.com>\nSigned-off-by: Ryan Wu <rongjun0821@gmail.com>"}], "resolved_issues": [{"org": "prometheus", "repo": "prometheus", "number": -1, "state": "unknown", "title": "refactor(endpointslice): use service cache.Indexer to achieve better iteration performance", "body": "try to solve the TODO:\r\n\r\n```\r\n// TODO(brancz): use cache.Indexer to index endpointslices by\r\n// LabelServiceName so this operation doesn't have to iterate over all\r\n// endpoint objects.\r\nfor _, obj := range e.endpointSliceStore.List() {\r\n\tesa, err := e.getEndpointSliceAdaptor(obj)\r\n\tif err != nil {\r\n\t\te.logger.Error(\"converting to EndpointSlice object failed\", \"err\", err)\r\n\t\tcontinue\r\n\t}\r\n\tif lv, exists := esa.labels()[esa.labelServiceName()]; exists && lv == svc.Name {\r\n\t\te.enqueue(esa.get())\r\n\t}\r\n}\r\n```\r\n\r\n\r\n<!--\r\n    Please give your PR a title in the form \"area: short description\".  For example \"tsdb: reduce disk usage by 95%\"\r\n\r\n    If your PR is to fix an issue, put \"Fixes #issue-number\" in the description.\r\n\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s / --signoff flag to `git commit`. See https://github.com/apps/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n"}], "fix_patch": "diff --git a/discovery/kubernetes/endpointslice.go b/discovery/kubernetes/endpointslice.go\nindex 1002025128b..625601abc1f 100644\n--- a/discovery/kubernetes/endpointslice.go\n+++ b/discovery/kubernetes/endpointslice.go\n@@ -32,6 +32,8 @@ import (\n \t\"github.com/prometheus/prometheus/util/strutil\"\n )\n \n+const serviceIndex = \"service\"\n+\n // EndpointSlice discovers new endpoint targets.\n type EndpointSlice struct {\n \tlogger *slog.Logger\n@@ -101,19 +103,14 @@ func NewEndpointSlice(l *slog.Logger, eps cache.SharedIndexInformer, svc, pod, n\n \t\t\treturn\n \t\t}\n \n-\t\t// TODO(brancz): use cache.Indexer to index endpointslices by\n-\t\t// LabelServiceName so this operation doesn't have to iterate over all\n-\t\t// endpoint objects.\n-\t\tfor _, obj := range e.endpointSliceStore.List() {\n-\t\t\tes, ok := obj.(*v1.EndpointSlice)\n-\t\t\tif !ok {\n-\t\t\t\te.logger.Error(\"converting to EndpointSlice object failed\", \"err\", err)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\t// Only consider the underlying EndpointSlices in the same namespace.\n-\t\t\tif svcName, exists := es.Labels[v1.LabelServiceName]; exists && svcName == svc.Name && es.Namespace == svc.Namespace {\n-\t\t\t\te.enqueue(es)\n-\t\t\t}\n+\t\tendpointSlices, err := e.endpointSliceInf.GetIndexer().ByIndex(serviceIndex, namespacedName(svc.Namespace, svc.Name))\n+\t\tif err != nil {\n+\t\t\te.logger.Error(\"getting endpoint slices by service name failed\", \"err\", err)\n+\t\t\treturn\n+\t\t}\n+\n+\t\tfor _, endpointSlice := range endpointSlices {\n+\t\t\te.enqueue(endpointSlice)\n \t\t}\n \t}\n \t_, err = e.serviceInf.AddEventHandler(cache.ResourceEventHandlerFuncs{\ndiff --git a/discovery/kubernetes/kubernetes.go b/discovery/kubernetes/kubernetes.go\nindex 2c4829ca8d8..ebd2adb87a0 100644\n--- a/discovery/kubernetes/kubernetes.go\n+++ b/discovery/kubernetes/kubernetes.go\n@@ -716,28 +716,41 @@ func (d *Discovery) newEndpointsByNodeInformer(plw *cache.ListWatch) cache.Share\n \n func (d *Discovery) newEndpointSlicesByNodeInformer(plw *cache.ListWatch, object runtime.Object) cache.SharedIndexInformer {\n \tindexers := make(map[string]cache.IndexFunc)\n+\tindexers[serviceIndex] = func(obj interface{}) ([]string, error) {\n+\t\te, ok := obj.(*disv1.EndpointSlice)\n+\t\tif !ok {\n+\t\t\treturn nil, errors.New(\"object is not an endpointslice\")\n+\t\t}\n+\n+\t\tsvcName, exists := e.Labels[disv1.LabelServiceName]\n+\t\tif !exists {\n+\t\t\treturn nil, nil\n+\t\t}\n+\n+\t\treturn []string{namespacedName(e.Namespace, svcName)}, nil\n+\t}\n \tif !d.attachMetadata.Node {\n \t\treturn d.mustNewSharedIndexInformer(plw, object, resyncDisabled, indexers)\n \t}\n \n \tindexers[nodeIndex] = func(obj interface{}) ([]string, error) {\n+\t\te, ok := obj.(*disv1.EndpointSlice)\n+\t\tif !ok {\n+\t\t\treturn nil, errors.New(\"object is not an endpointslice\")\n+\t\t}\n+\n \t\tvar nodes []string\n-\t\tswitch e := obj.(type) {\n-\t\tcase *disv1.EndpointSlice:\n-\t\t\tfor _, target := range e.Endpoints {\n-\t\t\t\tif target.TargetRef != nil {\n-\t\t\t\t\tswitch target.TargetRef.Kind {\n-\t\t\t\t\tcase \"Pod\":\n-\t\t\t\t\t\tif target.NodeName != nil {\n-\t\t\t\t\t\t\tnodes = append(nodes, *target.NodeName)\n-\t\t\t\t\t\t}\n-\t\t\t\t\tcase \"Node\":\n-\t\t\t\t\t\tnodes = append(nodes, target.TargetRef.Name)\n+\t\tfor _, target := range e.Endpoints {\n+\t\t\tif target.TargetRef != nil {\n+\t\t\t\tswitch target.TargetRef.Kind {\n+\t\t\t\tcase \"Pod\":\n+\t\t\t\t\tif target.NodeName != nil {\n+\t\t\t\t\t\tnodes = append(nodes, *target.NodeName)\n \t\t\t\t\t}\n+\t\t\t\tcase \"Node\":\n+\t\t\t\t\tnodes = append(nodes, target.TargetRef.Name)\n \t\t\t\t}\n \t\t\t}\n-\t\tdefault:\n-\t\t\treturn nil, errors.New(\"object is not an endpointslice\")\n \t\t}\n \n \t\treturn nodes, nil\n", "test_patch": "diff --git a/discovery/kubernetes/endpointslice_test.go b/discovery/kubernetes/endpointslice_test.go\nindex 21d49defd40..9eea9abd7b0 100644\n--- a/discovery/kubernetes/endpointslice_test.go\n+++ b/discovery/kubernetes/endpointslice_test.go\n@@ -1194,11 +1194,12 @@ func TestEndpointSliceInfIndexersCount(t *testing.T) {\n \t\tt.Run(tc.name, func(t *testing.T) {\n \t\t\tt.Parallel()\n \t\t\tvar (\n-\t\t\t\tn                    *Discovery\n-\t\t\t\tmainInfIndexersCount int\n+\t\t\t\tn *Discovery\n+\t\t\t\t// service indexer is enabled by default\n+\t\t\t\tmainInfIndexersCount = 1\n \t\t\t)\n \t\t\tif tc.withNodeMetadata {\n-\t\t\t\tmainInfIndexersCount = 1\n+\t\t\t\tmainInfIndexersCount++\n \t\t\t\tn, _ = makeDiscoveryWithMetadata(RoleEndpointSlice, NamespaceDiscovery{}, AttachMetadataConfig{Node: true})\n \t\t\t} else {\n \t\t\t\tn, _ = makeDiscovery(RoleEndpointSlice, NamespaceDiscovery{})\n"}
{"org": "cockroachdb", "repo": "cockroach", "number": 144021, "state": "closed", "title": "sql: optimize the prepared statements cache", "body": "#### querycache,prep: move PrepareMetadata to new prep package\r\n\r\nThis is a mechanical change that moves `querycache.PrepareMetadata` to a\r\nnew `prep` package in preparation for future changes.\r\n\r\nRelease note: None\r\n\r\n#### sql,prep: move PreparedStatement to prep package\r\n\r\nThis is a mechanical change the moves `sql.PreparedStatement` to the\r\n`prep` package in preparation for future changes.\r\n\r\nRelease note: None\r\n\r\n#### prep: add Cache\r\n\r\nA new `prep.Cache` data structure has been added which maps prepared\r\nstatement names to `*prep.Statement`s. `Cache` provides a concise API\r\nand it is optimized for the most common interactions in most workloads:\r\n\r\n  1. Fetching prepared statements with `Get`.\r\n  2. Snapshotting the `Cache` with `Commit` when no changes have been\r\n     applied since the last `Commit`.\r\n\r\nIn order to facilitate automatic transaction retries which rewind the\r\ncommand buffer, changes to the prepared statements cache must be undone.\r\nWe currently achieve this using two maps: one up-to-date map and the\r\nother a snapshot of the prepared statements after the last transaction\r\ncommitted. Before retrying, we clear the up-to-date copy of the map and\r\ncopy the contents of the snapshot into it, effectively undoing and\r\nchanges to the cache in the current transaction. If the transaction is\r\nsuccessful, we clear the snapshot and copy the up-to-date map into it,\r\neffectively creating a new snapshot.\r\n\r\nThis approach is expensive even if the prepared statements cache is not\r\naltered. If the cache hasn't changed, we can avoid clearing and copying\r\ninto the snapshot, but we still need to check for map equality to\r\ndetermine if there have been changes. This equality check has time\r\ncomplexity proportional to the number of cached prepared statements.\r\n\r\nAlso, there are two other maps we use implement an LRU eviction policy.\r\nThey keep track of the ordering of most recent access for each statement\r\n(one is up-to-date and the other is a snapshot). Every time a prepared\r\nstatement is executed that was not the previously executed statement,\r\nthe most recent access ordering changes. Therefore, we can rarely avoid\r\nthe clear and copy of the snapshot like we can with the other maps.\r\n\r\nThe current implementation also lacks a clear API - the four maps are\r\nseparate objects and directly accessed across the `sql` pacakge and\r\nprone to misuse.\r\n\r\n`Cache` achieves similar behavior more efficiently with a clear API. It\r\nprovides `Add`, `Get`, `Remove`, and other methods with obvious purpose.\r\nThe `Rewind` and `Commit` methods facilitate automatic retries. `Commit`\r\nis a constant-time operation if there have been no changes to the\r\n`Cache` since the last `Commit`. See the inline comments for more\r\ndetails.\r\n\r\nThe behavior of `Cache`, when used, will vary slightly from the current\r\nbehavior. Prior to this commit prepared statements were evicted as soon\r\nnew statements were added. Now, prepared statements are evicted when the\r\ncache is committed, i.e., when statements succeed (or fail without a\r\nretriable error) and we know that the command buffer will not be\r\nrewound. This difference in behavior should have no meaningful impact\r\nfor users.\r\n\r\nFixes #143732\r\n\r\nRelease note (performance improvement): Prepared statements are now more\r\nefficiently cached.\r\n\r\n#### sql: replace prepared statement maps with prep.Cache\r\n\r\nFour maps using to implement the prepared statements cache have been\r\nrepalced with `prep.Cache`. Additional, a snapshot of a connection's\r\nportals is now kept in a map in `prepStmtNamespace` so that\r\n`prepStmtNamespace` can implement `commit` and `rewind` methods.\r\n\r\nRelease note (performance improvement): Prepared statements are now more\r\nefficiently cached.\r\n\r\n#### prep: decrement statement reference counts in Cache.Init\r\n\r\nPrepared statement reference counts are now decremented in `Cache.Init`\r\nif the cache is non-empty. This makes it easier and safe to clear a\r\n`Cache` with `Init`.\r\n\r\nAlso, some minor changes have been made to simplify the test oracle.\r\n\r\nRelease note: None\r\n", "url": "https://api.github.com/repos/cockroachdb/cockroach/pulls/144021", "id": 2444584700, "node_id": "PR_kwDOAPy9g86RtWb8", "html_url": "https://github.com/cockroachdb/cockroach/pull/144021", "diff_url": "https://github.com/cockroachdb/cockroach/pull/144021.diff", "patch_url": "https://github.com/cockroachdb/cockroach/pull/144021.patch", "issue_url": "https://api.github.com/repos/cockroachdb/cockroach/issues/144021", "created_at": "2025-04-08T00:23:08+00:00", "updated_at": "2025-06-16T13:45:33+00:00", "closed_at": "2025-04-21T22:02:45+00:00", "merged_at": "2025-04-21T22:02:45+00:00", "merge_commit_sha": "3fd6c603858b94ec7a76d7e8891caea079a34ab4", "labels": ["o-perf-efficiency", "v25.3.0-prerelease"], "draft": false, "commits_url": "https://api.github.com/repos/cockroachdb/cockroach/pulls/144021/commits", "review_comments_url": "https://api.github.com/repos/cockroachdb/cockroach/pulls/144021/comments", "review_comment_url": "https://api.github.com/repos/cockroachdb/cockroach/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/cockroachdb/cockroach/issues/144021/comments", "base": {"label": "cockroachdb:master", "ref": "master", "sha": "61868f0ae68a5fb0cadeda92f0be913e53b41c0f", "user": {"login": "cockroachdb", "id": 6748139, "node_id": "MDEyOk9yZ2FuaXphdGlvbjY3NDgxMzk=", "avatar_url": "https://avatars.githubusercontent.com/u/6748139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cockroachdb", "html_url": "https://github.com/cockroachdb", "followers_url": "https://api.github.com/users/cockroachdb/followers", "following_url": "https://api.github.com/users/cockroachdb/following{/other_user}", "gists_url": "https://api.github.com/users/cockroachdb/gists{/gist_id}", "starred_url": "https://api.github.com/users/cockroachdb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cockroachdb/subscriptions", "organizations_url": "https://api.github.com/users/cockroachdb/orgs", "repos_url": "https://api.github.com/users/cockroachdb/repos", "events_url": "https://api.github.com/users/cockroachdb/events{/privacy}", "received_events_url": "https://api.github.com/users/cockroachdb/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 16563587, "node_id": "MDEwOlJlcG9zaXRvcnkxNjU2MzU4Nw==", "name": "cockroach", "full_name": "cockroachdb/cockroach", "private": false, "owner": {"login": "cockroachdb", "id": 6748139, "node_id": "MDEyOk9yZ2FuaXphdGlvbjY3NDgxMzk=", "avatar_url": "https://avatars.githubusercontent.com/u/6748139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cockroachdb", "html_url": "https://github.com/cockroachdb", "followers_url": "https://api.github.com/users/cockroachdb/followers", "following_url": "https://api.github.com/users/cockroachdb/following{/other_user}", "gists_url": "https://api.github.com/users/cockroachdb/gists{/gist_id}", "starred_url": "https://api.github.com/users/cockroachdb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cockroachdb/subscriptions", "organizations_url": "https://api.github.com/users/cockroachdb/orgs", "repos_url": "https://api.github.com/users/cockroachdb/repos", "events_url": "https://api.github.com/users/cockroachdb/events{/privacy}", "received_events_url": "https://api.github.com/users/cockroachdb/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/cockroachdb/cockroach", "description": "CockroachDB â€” the cloud native, distributed SQL database designed for high availability, effortless scale, and control over data placement.", "fork": false, "url": "https://api.github.com/repos/cockroachdb/cockroach", "forks_url": "https://api.github.com/repos/cockroachdb/cockroach/forks", "keys_url": "https://api.github.com/repos/cockroachdb/cockroach/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/cockroachdb/cockroach/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/cockroachdb/cockroach/teams", "hooks_url": "https://api.github.com/repos/cockroachdb/cockroach/hooks", "issue_events_url": "https://api.github.com/repos/cockroachdb/cockroach/issues/events{/number}", "events_url": "https://api.github.com/repos/cockroachdb/cockroach/events", "assignees_url": "https://api.github.com/repos/cockroachdb/cockroach/assignees{/user}", "branches_url": "https://api.github.com/repos/cockroachdb/cockroach/branches{/branch}", "tags_url": "https://api.github.com/repos/cockroachdb/cockroach/tags", "blobs_url": "https://api.github.com/repos/cockroachdb/cockroach/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/cockroachdb/cockroach/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/cockroachdb/cockroach/git/refs{/sha}", "trees_url": "https://api.github.com/repos/cockroachdb/cockroach/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/cockroachdb/cockroach/statuses/{sha}", "languages_url": "https://api.github.com/repos/cockroachdb/cockroach/languages", "stargazers_url": "https://api.github.com/repos/cockroachdb/cockroach/stargazers", "contributors_url": "https://api.github.com/repos/cockroachdb/cockroach/contributors", "subscribers_url": "https://api.github.com/repos/cockroachdb/cockroach/subscribers", "subscription_url": "https://api.github.com/repos/cockroachdb/cockroach/subscription", "commits_url": "https://api.github.com/repos/cockroachdb/cockroach/commits{/sha}", "git_commits_url": "https://api.github.com/repos/cockroachdb/cockroach/git/commits{/sha}", "comments_url": "https://api.github.com/repos/cockroachdb/cockroach/comments{/number}", "issue_comment_url": "https://api.github.com/repos/cockroachdb/cockroach/issues/comments{/number}", "contents_url": "https://api.github.com/repos/cockroachdb/cockroach/contents/{+path}", "compare_url": "https://api.github.com/repos/cockroachdb/cockroach/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/cockroachdb/cockroach/merges", "archive_url": "https://api.github.com/repos/cockroachdb/cockroach/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/cockroachdb/cockroach/downloads", "issues_url": "https://api.github.com/repos/cockroachdb/cockroach/issues{/number}", "pulls_url": "https://api.github.com/repos/cockroachdb/cockroach/pulls{/number}", "milestones_url": "https://api.github.com/repos/cockroachdb/cockroach/milestones{/number}", "notifications_url": "https://api.github.com/repos/cockroachdb/cockroach/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/cockroachdb/cockroach/labels{/name}", "releases_url": "https://api.github.com/repos/cockroachdb/cockroach/releases{/id}", "deployments_url": "https://api.github.com/repos/cockroachdb/cockroach/deployments", "created_at": "2014-02-06T00:18:47Z", "updated_at": "2026-01-07T08:40:33Z", "pushed_at": "2026-01-07T08:41:17Z", "git_url": "git://github.com/cockroachdb/cockroach.git", "ssh_url": "git@github.com:cockroachdb/cockroach.git", "clone_url": "https://github.com/cockroachdb/cockroach.git", "svn_url": "https://github.com/cockroachdb/cockroach", "homepage": "https://www.cockroachlabs.com", "size": 3460906, "stargazers_count": 31701, "watchers_count": 31701, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": false, "has_wiki": true, "has_pages": false, "has_discussions": true, "forks_count": 4054, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 7878, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "topics": ["cockroachdb", "database", "distributed-database", "go", "hacktoberfest", "sql"], "visibility": "public", "forks": 4054, "open_issues": 7878, "watchers": 31701, "default_branch": "master"}}, "commits": [{"sha": "6e2fc55cc7444618a98acfb028d8d3773078e0cb", "parents": ["61868f0ae68a5fb0cadeda92f0be913e53b41c0f"], "message": "querycache,prep: move PrepareMetadata to new prep package\n\nThis is a mechanical change that moves `querycache.PrepareMetadata` to a\nnew `prep` package in preparation for future changes.\n\nRelease note: None"}, {"sha": "935cd7abc9abe3018b40f3718aaf0e051c8a9022", "parents": ["6e2fc55cc7444618a98acfb028d8d3773078e0cb"], "message": "sql,prep: move PreparedStatement to prep package\n\nThis is a mechanical change the moves `sql.PreparedStatement` to the\n`prep` package in preparation for future changes.\n\nRelease note: None"}, {"sha": "38bea1ab74d159e00e2f909b32ee05ad8b80d772", "parents": ["935cd7abc9abe3018b40f3718aaf0e051c8a9022"], "message": "prep: add Cache\n\nA new `prep.Cache` data structure has been added which maps prepared\nstatement names to `*prep.Statement`s. `Cache` provides a concise API\nand it is optimized for the most common interactions in most workloads:\n\n  1. Fetching prepared statements with `Get`.\n  2. Snapshotting the `Cache` with `Commit` when no changes have been\n     applied since the last `Commit`.\n\nIn order to facilitate automatic transaction retries which rewind the\ncommand buffer, changes to the prepared statements cache must be undone.\nWe currently achieve this using two maps: one up-to-date map and the\nother a snapshot of the prepared statements after the last transaction\ncommitted. Before retrying, we clear the up-to-date copy of the map and\ncopy the contents of the snapshot into it, effectively undoing and\nchanges to the cache in the current transaction. If the transaction is\nsuccessful, we clear the snapshot and copy the up-to-date map into it,\neffectively creating a new snapshot.\n\nThis approach is expensive even if the prepared statements cache is not\naltered. If the cache hasn't changed, we can avoid clearing and copying\ninto the snapshot, but we still need to check for map equality to\ndetermine if there have been changes. This equality check has time\ncomplexity proportional to the number of cached prepared statements.\n\nAlso, there are two other maps we use implement an LRU eviction policy.\nThey keep track of the ordering of most recent access for each statement\n(one is up-to-date and the other is a snapshot). Every time a prepared\nstatement is executed that was not the previously executed statement,\nthe most recent access ordering changes. Therefore, we can rarely avoid\nthe clear and copy of the snapshot like we can with the other maps.\n\nThe current implementation also lacks a clear API - the four maps are\nseparate objects and directly accessed across the `sql` pacakge and\nprone to misuse.\n\n`Cache` achieves similar behavior more efficiently with a clear API. It\nprovides `Add`, `Get`, `Remove`, and other methods with obvious purpose.\nThe `Rewind` and `Commit` methods facilitate automatic retries. `Commit`\nis a constant-time operation if there have been no changes to the\n`Cache` since the last `Commit`. See the inline comments for more\ndetails.\n\nThe behavior of `Cache`, when used, will vary slightly from the current\nbehavior. Prior to this commit prepared statements were evicted as soon\nnew statements were added. Now, prepared statements are evicted when the\ncache is committed, i.e., when statements succeed (or fail without a\nretriable error) and we know that the command buffer will not be\nrewound. This difference in behavior should have no meaningful impact\nfor users.\n\nFixes #143732\n\nRelease note (performance improvement): Prepared statements are now more\nefficiently cached."}, {"sha": "ab08f767424c1785623e4c5376a1597d911d8e29", "parents": ["38bea1ab74d159e00e2f909b32ee05ad8b80d772"], "message": "sql: replace prepared statement maps with prep.Cache\n\nFour maps using to implement the prepared statements cache have been\nrepalced with `prep.Cache`. Additional, a snapshot of a connection's\nportals is now kept in a map in `prepStmtNamespace` so that\n`prepStmtNamespace` can implement `commit` and `rewind` methods.\n\nRelease note (performance improvement): Prepared statements are now more\nefficiently cached."}, {"sha": "81eb376268453d9b9d3923758af159ab746d748a", "parents": ["ab08f767424c1785623e4c5376a1597d911d8e29"], "message": "prep: decrement statement reference counts in Cache.Init\n\nPrepared statement reference counts are now decremented in `Cache.Init`\nif the cache is non-empty. This makes it easier and safe to clear a\n`Cache` with `Init`.\n\nAlso, some minor changes have been made to simplify the test oracle.\n\nRelease note: None"}], "resolved_issues": [{"org": "cockroachdb", "repo": "cockroach", "number": 143732, "state": "closed", "title": "sql: reduce overhead of committing/resetting prepared statements namespace", "body": "#134183 optimized the prepared statements namespace. However, checking for map equality and copying the LRU map (which almost always changes) for each query still adds significant overhead for simple queries. In a recent profile of the sysbench oltp_point_select it accounted for 6% of CPU time:\n\n<img width=\"918\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f99864be-6c97-40d8-abec-596fd90ec90f\" />\n\nThe overhead is proportional to the number of prepared statements per session. In the case of sysbench, this is roughly `9 * num_tables`. \n\nWe should design a new data structure for the prepared statements cache that optimizes for the common case where the cache is not changing.\n\n\nJira issue: CRDB-49017"}], "fix_patch": "diff --git a/pkg/BUILD.bazel b/pkg/BUILD.bazel\nindex 29437f7df16e..ac77c757ea63 100644\n--- a/pkg/BUILD.bazel\n+++ b/pkg/BUILD.bazel\n@@ -567,6 +567,7 @@ ALL_TESTS = [\n     \"//pkg/sql/physicalplan/replicaoracle:replicaoracle_test\",\n     \"//pkg/sql/physicalplan:physicalplan_test\",\n     \"//pkg/sql/plpgsql/parser:parser_test\",\n+    \"//pkg/sql/prep:prep_test\",\n     \"//pkg/sql/privilege:privilege_test\",\n     \"//pkg/sql/protoreflect:protoreflect_test\",\n     \"//pkg/sql/querycache:querycache_test\",\n@@ -2151,6 +2152,8 @@ GO_TARGETS = [\n     \"//pkg/sql/plpgsql/parser/lexbase:lexbase\",\n     \"//pkg/sql/plpgsql/parser:parser_test\",\n     \"//pkg/sql/plpgsql/parser:plpgparser\",\n+    \"//pkg/sql/prep:prep\",\n+    \"//pkg/sql/prep:prep_test\",\n     \"//pkg/sql/privilege:privilege\",\n     \"//pkg/sql/privilege:privilege_test\",\n     \"//pkg/sql/protoreflect/gprototest:gprototest\",\ndiff --git a/pkg/server/serverpb/BUILD.bazel b/pkg/server/serverpb/BUILD.bazel\nindex bf4d17f8d776..367a0a507bf6 100644\n--- a/pkg/server/serverpb/BUILD.bazel\n+++ b/pkg/server/serverpb/BUILD.bazel\n@@ -72,6 +72,7 @@ go_proto_library(\n         # NB: The grpc-gateway compiler injects a dependency on the descriptor\n         # package that Gazelle isn't prepared to deal with.\n         \"@com_github_golang_protobuf//descriptor:go_default_library_gen\",  # keep\n+        \"@org_golang_google_genproto_googleapis_api//annotations:go_default_library\",\n         \"//pkg/build\",\n         \"//pkg/clusterversion\",\n         \"//pkg/config/zonepb\",\n@@ -94,7 +95,6 @@ go_proto_library(\n         \"//pkg/util/log/logpb\",\n         \"//pkg/util/metric\",\n         \"//pkg/util/tracing/tracingpb\",\n-        \"@org_golang_google_genproto_googleapis_api//annotations:go_default_library\",\n     ],\n )\n \ndiff --git a/pkg/sql/BUILD.bazel b/pkg/sql/BUILD.bazel\nindex 03e6b74053b7..c5db13a757c0 100644\n--- a/pkg/sql/BUILD.bazel\n+++ b/pkg/sql/BUILD.bazel\n@@ -468,6 +468,7 @@ go_library(\n         \"//pkg/sql/physicalplan\",\n         \"//pkg/sql/physicalplan/replicaoracle\",\n         \"//pkg/sql/plpgsql/parser:plpgparser\",\n+        \"//pkg/sql/prep\",\n         \"//pkg/sql/privilege\",\n         \"//pkg/sql/protoreflect\",\n         \"//pkg/sql/querycache\",\n@@ -702,7 +703,6 @@ go_test(\n         \"pg_oid_test.go\",\n         \"pgwire_internal_test.go\",\n         \"plan_opt_test.go\",\n-        \"prepared_stmt_test.go\",\n         \"privileged_accessor_test.go\",\n         \"region_util_test.go\",\n         \"rename_test.go\",\ndiff --git a/pkg/sql/conn_executor.go b/pkg/sql/conn_executor.go\nindex 95ed3a1dccfd..a8eb3c5aa680 100644\n--- a/pkg/sql/conn_executor.go\n+++ b/pkg/sql/conn_executor.go\n@@ -9,7 +9,6 @@ import (\n \t\"context\"\n \t\"fmt\"\n \t\"io\"\n-\t\"maps\"\n \t\"math\"\n \t\"math/rand\"\n \t\"strings\"\n@@ -53,6 +52,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgnotice\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgwirecancel\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/regions\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/schemachanger/scerrors\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/asof\"\n@@ -78,6 +78,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/util/errorutil\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/fsm\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/hlc\"\n+\t\"github.com/cockroachdb/cockroach/pkg/util/humanizeutil\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/log\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/log/logcrash\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/log/severity\"\n@@ -1254,16 +1255,9 @@ func (s *Server) newConnExecutor(\n \t}\n \n \tex.extraTxnState.underOuterTxn = underOuterTxn\n-\tex.extraTxnState.prepStmtsNamespace = prepStmtNamespace{\n-\t\tprepStmts:    make(map[string]*PreparedStatement),\n-\t\tprepStmtsLRU: make(map[string]struct{ prev, next string }),\n-\t\tportals:      make(map[string]PreparedPortal),\n-\t}\n-\tex.extraTxnState.prepStmtsNamespaceAtTxnRewindPos = prepStmtNamespace{\n-\t\tprepStmts:    make(map[string]*PreparedStatement),\n-\t\tprepStmtsLRU: make(map[string]struct{ prev, next string }),\n-\t\tportals:      make(map[string]PreparedPortal),\n-\t}\n+\tex.extraTxnState.prepStmtsNamespace.prepStmts.Init(ctx)\n+\tex.extraTxnState.prepStmtsNamespace.portals = make(map[string]PreparedPortal)\n+\tex.extraTxnState.prepStmtsNamespace.portalsSnapshot = make(map[string]PreparedPortal)\n \tex.extraTxnState.prepStmtsNamespaceMemAcc = ex.sessionMon.MakeBoundAccount()\n \tdsdp := catsessiondata.NewDescriptorSessionDataStackProvider(sdMutIterator.sds)\n \tex.extraTxnState.descCollection = s.cfg.CollectionFactory.NewCollection(\n@@ -1357,9 +1351,6 @@ func (ex *connExecutor) close(ctx context.Context, closeType closeType) {\n \tex.extraTxnState.prepStmtsNamespace.closeAllPortals(\n \t\tctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t)\n-\tex.extraTxnState.prepStmtsNamespaceAtTxnRewindPos.closeAllPortals(\n-\t\tctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n-\t)\n \tif err := ex.extraTxnState.sqlCursors.closeAll(&ex.planner, cursorCloseForExplicitClose); err != nil {\n \t\tlog.Warningf(ctx, \"error closing cursors: %v\", err)\n \t}\n@@ -1418,10 +1409,7 @@ func (ex *connExecutor) close(ctx context.Context, closeType closeType) {\n \tif closeType != panicClose {\n \t\t// Close all statements and prepared portals. The cursors have already been\n \t\t// closed.\n-\t\tex.extraTxnState.prepStmtsNamespace.resetToEmpty(\n-\t\t\tctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n-\t\t)\n-\t\tex.extraTxnState.prepStmtsNamespaceAtTxnRewindPos.resetToEmpty(\n+\t\tex.extraTxnState.prepStmtsNamespace.clear(\n \t\t\tctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t\t)\n \t\tex.extraTxnState.prepStmtsNamespaceMemAcc.Close(ctx)\n@@ -1586,27 +1574,19 @@ type connExecutor struct {\n \t\t// Set via setTxnRewindPos().\n \t\ttxnRewindPos CmdPos\n \n-\t\t// prepStmtNamespace contains the prepared statements and portals that the\n-\t\t// session currently has access to.\n-\t\t// Portals are bound to a transaction and they're all destroyed once the\n-\t\t// transaction finishes.\n-\t\t// Prepared statements are not transactional and so it's a bit weird that\n-\t\t// they're part of extraTxnState, but it's convenient to put them here\n-\t\t// because they need the same kind of \"snapshoting\" as the portals (see\n-\t\t// prepStmtsNamespaceAtTxnRewindPos).\n+\t\t// prepStmtNamespace contains the portals and prepared statements that\n+\t\t// the session currently has access to. To interact properly with\n+\t\t// automatic transaction retries, we need the ability to restore portals\n+\t\t// and prepared statements as they were at the previous txnRewindPos. To\n+\t\t// achieve this for portals we take a snapshot even time txnRewindPos is\n+\t\t// advanced. For prepared statements we use the Commit and Rewind\n+\t\t// methods of prep.Cache.\n+\t\t//\n+\t\t// Prepared statements are not transactional and so it's a bit weird\n+\t\t// that they're part of extraTxnState, but it's convenient to put them\n+\t\t// here because they need the same kind of snapshotting as the portals.\n \t\tprepStmtsNamespace prepStmtNamespace\n \n-\t\t// prepStmtsNamespaceAtTxnRewindPos is a snapshot of the prep stmts/portals\n-\t\t// (ex.prepStmtsNamespace) before processing the command at position\n-\t\t// txnRewindPos.\n-\t\t// Here's the deal: prepared statements are not transactional, but they do\n-\t\t// need to interact properly with automatic retries (i.e. rewinding the\n-\t\t// command buffer). When doing a rewind, we need to be able to restore the\n-\t\t// prep stmts as they were. We do this by taking a snapshot every time\n-\t\t// txnRewindPos is advanced. Prepared statements are shared between the two\n-\t\t// collections, but these collections are periodically reconciled.\n-\t\tprepStmtsNamespaceAtTxnRewindPos prepStmtNamespace\n-\n \t\t// prepStmtsNamespaceMemAcc is the memory account that is shared\n \t\t// between prepStmtsNamespace and prepStmtsNamespaceAtTxnRewindPos. It\n \t\t// tracks the memory usage of portals and should be closed upon\n@@ -1939,23 +1919,14 @@ func (ch *ctxHolder) unhijack() {\n type prepStmtNamespace struct {\n \t// prepStmts contains the prepared statements currently available on the\n \t// session.\n-\tprepStmts map[string]*PreparedStatement\n-\t// prepStmtsLRU is a circular doubly-linked list containing the prepared\n-\t// statement names ordered by most recent access (needed to determine\n-\t// evictions when prepared_statements_cache_size is set). There is a special\n-\t// entry for the empty string which is both the head and tail of the\n-\t// list. (Consequently, if it exists, the actual prepared statement for the\n-\t// empty string does not have an entry in this list and cannot be evicted.)\n-\tprepStmtsLRU map[string]struct{ prev, next string }\n-\t// prepStmtsLRUAlloc is the total amount of memory allocated for prepared\n-\t// statements in prepStmtsLRU. This will sometimes be less than\n-\t// ex.sessionPreparedMon.AllocBytes() because refcounting causes us to hold\n-\t// onto more PreparedStatements than are currently in the LRU list.\n-\tprepStmtsLRUAlloc int64\n+\tprepStmts prep.Cache\n \t// portals contains the portals currently available on the session. Note\n \t// that PreparedPortal.accountForCopy needs to be called if a copy of a\n \t// PreparedPortal is retained.\n \tportals map[string]PreparedPortal\n+\t// portalsSnapshot is a snapshot of portals at the time of the last call to\n+\t// commit.\n+\tportalsSnapshot map[string]PreparedPortal\n }\n \n // HasActivePortals returns true if there are portals in the session.\n@@ -1969,95 +1940,34 @@ func (ns *prepStmtNamespace) HasPortal(s string) bool {\n \treturn ok\n }\n \n-const prepStmtsLRUHead = \"\"\n-const prepStmtsLRUTail = \"\"\n-\n-// addLRUEntry adds a new prepared statement name to the LRU list. It is an\n-// error to re-add an existing name to the LRU list.\n-func (ns *prepStmtNamespace) addLRUEntry(name string, alloc int64) {\n-\tif name == prepStmtsLRUHead {\n-\t\treturn\n-\t}\n-\tif _, ok := ns.prepStmtsLRU[name]; ok {\n-\t\t// Assert that we're not re-adding an existing name to the LRU list.\n-\t\tpanic(errors.AssertionFailedf(\n-\t\t\t\"prepStmtsLRU unexpected existing entry (%s): %v\", name, ns.prepStmtsLRU,\n-\t\t))\n-\t}\n-\tvar this struct{ prev, next string }\n-\tthis.prev = prepStmtsLRUHead\n-\t// Note: must do this serially in case head and next are the same entry.\n-\thead := ns.prepStmtsLRU[this.prev]\n-\tthis.next = head.next\n-\thead.next = name\n-\tns.prepStmtsLRU[prepStmtsLRUHead] = head\n-\tnext, ok := ns.prepStmtsLRU[this.next]\n-\tif !ok || next.prev != prepStmtsLRUHead {\n-\t\t// Assert that the chain isn't broken before we modify it.\n-\t\tpanic(errors.AssertionFailedf(\n-\t\t\t\"prepStmtsLRU head entry not correct (%s): %v\", this.next, ns.prepStmtsLRU,\n-\t\t))\n-\t}\n-\tnext.prev = name\n-\tns.prepStmtsLRU[this.next] = next\n-\tns.prepStmtsLRU[name] = this\n-\tns.prepStmtsLRUAlloc += alloc\n-}\n-\n-// delLRUEntry removes a prepared statement name from the LRU list. (It is not an\n-// error to remove a non-existent prepared statement.)\n-func (ns *prepStmtNamespace) delLRUEntry(name string, alloc int64) {\n-\tif name == prepStmtsLRUHead {\n-\t\treturn\n+func (ns *prepStmtNamespace) closeAllPortals(\n+\tctx context.Context, prepStmtsNamespaceMemAcc *mon.BoundAccount,\n+) {\n+\tfor name, p := range ns.portals {\n+\t\tp.close(ctx, prepStmtsNamespaceMemAcc, name)\n+\t\tdelete(ns.portals, name)\n \t}\n-\tthis, ok := ns.prepStmtsLRU[name]\n-\tif !ok {\n-\t\t// Not an error to remove a non-existent prepared statement.\n-\t\treturn\n+\tfor name, p := range ns.portalsSnapshot {\n+\t\tp.close(ctx, prepStmtsNamespaceMemAcc, name)\n+\t\tdelete(ns.portalsSnapshot, name)\n \t}\n-\t// Note: must do this serially in case prev and next are the same entry.\n-\tprev, ok := ns.prepStmtsLRU[this.prev]\n-\tif !ok || prev.next != name {\n-\t\t// Assert that the chain isn't broken before we modify it.\n-\t\tpanic(errors.AssertionFailedf(\n-\t\t\t\"prepStmtsLRU prev entry not correct (%s): %v\", this.prev, ns.prepStmtsLRU,\n-\t\t))\n-\t}\n-\tprev.next = this.next\n-\tns.prepStmtsLRU[this.prev] = prev\n-\tnext, ok := ns.prepStmtsLRU[this.next]\n-\tif !ok || next.prev != name {\n-\t\t// Assert that the chain isn't broken before we modify it.\n-\t\tpanic(errors.AssertionFailedf(\n-\t\t\t\"prepStmtsLRU next entry not correct (%s): %v\", this.next, ns.prepStmtsLRU,\n-\t\t))\n-\t}\n-\tnext.prev = this.prev\n-\tns.prepStmtsLRU[this.next] = next\n-\tdelete(ns.prepStmtsLRU, name)\n-\tns.prepStmtsLRUAlloc -= alloc\n }\n \n-// touchLRUEntry moves an existing prepared statement to the front of the LRU\n-// list.\n-func (ns *prepStmtNamespace) touchLRUEntry(name string) {\n-\tif name == prepStmtsLRUHead {\n-\t\treturn\n-\t}\n-\tif ns.prepStmtsLRU[prepStmtsLRUHead].next == name {\n-\t\t// Already at the front of the list.\n-\t\treturn\n+func (ns *prepStmtNamespace) closePortals(\n+\tctx context.Context, prepStmtsNamespaceMemAcc *mon.BoundAccount,\n+) {\n+\tfor name, p := range ns.portals {\n+\t\tp.close(ctx, prepStmtsNamespaceMemAcc, name)\n+\t\tdelete(ns.portals, name)\n \t}\n-\tns.delLRUEntry(name, 0)\n-\tns.addLRUEntry(name, 0)\n }\n \n-func (ns *prepStmtNamespace) closeAllPortals(\n+func (ns *prepStmtNamespace) closeSnapshotPortals(\n \tctx context.Context, prepStmtsNamespaceMemAcc *mon.BoundAccount,\n ) {\n-\tfor name, p := range ns.portals {\n+\tfor name, p := range ns.portalsSnapshot {\n \t\tp.close(ctx, prepStmtsNamespaceMemAcc, name)\n-\t\tdelete(ns.portals, name)\n+\t\tdelete(ns.portalsSnapshot, name)\n \t}\n }\n \n@@ -2073,14 +1983,18 @@ func (ns *prepStmtNamespace) closeAllPausablePortals(\n }\n \n // MigratablePreparedStatements returns a mapping of all prepared statements.\n-func (ns *prepStmtNamespace) MigratablePreparedStatements() []sessiondatapb.MigratableSession_PreparedStatement {\n-\tret := make([]sessiondatapb.MigratableSession_PreparedStatement, 0, len(ns.prepStmts))\n+func (ns *prepStmtNamespace) MigratablePreparedStatements() (\n+\t[]sessiondatapb.MigratableSession_PreparedStatement,\n+\terror,\n+) {\n+\tif ns.prepStmts.Dirty() {\n+\t\treturn nil, errors.AssertionFailedf(\"cannot serialize dirty prepared statements cache\")\n+\t}\n \n \t// Serialize prepared statements from least-recently used to most-recently\n \t// used, so that we build the LRU list correctly when deserializing.\n-\tfor e, ok := ns.prepStmtsLRU[prepStmtsLRUTail]; ok && e.prev != prepStmtsLRUHead; e, ok = ns.prepStmtsLRU[e.prev] {\n-\t\tname := e.prev\n-\t\tstmt := ns.prepStmts[name]\n+\tret := make([]sessiondatapb.MigratableSession_PreparedStatement, 0, ns.prepStmts.Len())\n+\tns.prepStmts.ForEachLRU(func(name string, stmt *prep.Statement) {\n \t\tret = append(\n \t\t\tret,\n \t\t\tsessiondatapb.MigratableSession_PreparedStatement{\n@@ -2089,33 +2003,22 @@ func (ns *prepStmtNamespace) MigratablePreparedStatements() []sessiondatapb.Migr\n \t\t\t\tSQL:                  stmt.SQL,\n \t\t\t},\n \t\t)\n-\t}\n-\t// Finally, serialize the anonymous prepared statement (if it exists).\n-\tif stmt, ok := ns.prepStmts[\"\"]; ok {\n-\t\tret = append(\n-\t\t\tret,\n-\t\t\tsessiondatapb.MigratableSession_PreparedStatement{\n-\t\t\t\tName:                 \"\",\n-\t\t\t\tPlaceholderTypeHints: stmt.InferredTypes,\n-\t\t\t\tSQL:                  stmt.SQL,\n-\t\t\t},\n-\t\t)\n-\t}\n-\n-\treturn ret\n+\t})\n+\treturn ret, nil\n }\n \n func (ns *prepStmtNamespace) String() string {\n \tvar sb strings.Builder\n \tsb.WriteString(\"Prep stmts: \")\n-\t// Put the anonymous prepared statement first (if it exists).\n-\tif _, ok := ns.prepStmts[\"\"]; ok {\n-\t\tsb.WriteString(\"\\\"\\\" \")\n-\t}\n-\tfor e, ok := ns.prepStmtsLRU[prepStmtsLRUHead]; ok && e.next != prepStmtsLRUTail; e, ok = ns.prepStmtsLRU[e.next] {\n-\t\tsb.WriteString(e.next + \" \")\n+\tif ns.prepStmts.Dirty() {\n+\t\tsb.WriteString(\"<dirty>\")\n+\t} else {\n+\t\tns.prepStmts.ForEachLRU(func(name string, stmt *prep.Statement) {\n+\t\t\tsb.WriteString(name)\n+\t\t\tsb.WriteByte(' ')\n+\t\t})\n \t}\n-\tfmt.Fprintf(&sb, \"LRU alloc: %d \", ns.prepStmtsLRUAlloc)\n+\tfmt.Fprintf(&sb, \"Size: %d \", ns.prepStmts.Size())\n \tsb.WriteString(\"Portals: \")\n \tfor name := range ns.portals {\n \t\tsb.WriteString(name + \" \")\n@@ -2123,48 +2026,41 @@ func (ns *prepStmtNamespace) String() string {\n \treturn sb.String()\n }\n \n-// resetToEmpty deallocates the prepStmtNamespace.\n-func (ns *prepStmtNamespace) resetToEmpty(\n+// clear empties the prepStmtNamespace.\n+func (ns *prepStmtNamespace) clear(\n \tctx context.Context, prepStmtsNamespaceMemAcc *mon.BoundAccount,\n ) {\n-\t// No errors could occur since we're releasing the resources.\n-\t_ = ns.resetTo(ctx, &prepStmtNamespace{}, prepStmtsNamespaceMemAcc)\n+\tns.prepStmts.Init(ctx)\n+\tns.closeAllPortals(ctx, prepStmtsNamespaceMemAcc)\n }\n \n-// resetTo resets a namespace to equate another one (`to`). All the receiver's\n-// references are released and all the to's references are duplicated.\n-//\n-// An empty `to` can be passed in to deallocate everything.\n-//\n-// It can only return an error if we've reached the memory limit and had to make\n-// a copy of portals.\n-func (ns *prepStmtNamespace) resetTo(\n-\tctx context.Context, to *prepStmtNamespace, prepStmtsNamespaceMemAcc *mon.BoundAccount,\n+func (ns *prepStmtNamespace) commit(\n+\tctx context.Context, maxSize int64, prepStmtsNamespaceMemAcc *mon.BoundAccount,\n ) error {\n-\t// Reset prepStmts.\n-\tif !maps.Equal(ns.prepStmts, to.prepStmts) {\n-\t\tfor name, ps := range ns.prepStmts {\n-\t\t\tps.decRef(ctx)\n-\t\t\tdelete(ns.prepStmts, name)\n-\t\t}\n-\t\tfor name, ps := range to.prepStmts {\n-\t\t\tps.incRef(ctx)\n-\t\t\tns.prepStmts[name] = ps\n-\t\t}\n+\tevicted := ns.prepStmts.Commit(ctx, maxSize)\n+\tfor i := range evicted {\n+\t\tlog.VEventf(\n+\t\t\tctx, 1,\n+\t\t\t\"prepared statements are using more than prepared_statements_cache_size (%s), \"+\n+\t\t\t\t\"automatically deallocating %s\", string(humanizeutil.IBytes(maxSize)), evicted[i],\n+\t\t)\n \t}\n-\n-\t// Reset prepStmtsLRU.\n-\tif !maps.Equal(ns.prepStmtsLRU, to.prepStmtsLRU) {\n-\t\tclear(ns.prepStmtsLRU)\n-\t\tmaps.Copy(ns.prepStmtsLRU, to.prepStmtsLRU)\n+\tns.closeSnapshotPortals(ctx, prepStmtsNamespaceMemAcc)\n+\tfor name, p := range ns.portals {\n+\t\tif err := p.accountForCopy(ctx, prepStmtsNamespaceMemAcc, name); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tns.portalsSnapshot[name] = p\n \t}\n+\treturn nil\n+}\n \n-\t// Reset prepStmtsLRUAlloc.\n-\tns.prepStmtsLRUAlloc = to.prepStmtsLRUAlloc\n-\n-\t// Reset portals.\n-\tns.closeAllPortals(ctx, prepStmtsNamespaceMemAcc)\n-\tfor name, p := range to.portals {\n+func (ns *prepStmtNamespace) rewind(\n+\tctx context.Context, prepStmtsNamespaceMemAcc *mon.BoundAccount,\n+) error {\n+\tns.prepStmts.Rewind(ctx)\n+\tns.closePortals(ctx, prepStmtsNamespaceMemAcc)\n+\tfor name, p := range ns.portalsSnapshot {\n \t\tif err := p.accountForCopy(ctx, prepStmtsNamespaceMemAcc, name); err != nil {\n \t\t\treturn err\n \t\t}\n@@ -2201,7 +2097,7 @@ func (ex *connExecutor) resetExtraTxnState(ctx context.Context, ev txnEvent, pay\n \t}\n \n \t// Close all portals.\n-\tex.extraTxnState.prepStmtsNamespace.closeAllPortals(\n+\tex.extraTxnState.prepStmtsNamespace.closePortals(\n \t\tctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t)\n \n@@ -2221,7 +2117,7 @@ func (ex *connExecutor) resetExtraTxnState(ctx context.Context, ev txnEvent, pay\n \n \tswitch ev.eventType {\n \tcase txnCommit, txnRollback, txnPrepare:\n-\t\tex.extraTxnState.prepStmtsNamespaceAtTxnRewindPos.closeAllPortals(\n+\t\tex.extraTxnState.prepStmtsNamespace.closeSnapshotPortals(\n \t\t\tctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t\t)\n \t\tex.extraTxnState.savepoints.clear()\n@@ -3471,16 +3367,17 @@ func stmtHasNoData(stmt tree.Statement) bool {\n // commitPrepStmtNamespace deallocates everything in\n // prepStmtsNamespaceAtTxnRewindPos that's not part of prepStmtsNamespace.\n func (ex *connExecutor) commitPrepStmtNamespace(ctx context.Context) error {\n-\treturn ex.extraTxnState.prepStmtsNamespaceAtTxnRewindPos.resetTo(\n-\t\tctx, &ex.extraTxnState.prepStmtsNamespace, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n+\tcacheSize := ex.sessionData().PreparedStatementsCacheSize\n+\treturn ex.extraTxnState.prepStmtsNamespace.commit(\n+\t\tctx, cacheSize, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t)\n }\n \n-// commitPrepStmtNamespace deallocates everything in prepStmtsNamespace that's\n+// rewindPrepStmtNamespace deallocates everything in prepStmtsNamespace that's\n // not part of prepStmtsNamespaceAtTxnRewindPos.\n func (ex *connExecutor) rewindPrepStmtNamespace(ctx context.Context) error {\n-\treturn ex.extraTxnState.prepStmtsNamespace.resetTo(\n-\t\tctx, &ex.extraTxnState.prepStmtsNamespaceAtTxnRewindPos, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n+\treturn ex.extraTxnState.prepStmtsNamespace.rewind(\n+\t\tctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t)\n }\n \n@@ -4205,7 +4102,7 @@ func (ex *connExecutor) txnStateTransitionsApplyWrapper(\n \t\tex.resetExtraTxnState(ex.Ctx(), advInfo.txnEvent, payloadErr)\n \t\t// Since we're finalizing the SQL transaction (commit, rollback, prepare),\n \t\t// there's no need to keep the prepared stmts for a txn rewind.\n-\t\tex.extraTxnState.prepStmtsNamespaceAtTxnRewindPos.closeAllPortals(\n+\t\tex.extraTxnState.prepStmtsNamespace.closeSnapshotPortals(\n \t\t\tex.Ctx(), &ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t\t)\n \t\tex.resetPlanner(ex.Ctx(), &ex.planner, nil, ex.server.cfg.Clock.PhysicalTime())\n@@ -4839,38 +4736,34 @@ type connExPrepStmtsAccessor struct {\n var _ preparedStatementsAccessor = connExPrepStmtsAccessor{}\n \n // List is part of the preparedStatementsAccessor interface.\n-func (ps connExPrepStmtsAccessor) List() map[string]*PreparedStatement {\n-\t// Return a copy of the data, to prevent modification of the map.\n-\tstmts := ps.ex.extraTxnState.prepStmtsNamespace.prepStmts\n-\tret := make(map[string]*PreparedStatement, len(stmts))\n-\tfor key, stmt := range stmts {\n-\t\tret[key] = stmt\n-\t}\n+func (ps connExPrepStmtsAccessor) List() map[string]*prep.Statement {\n+\tprepStmts := ps.ex.extraTxnState.prepStmtsNamespace.prepStmts\n+\tret := make(map[string]*prep.Statement, prepStmts.Len())\n+\tprepStmts.ForEach(func(name string, stmt *prep.Statement) {\n+\t\tret[name] = stmt\n+\t})\n \treturn ret\n }\n \n // Get is part of the preparedStatementsAccessor interface.\n-func (ps connExPrepStmtsAccessor) Get(name string, touchLRU bool) (*PreparedStatement, bool) {\n-\ts, ok := ps.ex.extraTxnState.prepStmtsNamespace.prepStmts[name]\n-\tif ok && touchLRU {\n-\t\tps.ex.extraTxnState.prepStmtsNamespace.touchLRUEntry(name)\n-\t}\n-\treturn s, ok\n+func (ps connExPrepStmtsAccessor) Get(name string) (*prep.Statement, bool) {\n+\tprepStmts := &ps.ex.extraTxnState.prepStmtsNamespace.prepStmts\n+\treturn prepStmts.Get(name)\n }\n \n // Delete is part of the preparedStatementsAccessor interface.\n func (ps connExPrepStmtsAccessor) Delete(ctx context.Context, name string) bool {\n-\t_, ok := ps.Get(name, false /* touchLRU */)\n-\tif !ok {\n+\tprepStmts := &ps.ex.extraTxnState.prepStmtsNamespace.prepStmts\n+\tif !prepStmts.Has(name) {\n \t\treturn false\n \t}\n-\tps.ex.deletePreparedStmt(ctx, name)\n+\tprepStmts.Remove(name)\n \treturn true\n }\n \n // DeleteAll is part of the preparedStatementsAccessor interface.\n func (ps connExPrepStmtsAccessor) DeleteAll(ctx context.Context) {\n-\tps.ex.extraTxnState.prepStmtsNamespace.resetToEmpty(\n+\tps.ex.extraTxnState.prepStmtsNamespace.clear(\n \t\tctx, &ps.ex.extraTxnState.prepStmtsNamespaceMemAcc,\n \t)\n }\ndiff --git a/pkg/sql/conn_executor_exec.go b/pkg/sql/conn_executor_exec.go\nindex e67c70d54052..83b173d5c1a2 100644\n--- a/pkg/sql/conn_executor_exec.go\n+++ b/pkg/sql/conn_executor_exec.go\n@@ -39,6 +39,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgcode\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/physicalplan\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/regions\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/asof\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/eval\"\n@@ -150,7 +151,7 @@ func (ex *connExecutor) execStmt(\n \t\tev, payload = ex.execStmtInNoTxnState(ctx, parserStmt, res)\n \n \tcase stateOpen:\n-\t\tvar preparedStmt *PreparedStatement\n+\t\tvar preparedStmt *prep.Statement\n \t\tif portal != nil {\n \t\t\tpreparedStmt = portal.Stmt\n \t\t}\n@@ -349,7 +350,7 @@ func (ex *connExecutor) execPortal(\n func (ex *connExecutor) execStmtInOpenState(\n \tctx context.Context,\n \tparserStmt statements.Statement[tree.Statement],\n-\tprepared *PreparedStatement,\n+\tprepared *prep.Statement,\n \tpinfo *tree.PlaceholderInfo,\n \tres RestrictedCommandResult,\n \tcanAutoCommit bool,\n@@ -524,11 +525,10 @@ func (ex *connExecutor) execStmtInOpenState(\n \t\t// Replace the `EXECUTE foo` statement with the prepared statement, and\n \t\t// continue execution.\n \t\tname := e.Name.String()\n-\t\tps, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[name]\n+\t\tps, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts.Get(name)\n \t\tif !ok {\n \t\t\treturn makeErrEvent(newPreparedStmtDNEError(ex.sessionData(), name))\n \t\t}\n-\t\tex.extraTxnState.prepStmtsNamespace.touchLRUEntry(name)\n \n \t\tvar err error\n \t\tpinfo, err = ex.planner.fillInPlaceholders(ctx, ps, name, e.Params)\n@@ -844,7 +844,7 @@ func (ex *connExecutor) execStmtInOpenState(\n \t\t// This is handling the SQL statement \"PREPARE\". See execPrepare for\n \t\t// handling of the protocol-level command for preparing statements.\n \t\tname := s.Name.String()\n-\t\tif _, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[name]; ok {\n+\t\tif ex.extraTxnState.prepStmtsNamespace.prepStmts.Has(name) {\n \t\t\terr := pgerror.Newf(\n \t\t\t\tpgcode.DuplicatePreparedStatement,\n \t\t\t\t\"prepared statement %q already exists\", name,\n@@ -905,7 +905,7 @@ func (ex *connExecutor) execStmtInOpenState(\n \t\t\tp.extendedEvalCtx.Placeholders = oldPlaceholders\n \t\t}()\n \t\tif _, err := ex.addPreparedStmt(\n-\t\t\tctx, name, prepStmt, typeHints, rawTypeHints, PreparedStatementOriginSQL,\n+\t\t\tctx, name, prepStmt, typeHints, rawTypeHints, prep.StatementOriginSQL,\n \t\t); err != nil {\n \t\t\treturn makeErrEvent(err)\n \t\t}\n@@ -1482,11 +1482,10 @@ func (ex *connExecutor) execStmtInOpenStateWithPausablePortal(\n \t\t// Replace the `EXECUTE foo` statement with the prepared statement, and\n \t\t// continue execution.\n \t\tname := e.Name.String()\n-\t\tps, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[name]\n+\t\tps, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts.Get(name)\n \t\tif !ok {\n \t\t\treturn makeErrEvent(newPreparedStmtDNEError(ex.sessionData(), name))\n \t\t}\n-\t\tex.extraTxnState.prepStmtsNamespace.touchLRUEntry(name)\n \n \t\tvar err error\n \t\tpinfo, err = ex.planner.fillInPlaceholders(ctx, ps, name, e.Params)\n@@ -1871,7 +1870,7 @@ func (ex *connExecutor) execStmtInOpenStateWithPausablePortal(\n \t\t// This is handling the SQL statement \"PREPARE\". See execPrepare for\n \t\t// handling of the protocol-level command for preparing statements.\n \t\tname := s.Name.String()\n-\t\tif _, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[name]; ok {\n+\t\tif ex.extraTxnState.prepStmtsNamespace.prepStmts.Has(name) {\n \t\t\terr := pgerror.Newf(\n \t\t\t\tpgcode.DuplicatePreparedStatement,\n \t\t\t\t\"prepared statement %q already exists\", name,\n@@ -1932,7 +1931,7 @@ func (ex *connExecutor) execStmtInOpenStateWithPausablePortal(\n \t\t\tp.extendedEvalCtx.Placeholders = oldPlaceholders\n \t\t}()\n \t\tif _, err := ex.addPreparedStmt(\n-\t\t\tctx, name, prepStmt, typeHints, rawTypeHints, PreparedStatementOriginSQL,\n+\t\t\tctx, name, prepStmt, typeHints, rawTypeHints, prep.StatementOriginSQL,\n \t\t); err != nil {\n \t\t\treturn makeErrEvent(err)\n \t\t}\n@@ -2513,7 +2512,7 @@ func (ex *connExecutor) commitSQLTransactionInternal(ctx context.Context) (retEr\n \t\treturn err\n \t}\n \n-\tex.extraTxnState.prepStmtsNamespace.closeAllPortals(ctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc)\n+\tex.extraTxnState.prepStmtsNamespace.closePortals(ctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc)\n \n \t// We need to step the transaction's internal read sequence before committing\n \t// if it has stepping enabled. If it doesn't have stepping enabled, then we\n@@ -2643,7 +2642,7 @@ func (ex *connExecutor) rollbackSQLTransaction(\n \t\treturn ex.makeErrEvent(err, stmt)\n \t}\n \n-\tex.extraTxnState.prepStmtsNamespace.closeAllPortals(ctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc)\n+\tex.extraTxnState.prepStmtsNamespace.closePortals(ctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc)\n \tex.recordDDLTxnTelemetry(true /* failed */)\n \n \t// A non-retryable error automatically rolls-back the transaction if there are\n@@ -4427,10 +4426,7 @@ func logTraceAboveThreshold(\n }\n \n func (ex *connExecutor) execWithProfiling(\n-\tctx context.Context,\n-\tast tree.Statement,\n-\tprepared *PreparedStatement,\n-\top func(context.Context) error,\n+\tctx context.Context, ast tree.Statement, prepared *prep.Statement, op func(context.Context) error,\n ) error {\n \tvar err error\n \tif ex.server.cfg.Settings.CPUProfileType() == cluster.CPUProfileWithLabels {\ndiff --git a/pkg/sql/conn_executor_prepare.go b/pkg/sql/conn_executor_prepare.go\nindex f17c5d30b580..7c357087c3bb 100644\n--- a/pkg/sql/conn_executor_prepare.go\n+++ b/pkg/sql/conn_executor_prepare.go\n@@ -12,7 +12,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgcode\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgwirebase\"\n-\t\"github.com/cockroachdb/cockroach/pkg/sql/querycache\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/tree\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sessiondata\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sqlerrors\"\n@@ -20,7 +20,6 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/util/fsm\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/humanizeutil\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/log\"\n-\t\"github.com/cockroachdb/cockroach/pkg/util/timeutil\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/tracing\"\n \t\"github.com/cockroachdb/errors\"\n \t\"github.com/cockroachdb/redact\"\n@@ -61,7 +60,7 @@ func (ex *connExecutor) execPrepare(\n \n \t// The anonymous statement can be overwritten.\n \tif parseCmd.Name != \"\" {\n-\t\tif _, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[parseCmd.Name]; ok {\n+\t\tif ex.extraTxnState.prepStmtsNamespace.prepStmts.Has(parseCmd.Name) {\n \t\t\terr := pgerror.Newf(\n \t\t\t\tpgcode.DuplicatePreparedStatement,\n \t\t\t\t\"prepared statement %q already exists\", parseCmd.Name,\n@@ -81,7 +80,7 @@ func (ex *connExecutor) execPrepare(\n \t\tstmt,\n \t\tparseCmd.TypeHints,\n \t\tparseCmd.RawTypeHints,\n-\t\tPreparedStatementOriginWire,\n+\t\tprep.StatementOriginWire,\n \t)\n \tif err != nil {\n \t\treturn retErr(err)\n@@ -90,7 +89,7 @@ func (ex *connExecutor) execPrepare(\n \treturn nil, nil\n }\n \n-// addPreparedStmt creates a new PreparedStatement with the provided name using\n+// addPreparedStmt creates a new prep.Statement with the provided name using\n // the given query. The new prepared statement is added to the connExecutor and\n // also returned. It is illegal to call this when a statement with that name\n // already exists (even for anonymous prepared statements).\n@@ -104,9 +103,9 @@ func (ex *connExecutor) addPreparedStmt(\n \tstmt Statement,\n \tplaceholderHints tree.PlaceholderTypes,\n \trawTypeHints []oid.Oid,\n-\torigin PreparedStatementOrigin,\n-) (*PreparedStatement, error) {\n-\tif _, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[name]; ok {\n+\torigin prep.StatementOrigin,\n+) (*prep.Statement, error) {\n+\tif ex.extraTxnState.prepStmtsNamespace.prepStmts.Has(name) {\n \t\treturn nil, pgerror.Newf(\n \t\t\tpgcode.DuplicatePreparedStatement,\n \t\t\t\"prepared statement %q already exists\", name,\n@@ -120,36 +119,17 @@ func (ex *connExecutor) addPreparedStmt(\n \t}\n \n \tif len(prepared.TypeHints) > pgwirebase.MaxPreparedStatementArgs {\n-\t\tprepared.memAcc.Close(ctx)\n+\t\tprepared.MemAcc().Close(ctx)\n \t\treturn nil, pgwirebase.NewProtocolViolationErrorf(\n \t\t\t\"more than %d arguments to prepared statement: %d\",\n \t\t\tpgwirebase.MaxPreparedStatementArgs, len(prepared.TypeHints))\n \t}\n \n-\tif err := prepared.memAcc.Grow(ctx, int64(len(name))); err != nil {\n-\t\tprepared.memAcc.Close(ctx)\n+\tif err := prepared.MemAcc().Grow(ctx, int64(len(name))); err != nil {\n+\t\tprepared.MemAcc().Close(ctx)\n \t\treturn nil, err\n \t}\n-\tex.extraTxnState.prepStmtsNamespace.prepStmts[name] = prepared\n-\tex.extraTxnState.prepStmtsNamespace.addLRUEntry(name, prepared.memAcc.Allocated())\n-\n-\t// Check if we're over prepared_statements_cache_size.\n-\tcacheSize := ex.sessionData().PreparedStatementsCacheSize\n-\tif cacheSize != 0 {\n-\t\tlru := ex.extraTxnState.prepStmtsNamespace.prepStmtsLRU\n-\t\t// While we're over the cache size, deallocate the LRU prepared statement.\n-\t\tfor tail := lru[prepStmtsLRUTail]; tail.prev != prepStmtsLRUHead && tail.prev != name; tail = lru[prepStmtsLRUTail] {\n-\t\t\tif ex.extraTxnState.prepStmtsNamespace.prepStmtsLRUAlloc <= cacheSize {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\tlog.VEventf(\n-\t\t\t\tctx, 1,\n-\t\t\t\t\"prepared statements are using more than prepared_statements_cache_size (%s), \"+\n-\t\t\t\t\t\"automatically deallocating %s\", string(humanizeutil.IBytes(cacheSize)), tail.prev,\n-\t\t\t)\n-\t\t\tex.deletePreparedStmt(ctx, tail.prev)\n-\t\t}\n-\t}\n+\tex.extraTxnState.prepStmtsNamespace.prepStmts.Add(name, prepared, prepared.MemAcc().Allocated())\n \n \t// Remember the inferred placeholder types so they can be reported on\n \t// Describe. First, try to preserve the hints sent by the client.\n@@ -177,20 +157,14 @@ func (ex *connExecutor) prepare(\n \tstmt Statement,\n \tplaceholderHints tree.PlaceholderTypes,\n \trawTypeHints []oid.Oid,\n-\torigin PreparedStatementOrigin,\n-) (_ *PreparedStatement, retErr error) {\n-\n-\tprepared := &PreparedStatement{\n-\t\tmemAcc:   ex.sessionPreparedMon.MakeBoundAccount(),\n-\t\trefCount: 1,\n+\torigin prep.StatementOrigin,\n+) (_ *prep.Statement, retErr error) {\n \n-\t\tcreatedAt: timeutil.Now(),\n-\t\torigin:    origin,\n-\t}\n+\tprepared := prep.NewStatement(origin, ex.sessionPreparedMon.MakeBoundAccount())\n \tdefer func() {\n \t\t// Make sure to close the memory account if an error is returned.\n \t\tif retErr != nil {\n-\t\t\tprepared.memAcc.Close(ctx)\n+\t\t\tprepared.MemAcc().Close(ctx)\n \t\t}\n \t}()\n \n@@ -215,7 +189,7 @@ func (ex *connExecutor) prepare(\n \tvar flags planFlags\n \tprepare := func(ctx context.Context, txn *kv.Txn) (err error) {\n \t\tp := &ex.planner\n-\t\tif origin == PreparedStatementOriginWire {\n+\t\tif origin == prep.StatementOriginWire {\n \t\t\t// If the PREPARE command was issued as a SQL statement or through\n \t\t\t// deserialize_session, then we have already reset the planner at the very\n \t\t\t// beginning of the execution (in execStmtInOpenState). We might have also\n@@ -250,7 +224,7 @@ func (ex *connExecutor) prepare(\n \t\t\t}\n \t\t}\n \n-\t\tprepared.PrepareMetadata = querycache.PrepareMetadata{\n+\t\tprepared.Metadata = prep.Metadata{\n \t\t\tPlaceholderTypesInfo: tree.PlaceholderTypesInfo{\n \t\t\t\tTypeHints: placeholderHints,\n \t\t\t\tTypes:     placeholderHints,\n@@ -282,7 +256,7 @@ func (ex *connExecutor) prepare(\n \n \t// Use the existing transaction.\n \tif err := prepare(ctx, ex.state.mu.txn); err != nil {\n-\t\tif origin != PreparedStatementOriginSessionMigration {\n+\t\tif origin != prep.StatementOriginSessionMigration {\n \t\t\treturn nil, err\n \t\t} else {\n \t\t\tf := tree.NewFmtCtx(tree.FmtMarkRedactionNode | tree.FmtOmitNameRedaction | tree.FmtSimple)\n@@ -293,7 +267,7 @@ func (ex *connExecutor) prepare(\n \t}\n \n \t// Account for the memory used by this prepared statement.\n-\tif err := prepared.memAcc.Grow(ctx, prepared.MemoryEstimate()); err != nil {\n+\tif err := prepared.MemAcc().Grow(ctx, prepared.MemoryEstimate()); err != nil {\n \t\treturn nil, err\n \t}\n \tex.updateOptCounters(flags)\n@@ -307,7 +281,7 @@ func (ex *connExecutor) populatePrepared(\n \ttxn *kv.Txn,\n \tplaceholderHints tree.PlaceholderTypes,\n \tp *planner,\n-\torigin PreparedStatementOrigin,\n+\torigin prep.StatementOrigin,\n ) (planFlags, error) {\n \tif before := ex.server.cfg.TestingKnobs.BeforePrepare; before != nil {\n \t\tif err := before(ctx, ex.planner.stmt.String(), txn); err != nil {\n@@ -323,7 +297,7 @@ func (ex *connExecutor) populatePrepared(\n \t// there is no way for the statement being prepared to be executed in this\n \t// transaction, so there's no need to fix the timestamp, unlike how we must\n \t// for pgwire- or SQL-level prepared statements.\n-\tif origin != PreparedStatementOriginSessionMigration {\n+\tif origin != prep.StatementOriginSessionMigration {\n \t\tif err := ex.handleAOST(ctx, p.stmt.AST); err != nil {\n \t\t\treturn 0, err\n \t\t}\n@@ -349,7 +323,7 @@ func (ex *connExecutor) populatePrepared(\n func (ex *connExecutor) execBind(\n \tctx context.Context, bindCmd BindStmt,\n ) (fsm.Event, fsm.EventPayload) {\n-\tvar ps *PreparedStatement\n+\tvar ps *prep.Statement\n \tretErr := func(err error) (fsm.Event, fsm.EventPayload) {\n \t\tif bindCmd.PreparedStatementName != \"\" {\n \t\t\terr = errors.WithDetailf(err, \"statement name %q\", bindCmd.PreparedStatementName)\n@@ -364,13 +338,11 @@ func (ex *connExecutor) execBind(\n \t}\n \n \tvar ok bool\n-\tps, ok = ex.extraTxnState.prepStmtsNamespace.prepStmts[bindCmd.PreparedStatementName]\n+\tps, ok = ex.extraTxnState.prepStmtsNamespace.prepStmts.Get(bindCmd.PreparedStatementName)\n \tif !ok {\n \t\treturn retErr(newPreparedStmtDNEError(ex.sessionData(), bindCmd.PreparedStatementName))\n \t}\n \n-\tex.extraTxnState.prepStmtsNamespace.touchLRUEntry(bindCmd.PreparedStatementName)\n-\n \t// We need to make sure type resolution happens within a transaction.\n \t// Otherwise, for user-defined types we won't take the correct leases and\n \t// will get back out of date type information.\n@@ -559,7 +531,7 @@ func (ex *connExecutor) execBind(\n func (ex *connExecutor) addPortal(\n \tctx context.Context,\n \tportalName string,\n-\tstmt *PreparedStatement,\n+\tstmt *prep.Statement,\n \tqargs tree.QueryArguments,\n \toutFormats []pgwirebase.FormatCode,\n ) error {\n@@ -591,14 +563,7 @@ func (ex *connExecutor) exhaustPortal(portalName string) {\n }\n \n func (ex *connExecutor) deletePreparedStmt(ctx context.Context, name string) {\n-\tps, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[name]\n-\tif !ok {\n-\t\treturn\n-\t}\n-\talloc := ps.memAcc.Allocated()\n-\tps.decRef(ctx)\n-\tdelete(ex.extraTxnState.prepStmtsNamespace.prepStmts, name)\n-\tex.extraTxnState.prepStmtsNamespace.delLRUEntry(name, alloc)\n+\tex.getPrepStmtsAccessor().Delete(ctx, name)\n }\n \n func (ex *connExecutor) deletePortal(ctx context.Context, name string) {\n@@ -615,8 +580,7 @@ func (ex *connExecutor) execDelPrepStmt(\n ) (fsm.Event, fsm.EventPayload) {\n \tswitch delCmd.Type {\n \tcase pgwirebase.PrepareStatement:\n-\t\t_, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[delCmd.Name]\n-\t\tif !ok {\n+\t\tif !ex.extraTxnState.prepStmtsNamespace.prepStmts.Has(delCmd.Name) {\n \t\t\t// The spec says \"It is not an error to issue Close against a nonexistent\n \t\t\t// statement or portal name\". See\n \t\t\t// https://www.postgresql.org/docs/current/static/protocol-flow.html.\n@@ -647,19 +611,18 @@ func (ex *connExecutor) execDescribe(\n \n \tswitch descCmd.Type {\n \tcase pgwirebase.PrepareStatement:\n-\t\tps, ok := ex.extraTxnState.prepStmtsNamespace.prepStmts[string(descCmd.Name)]\n+\t\tprepStmts := ex.extraTxnState.prepStmtsNamespace.prepStmts\n+\t\tps, ok := prepStmts.Get(string(descCmd.Name))\n \t\tif !ok {\n \t\t\treturn retErr(newPreparedStmtDNEError(ex.sessionData(), string(descCmd.Name)))\n \t\t}\n-\t\t// Not currently counting this as an LRU touch on prepStmtsLRU for\n-\t\t// prepared_statements_cache_size (but maybe we should?).\n \n \t\tast := ps.AST\n \t\tif execute, ok := ast.(*tree.Execute); ok {\n \t\t\t// If we're describing an EXECUTE, we need to look up the statement type\n \t\t\t// of the prepared statement that the EXECUTE refers to, or else we'll\n \t\t\t// return the wrong information for describe.\n-\t\t\tinnerPs, found := ex.extraTxnState.prepStmtsNamespace.prepStmts[string(execute.Name)]\n+\t\t\tinnerPs, found := prepStmts.Get(string(execute.Name))\n \t\t\tif !found {\n \t\t\t\treturn retErr(newPreparedStmtDNEError(ex.sessionData(), string(execute.Name)))\n \t\t\t}\ndiff --git a/pkg/sql/execute.go b/pkg/sql/execute.go\nindex bd00023e871a..bc7940861ed5 100644\n--- a/pkg/sql/execute.go\n+++ b/pkg/sql/execute.go\n@@ -11,6 +11,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/catalog/schemaexpr\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgcode\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/tree\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/volatility\"\n \t\"github.com/cockroachdb/errors\"\n@@ -21,7 +22,7 @@ import (\n // the referenced prepared statement and correctly updated placeholder info.\n // See https://www.postgresql.org/docs/current/static/sql-execute.html for details.\n func (p *planner) fillInPlaceholders(\n-\tctx context.Context, ps *PreparedStatement, name string, params tree.Exprs,\n+\tctx context.Context, ps *prep.Statement, name string, params tree.Exprs,\n ) (*tree.PlaceholderInfo, error) {\n \tif len(ps.Types) != len(params) {\n \t\treturn nil, pgerror.Newf(pgcode.Syntax,\ndiff --git a/pkg/sql/faketreeeval/evalctx.go b/pkg/sql/faketreeeval/evalctx.go\nindex 2f317243961a..a1c0dcb918c9 100644\n--- a/pkg/sql/faketreeeval/evalctx.go\n+++ b/pkg/sql/faketreeeval/evalctx.go\n@@ -705,8 +705,11 @@ func (ps *DummyPreparedStatementState) HasActivePortals() bool {\n }\n \n // MigratablePreparedStatements is part of the tree.PreparedStatementState interface.\n-func (ps *DummyPreparedStatementState) MigratablePreparedStatements() []sessiondatapb.MigratableSession_PreparedStatement {\n-\treturn nil\n+func (ps *DummyPreparedStatementState) MigratablePreparedStatements() (\n+\t[]sessiondatapb.MigratableSession_PreparedStatement,\n+\terror,\n+) {\n+\treturn nil, nil\n }\n \n // HasPortal is part of the tree.PreparedStatementState interface.\ndiff --git a/pkg/sql/pg_catalog.go b/pkg/sql/pg_catalog.go\nindex 156ad285f786..27cdf16b4eb2 100644\n--- a/pkg/sql/pg_catalog.go\n+++ b/pkg/sql/pg_catalog.go\n@@ -35,6 +35,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/oidext\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgcode\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/privilege\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/builtins\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/builtins/builtinsregistry\"\n@@ -2482,7 +2483,7 @@ https://www.postgresql.org/docs/9.6/view-pg-prepared-statements.html`,\n \tschema: vtable.PGCatalogPreparedStatements,\n \tpopulate: func(ctx context.Context, p *planner, dbContext catalog.DatabaseDescriptor, addRow func(...tree.Datum) error) error {\n \t\tfor name, stmt := range p.preparedStatements.List() {\n-\t\t\tplaceholderTypes := stmt.PrepareMetadata.PlaceholderTypesInfo.Types\n+\t\t\tplaceholderTypes := stmt.Metadata.PlaceholderTypesInfo.Types\n \t\t\tparamTypes := tree.NewDArray(types.RegType)\n \t\t\tparamTypes.Array = make(tree.Datums, len(placeholderTypes))\n \t\t\tparamNames := make([]string, len(placeholderTypes))\n@@ -2503,11 +2504,11 @@ https://www.postgresql.org/docs/9.6/view-pg-prepared-statements.html`,\n \t\t\t}\n \n \t\t\tfromSQL := tree.DBoolFalse\n-\t\t\tif stmt.origin == PreparedStatementOriginSQL {\n+\t\t\tif stmt.Origin() == prep.StatementOriginSQL {\n \t\t\t\tfromSQL = tree.DBoolTrue\n \t\t\t}\n \n-\t\t\tts, err := tree.MakeDTimestampTZ(stmt.createdAt, time.Microsecond)\n+\t\t\tts, err := tree.MakeDTimestampTZ(stmt.CreatedAt(), time.Microsecond)\n \t\t\tif err != nil {\n \t\t\t\treturn err\n \t\t\t}\ndiff --git a/pkg/sql/plan_opt.go b/pkg/sql/plan_opt.go\nindex 3255bc69484e..4527f4f55834 100644\n--- a/pkg/sql/plan_opt.go\n+++ b/pkg/sql/plan_opt.go\n@@ -28,6 +28,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgwirebase\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/physicalplan\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/querycache\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/eval\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/tree\"\n@@ -52,13 +53,13 @@ var queryCacheEnabled = settings.RegisterBoolSetting(\n //   - AnonymizedStr\n //   - BaseMemo (for reuse during exec, if appropriate).\n func (p *planner) prepareUsingOptimizer(\n-\tctx context.Context, origin PreparedStatementOrigin,\n+\tctx context.Context, origin prep.StatementOrigin,\n ) (planFlags, error) {\n \tstmt := &p.stmt\n \n \topc := &p.optPlanningCtx\n \topc.reset(ctx)\n-\tif origin == PreparedStatementOriginSessionMigration {\n+\tif origin == prep.StatementOriginSessionMigration {\n \t\topc.flags.Set(planFlagSessionMigration)\n \t}\n \n@@ -94,7 +95,7 @@ func (p *planner) prepareUsingOptimizer(\n \t\t// we need to set the expected output columns to the output columns of the\n \t\t// prepared statement that the user is trying to execute.\n \t\tname := string(t.Name)\n-\t\tprepared, ok := p.preparedStatements.Get(name, true /* touchLRU */)\n+\t\tprepared, ok := p.preparedStatements.Get(name)\n \t\tif !ok {\n \t\t\t// We're trying to prepare an EXECUTE of a statement that doesn't exist.\n \t\t\t// Let's just give up at this point.\n@@ -133,8 +134,8 @@ func (p *planner) prepareUsingOptimizer(\n \n \tif opc.useCache {\n \t\tcachedData, ok := p.execCfg.QueryCache.Find(&p.queryCacheSession, stmt.SQL)\n-\t\tif ok && cachedData.PrepareMetadata != nil {\n-\t\t\tpm := cachedData.PrepareMetadata\n+\t\tif ok && cachedData.Metadata != nil {\n+\t\t\tpm := cachedData.Metadata\n \t\t\t// Check that the type hints match (the type hints affect type checking).\n \t\t\tif !pm.TypeHints.Identical(p.semaCtx.Placeholders.TypeHints) {\n \t\t\t\topc.log(ctx, \"query cache hit but type hints don't match\")\n@@ -229,17 +230,17 @@ func (p *planner) prepareUsingOptimizer(\n \t\t\tstmt.Prepared.BaseMemo = memo\n \t\t}\n \t\tif opc.useCache {\n-\t\t\t// execPrepare sets the PrepareMetadata.InferredTypes field after this\n-\t\t\t// point. However, once the PrepareMetadata goes into the cache, it\n+\t\t\t// execPrepare sets the Metadata.InferredTypes field after this\n+\t\t\t// point. However, once the Metadata goes into the cache, it\n \t\t\t// can't be modified without causing race conditions. So make a copy of\n \t\t\t// it now.\n \t\t\t// TODO(radu): Determine if the extra object allocation is really\n \t\t\t// necessary.\n-\t\t\tpm := stmt.Prepared.PrepareMetadata\n+\t\t\tpm := stmt.Prepared.Metadata\n \t\t\tcachedData := querycache.CachedData{\n-\t\t\t\tSQL:             stmt.SQL,\n-\t\t\t\tMemo:            memo,\n-\t\t\t\tPrepareMetadata: &pm,\n+\t\t\t\tSQL:      stmt.SQL,\n+\t\t\t\tMemo:     memo,\n+\t\t\t\tMetadata: &pm,\n \t\t\t}\n \t\t\tp.execCfg.QueryCache.Add(&p.queryCacheSession, &cachedData)\n \t\t}\n@@ -610,14 +611,14 @@ func (opc *optPlanningCtx) incPlanTypeTelemetry(cachedMemo *memo.Memo) {\n // buildNonIdealGenericPlan returns true if we should attempt to build a\n // non-ideal generic query plan.\n func (opc *optPlanningCtx) buildNonIdealGenericPlan() bool {\n-\tprep := opc.p.stmt.Prepared\n+\tps := opc.p.stmt.Prepared\n \tswitch opc.p.SessionData().PlanCacheMode {\n \tcase sessiondatapb.PlanCacheModeForceGeneric:\n \t\treturn true\n \tcase sessiondatapb.PlanCacheModeAuto:\n \t\t// We need to build CustomPlanThreshold custom plans before considering\n \t\t// a generic plan.\n-\t\treturn prep.Costs.NumCustom() >= CustomPlanThreshold\n+\t\treturn ps.Costs.NumCustom() >= prep.CustomPlanThreshold\n \tdefault:\n \t\treturn false\n \t}\n@@ -628,17 +629,17 @@ func (opc *optPlanningCtx) buildNonIdealGenericPlan() bool {\n // plan is chosen if CustomPlanThreshold custom plans have already been built\n // and the generic plan is optimal or it has not yet been built.\n func (opc *optPlanningCtx) chooseGenericPlan() bool {\n-\tprep := opc.p.stmt.Prepared\n+\tps := opc.p.stmt.Prepared\n \t// Always use an ideal generic plan.\n-\tif prep.IdealGenericPlan {\n+\tif ps.IdealGenericPlan {\n \t\treturn true\n \t}\n \tswitch opc.p.SessionData().PlanCacheMode {\n \tcase sessiondatapb.PlanCacheModeForceGeneric:\n \t\treturn true\n \tcase sessiondatapb.PlanCacheModeAuto:\n-\t\treturn prep.Costs.NumCustom() >= CustomPlanThreshold &&\n-\t\t\t(!prep.Costs.HasGeneric() || prep.Costs.IsGenericOptimal())\n+\t\treturn ps.Costs.NumCustom() >= prep.CustomPlanThreshold &&\n+\t\t\t(!ps.Costs.HasGeneric() || ps.Costs.IsGenericOptimal())\n \tdefault:\n \t\treturn false\n \t}\n@@ -821,9 +822,9 @@ func (opc *optPlanningCtx) buildExecMemo(ctx context.Context) (_ *memo.Memo, _ e\n \t\t\t\tif err != nil {\n \t\t\t\t\treturn nil, err\n \t\t\t\t}\n-\t\t\t\t// Update the plan in the cache. If the cache entry had PrepareMetadata\n+\t\t\t\t// Update the plan in the cache. If the cache entry had Metadata\n \t\t\t\t// populated, it may no longer be valid.\n-\t\t\t\tcachedData.PrepareMetadata = nil\n+\t\t\t\tcachedData.Metadata = nil\n \t\t\t\tp.execCfg.QueryCache.Add(&p.queryCacheSession, &cachedData)\n \t\t\t\topc.flags.Set(planFlagOptCacheMiss)\n \t\t\t} else {\ndiff --git a/pkg/sql/planner.go b/pkg/sql/planner.go\nindex 3d8a37d0c5a3..e5132f5cc1aa 100644\n--- a/pkg/sql/planner.go\n+++ b/pkg/sql/planner.go\n@@ -35,6 +35,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/exprutil\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/idxusage\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/parser\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/privilege\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/querycache\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/regions\"\n@@ -801,8 +802,8 @@ type statementPreparer interface {\n \t\tstmt Statement,\n \t\tplaceholderHints tree.PlaceholderTypes,\n \t\trawTypeHints []oid.Oid,\n-\t\torigin PreparedStatementOrigin,\n-\t) (*PreparedStatement, error)\n+\t\torigin prep.StatementOrigin,\n+\t) (*prep.Statement, error)\n }\n \n var _ statementPreparer = &connExecutor{}\ndiff --git a/pkg/sql/prep/BUILD.bazel b/pkg/sql/prep/BUILD.bazel\nnew file mode 100644\nindex 000000000000..e5dfe221d624\n--- /dev/null\n+++ b/pkg/sql/prep/BUILD.bazel\n@@ -0,0 +1,41 @@\n+load(\"@io_bazel_rules_go//go:def.bzl\", \"go_library\", \"go_test\")\n+\n+go_library(\n+    name = \"prep\",\n+    srcs = [\n+        \"cache.go\",\n+        \"metadata.go\",\n+        \"statement.go\",\n+    ],\n+    importpath = \"github.com/cockroachdb/cockroach/pkg/sql/prep\",\n+    visibility = [\"//visibility:public\"],\n+    deps = [\n+        \"//pkg/sql/catalog/colinfo\",\n+        \"//pkg/sql/opt/memo\",\n+        \"//pkg/sql/parser/statements\",\n+        \"//pkg/sql/sem/tree\",\n+        \"//pkg/sql/types\",\n+        \"//pkg/util/log\",\n+        \"//pkg/util/mon\",\n+        \"//pkg/util/timeutil\",\n+        \"@com_github_cockroachdb_errors//:errors\",\n+        \"@com_github_lib_pq//oid\",\n+    ],\n+)\n+\n+go_test(\n+    name = \"prep_test\",\n+    srcs = [\n+        \"cache_test.go\",\n+        \"statement_test.go\",\n+    ],\n+    embed = [\":prep\"],\n+    deps = [\n+        \"//pkg/sql/opt/memo\",\n+        \"//pkg/util/leaktest\",\n+        \"//pkg/util/log\",\n+        \"//pkg/util/randutil\",\n+        \"@com_github_cockroachdb_errors//:errors\",\n+        \"@org_golang_x_exp//maps\",\n+    ],\n+)\ndiff --git a/pkg/sql/prep/cache.go b/pkg/sql/prep/cache.go\nnew file mode 100644\nindex 000000000000..2233051de58e\n--- /dev/null\n+++ b/pkg/sql/prep/cache.go\n@@ -0,0 +1,301 @@\n+// Copyright 2025 The Cockroach Authors.\n+//\n+// Use of this software is governed by the CockroachDB Software License\n+// included in the /LICENSE file.\n+\n+package prep\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/cockroachdb/errors\"\n+)\n+\n+// Cache maps prepared statement names to Statements. In order to facilitate\n+// automatic retries which rewind the command buffer, Cache implements the\n+// Rewind method to restore the Cache to its state at the time of the last\n+// Commit.\n+//\n+// Cache applies an LRU eviction policy during Commit. See Commit for more\n+// details.\n+//\n+// Cache is optimized for the most common interactions in most workloads:\n+//\n+//  1. Fetching prepared statements with Get.\n+//  2. Snapshotting the Cache with Commit when no changes have been applied\n+//     since the last Commit.\n+//\n+// Cache is not thread-safe.\n+type Cache struct {\n+\tm map[string]*entry\n+\t// \"uses\" contains all committed statements ordered from most recently used\n+\t// to least recently used.\n+\tuses list\n+\t// tape records calls to Add, Remove, and Get since the last Commit or\n+\t// Rewind.\n+\ttape []event\n+\tsize int64\n+}\n+\n+type entry struct {\n+\tname       string\n+\tstmt       *Statement\n+\tprev, next *entry\n+\tsize       int64\n+}\n+\n+type op uint8\n+\n+const (\n+\tadd op = iota\n+\tremove\n+\tget\n+)\n+\n+// event represent an Add, Remove, or Get in the tape.\n+type event struct {\n+\te *entry\n+\top\n+}\n+\n+// Init initializes an empty cache. If the cache contains any statements, they\n+// are cleared.\n+func (c *Cache) Init(ctx context.Context) {\n+\t// Clear the tape and decrement added statement reference counts.\n+\tfor i := range c.tape {\n+\t\tif c.tape[i].op == remove {\n+\t\t\t// Remove deletes the statement from the map without decrementing\n+\t\t\t// its reference count, so it must be decremented here.\n+\t\t\tc.tape[i].e.stmt.DecRef(ctx)\n+\t\t}\n+\t\tc.tape[i] = event{}\n+\t}\n+\t// Clear the map and decrement the statement reference counts.\n+\tfor name, e := range c.m {\n+\t\te.stmt.DecRef(ctx)\n+\t\tdelete(c.m, name)\n+\t}\n+\t// Reuse the tape and the map.\n+\t*c = Cache{\n+\t\tm:    c.m,\n+\t\ttape: c.tape[:0],\n+\t}\n+\t// Initialize the map and \"uses\" list.\n+\tif c.m == nil {\n+\t\tc.m = make(map[string]*entry)\n+\t}\n+\tc.uses.init()\n+}\n+\n+// Add adds a statement to the cache with the given name.\n+//\n+// NOTE: Panics if a statement with the given name already exists.\n+func (c *Cache) Add(name string, stmt *Statement, size int64) {\n+\tif _, ok := c.m[name]; ok {\n+\t\tpanic(errors.AssertionFailedf(\"cache entry %q already exists\", name))\n+\t}\n+\te := &entry{\n+\t\tname: name,\n+\t\tstmt: stmt,\n+\t\tsize: size,\n+\t}\n+\tc.m[name] = e\n+\tc.size += size\n+\tc.tape = append(c.tape, event{e, add})\n+}\n+\n+// Remove removes the statement with the given name from the cache. No-op if the\n+// name does not already exist in the cache.\n+func (c *Cache) Remove(name string) {\n+\te, ok := c.m[name]\n+\tif !ok {\n+\t\treturn\n+\t}\n+\tdelete(c.m, name)\n+\tc.tape = append(c.tape, event{e, remove})\n+\tc.size -= e.size\n+}\n+\n+// Has returns true if the prepared statement with the given name is in the\n+// cache. It is not considered an access and does not affect the ordering of\n+// statement eviction during Commit.\n+func (c *Cache) Has(name string) bool {\n+\t_, ok := c.m[name]\n+\treturn ok\n+}\n+\n+// Get looks up a prepared statement with the given name. It is considered an\n+// access so it affects the ordering of statement eviction during Commit.\n+func (c *Cache) Get(name string) (stmt *Statement, ok bool) {\n+\te, ok := c.m[name]\n+\tif !ok {\n+\t\treturn nil, false\n+\t}\n+\tc.tape = append(c.tape, event{e, get})\n+\treturn e.stmt, true\n+}\n+\n+// Len returns the number of prepared statements in the cache.\n+func (c *Cache) Len() int {\n+\treturn len(c.m)\n+}\n+\n+// Size returns the total size of the statements in the cache.\n+func (c *Cache) Size() int64 {\n+\treturn c.size\n+}\n+\n+// Commit makes permanent any changes to the cache via Add or Remove since the\n+// last Commit. Those changes can no longer be undone with Rewind.\n+//\n+// If maxSize is positive, Commit will evict prepared statements in the cache\n+// until the total size of statements is less than maxSize or until there is\n+// only one remaining statement in the cache, whichever comes first. The least\n+// recently accessed statements are evicted first, where Add and Get are\n+// considered accesses.\n+//\n+// The time complexity of Commit scales linearly with the number of changes made\n+// to the cache since the last Commit.\n+func (c *Cache) Commit(ctx context.Context, maxSize int64) (evicted []string) {\n+\t// Apply the events in the tape to the \"uses\" list and clear the tape.\n+\tfor i := range c.tape {\n+\t\tswitch c.tape[i].op {\n+\t\tcase add:\n+\t\t\tc.uses.push(c.tape[i].e)\n+\t\tcase remove:\n+\t\t\tc.tape[i].e.stmt.DecRef(ctx)\n+\t\t\tc.uses.remove(c.tape[i].e)\n+\t\tcase get:\n+\t\t\tc.uses.remove(c.tape[i].e)\n+\t\t\tc.uses.push(c.tape[i].e)\n+\t\tdefault:\n+\t\t\tpanic(errors.AssertionFailedf(\"unexpected op %d\", c.tape[i].op))\n+\t\t}\n+\t\tc.tape[i] = event{}\n+\t}\n+\tc.tape = c.tape[:0]\n+\t// Evict statements, if necessary, until the size is less than maxSize or\n+\t// there is only one statement in the cache.\n+\tfor e := c.uses.tail(); e != nil && e != c.uses.head() && maxSize > 0 && c.size > maxSize; {\n+\t\tprev := c.uses.prev(e)\n+\t\tdelete(c.m, e.name)\n+\t\tc.uses.remove(e)\n+\t\tc.size -= e.size\n+\t\tevicted = append(evicted, e.name)\n+\t\te.stmt.DecRef(ctx)\n+\t\te = prev\n+\t}\n+\treturn evicted\n+}\n+\n+// Rewind returns the cache to its state at the previous Commit time.\n+//\n+// Rewind is a constant-time operation if nothing has been added or removed\n+// since the last Commit or Rewind.\n+func (c *Cache) Rewind(ctx context.Context) {\n+\t// Undo the events in the tape in reverse order and clear the tape.\n+\tfor i := len(c.tape) - 1; i >= 0; i-- {\n+\t\te := c.tape[i].e\n+\t\tswitch c.tape[i].op {\n+\t\tcase add:\n+\t\t\tdelete(c.m, e.name)\n+\t\t\tc.size -= e.size\n+\t\t\te.stmt.DecRef(ctx)\n+\t\tcase remove:\n+\t\t\tc.m[e.name] = e\n+\t\t\tc.size += e.size\n+\t\tcase get:\n+\t\t\t// No-op. The \"uses\" list does not need to be reverted because it is\n+\t\t\t// only altered during Commit.\n+\t\t}\n+\t\tc.tape[i] = event{}\n+\t}\n+\tc.tape = c.tape[:0]\n+}\n+\n+// Dirty returns true if Add, Remove, or Get has been called on the cache since\n+// the last Commit or Rewind. Remove and Get calls with non-existent prepared\n+// statement names do not make the cache dirty.\n+func (c *Cache) Dirty() bool {\n+\treturn len(c.tape) > 0\n+}\n+\n+// ForEach calls fn on every statement in the cache. The order of the statements\n+// is non-deterministic.\n+func (c *Cache) ForEach(fn func(name string, stmt *Statement)) {\n+\tfor name, e := range c.m {\n+\t\tfn(name, e.stmt)\n+\t}\n+}\n+\n+// ForEachLRU calls fn on every statement in the cache in order from least\n+// recently used to most recently used. The cache must not be dirtyâ€”the ordering\n+// of \"uses\" is not maintained while the Cache is dirty so an LRU ordering\n+// cannot be provided.\n+//\n+// NOTE: Panics if the cache is dirty. This is required because the Cache does\n+// not keep track of LRU orderings of mutations until they are committed.\n+func (c *Cache) ForEachLRU(fn func(name string, stmt *Statement)) {\n+\tif c.Dirty() {\n+\t\tpanic(errors.AssertionFailedf(\"cannot iterate over dirty cache\"))\n+\t}\n+\tfor e := c.uses.tail(); e != nil; e = c.uses.prev(e) {\n+\t\tfn(e.name, e.stmt)\n+\t}\n+}\n+\n+// list is a doubly-linked list of entries.\n+type list struct {\n+\t// root is a sentinel value. root.next and root.prev are the head and the\n+\t// tail.\n+\troot entry\n+}\n+\n+// init initializes a list.\n+func (l *list) init() {\n+\tl.root.next = &l.root\n+\tl.root.prev = &l.root\n+}\n+\n+// head returns the first entry in the list or nil if the list is empty.\n+func (l *list) head() *entry {\n+\tif l.root.next == &l.root {\n+\t\treturn nil\n+\t}\n+\treturn l.root.next\n+}\n+\n+// tail returns the last entry in the list or nil if the list is empty.\n+func (l *list) tail() *entry {\n+\tif l.root.prev == &l.root {\n+\t\treturn nil\n+\t}\n+\treturn l.root.prev\n+}\n+\n+// prev returns the entry in the list before e or nil if one does not exist.\n+func (l *list) prev(e *entry) *entry {\n+\tif e.prev == &l.root {\n+\t\treturn nil\n+\t}\n+\treturn e.prev\n+}\n+\n+// push pushes the entry onto the front of the list. The entry must not already\n+// be in the list.\n+func (l *list) push(e *entry) {\n+\th := l.root.next\n+\te.next = h\n+\te.prev = h.prev\n+\th.prev = e\n+\tl.root.next = e\n+}\n+\n+// remove removes the entry from the list. The entry must already be in the\n+// list.\n+func (l *list) remove(e *entry) {\n+\te.prev.next = e.next\n+\te.next.prev = e.prev\n+\te.next = nil\n+\te.prev = nil\n+}\ndiff --git a/pkg/sql/querycache/prepared_statement.go b/pkg/sql/prep/metadata.go\nsimilarity index 91%\nrename from pkg/sql/querycache/prepared_statement.go\nrename to pkg/sql/prep/metadata.go\nindex 7454b98e5bb1..b8efed55c218 100644\n--- a/pkg/sql/querycache/prepared_statement.go\n+++ b/pkg/sql/prep/metadata.go\n@@ -3,7 +3,7 @@\n // Use of this software is governed by the CockroachDB Software License\n // included in the /LICENSE file.\n \n-package querycache\n+package prep\n \n import (\n \t\"unsafe\"\n@@ -15,9 +15,9 @@ import (\n \t\"github.com/lib/pq/oid\"\n )\n \n-// PrepareMetadata encapsulates information about a statement that is gathered\n+// Metadata encapsulates information about a statement that is gathered\n // during Prepare and is later used during Describe or Execute.\n-type PrepareMetadata struct {\n+type Metadata struct {\n \t// Note that AST may be nil if the prepared statement is empty.\n \tstatements.Statement[tree.Statement]\n \n@@ -42,7 +42,7 @@ type PrepareMetadata struct {\n \n // MemoryEstimate returns an estimation (in bytes) of how much memory is used by\n // the prepare metadata.\n-func (pm *PrepareMetadata) MemoryEstimate() int64 {\n+func (pm *Metadata) MemoryEstimate() int64 {\n \tres := int64(unsafe.Sizeof(*pm))\n \tres += int64(len(pm.SQL))\n \t// We don't have a good way of estimating the size of the AST. Just assume\ndiff --git a/pkg/sql/prep/statement.go b/pkg/sql/prep/statement.go\nnew file mode 100644\nindex 000000000000..879fec65eca9\n--- /dev/null\n+++ b/pkg/sql/prep/statement.go\n@@ -0,0 +1,221 @@\n+// Copyright 2025 The Cockroach Authors.\n+//\n+// Use of this software is governed by the CockroachDB Software License\n+// included in the /LICENSE file.\n+\n+package prep\n+\n+import (\n+\t\"context\"\n+\t\"time\"\n+\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/opt/memo\"\n+\t\"github.com/cockroachdb/cockroach/pkg/util/log\"\n+\t\"github.com/cockroachdb/cockroach/pkg/util/mon\"\n+\t\"github.com/cockroachdb/cockroach/pkg/util/timeutil\"\n+)\n+\n+// StatementOrigin is an enum representing the source of where\n+// the prepare statement was made.\n+type StatementOrigin int\n+\n+const (\n+\t// StatementOriginWire signifies the prepared statement was made\n+\t// over the wire.\n+\tStatementOriginWire StatementOrigin = iota + 1\n+\t// StatementOriginSQL signifies the prepared statement was made\n+\t// over a parsed SQL query.\n+\tStatementOriginSQL\n+\t// StatementOriginSessionMigration signifies that the prepared\n+\t// statement came from a call to crdb_internal.deserialize_session.\n+\tStatementOriginSessionMigration\n+)\n+\n+// Statement is a SQL statement that has been parsed and the types of arguments\n+// and results have been determined.\n+//\n+// Note that Statements maintain a reference counter internally. References need\n+// to be registered with incRef() and de-registered with decRef().\n+type Statement struct {\n+\tMetadata\n+\n+\t// BaseMemo is the memoized data structure constructed by the cost-based\n+\t// optimizer during prepare of a SQL statement.\n+\tBaseMemo *memo.Memo\n+\n+\t// GenericMemo, if present, is a fully-optimized memo that can be executed\n+\t// as-is.\n+\tGenericMemo *memo.Memo\n+\n+\t// IdealGenericPlan is true if GenericMemo is guaranteed to be optimal\n+\t// across all executions of the prepared statement. Ideal generic plans are\n+\t// generated when the statement has no placeholders nor fold-able stable\n+\t// expressions, or when the placeholder fast-path is utilized.\n+\tIdealGenericPlan bool\n+\n+\t// Costs tracks the costs of previously optimized custom and generic plans.\n+\tCosts planCosts\n+\n+\t// refCount keeps track of the number of references to this PreparedStatement.\n+\t// New references are registered through incRef().\n+\t// Once refCount hits 0 (through calls to decRef()), the following memAcc is\n+\t// closed.\n+\t// Most references are being held by portals created from this prepared\n+\t// statement.\n+\trefCount int\n+\tmemAcc   mon.BoundAccount\n+\n+\t// origin is the protocol in which this prepare statement was created. Used\n+\t// for reporting on `pg_prepared_statements`.\n+\torigin StatementOrigin\n+\t// createdAt is the timestamp this prepare statement was made at.\n+\t// Used for reporting on `pg_prepared_statements`.\n+\tcreatedAt time.Time\n+}\n+\n+// NewStatement initializes and returns a new Statement.\n+func NewStatement(origin StatementOrigin, memAcc mon.BoundAccount) *Statement {\n+\treturn &Statement{\n+\t\torigin:    origin,\n+\t\tmemAcc:    memAcc,\n+\t\trefCount:  1,\n+\t\tcreatedAt: timeutil.Now(),\n+\t}\n+}\n+\n+// Origin returns the origin the prepared statement.\n+func (p *Statement) Origin() StatementOrigin {\n+\treturn p.origin\n+}\n+\n+// MemAcc returns the bound account of the prepared statement.\n+func (p *Statement) MemAcc() *mon.BoundAccount {\n+\treturn &p.memAcc\n+}\n+\n+// CreatedAt returns the time that the prepared statement was created.\n+func (p *Statement) CreatedAt() time.Time {\n+\treturn p.createdAt\n+}\n+\n+// MemoryEstimate returns a rough estimate of the PreparedStatement's memory\n+// usage, in bytes.\n+func (p *Statement) MemoryEstimate() int64 {\n+\t// Account for the memory used by this prepared statement:\n+\t//   1. Size of the prepare metadata.\n+\t//   2. Size of the prepared memo, if using the cost-based optimizer.\n+\tsize := p.Metadata.MemoryEstimate()\n+\tif p.BaseMemo != nil {\n+\t\tsize += p.BaseMemo.MemoryEstimate()\n+\t}\n+\tif p.GenericMemo != nil {\n+\t\tsize += p.GenericMemo.MemoryEstimate()\n+\t}\n+\treturn size\n+}\n+\n+func (p *Statement) DecRef(ctx context.Context) {\n+\tif p.refCount <= 0 {\n+\t\tlog.Fatal(ctx, \"corrupt PreparedStatement refcount\")\n+\t}\n+\tp.refCount--\n+\tif p.refCount == 0 {\n+\t\tp.memAcc.Close(ctx)\n+\t}\n+}\n+\n+func (p *Statement) IncRef(ctx context.Context) {\n+\tif p.refCount <= 0 {\n+\t\tlog.Fatal(ctx, \"corrupt PreparedStatement refcount\")\n+\t}\n+\tp.refCount++\n+}\n+\n+const (\n+\t// CustomPlanThreshold is the maximum number of custom plan costs tracked by\n+\t// planCosts. It is also the number of custom plans executed when\n+\t// plan_cache_mode=auto before attempting to generate a generic plan.\n+\tCustomPlanThreshold = 5\n+)\n+\n+// planCosts tracks costs of generic and custom plans.\n+type planCosts struct {\n+\tgeneric memo.Cost\n+\tcustom  struct {\n+\t\tnextIdx int\n+\t\tlength  int\n+\t\tcosts   [CustomPlanThreshold]memo.Cost\n+\t}\n+}\n+\n+// HasGeneric returns true if the planCosts has a generic plan cost.\n+func (p *planCosts) HasGeneric() bool {\n+\treturn p.generic.C != 0\n+}\n+\n+// SetGeneric sets the cost of the generic plan.\n+func (p *planCosts) SetGeneric(cost memo.Cost) {\n+\tp.generic = cost\n+}\n+\n+// AddCustom adds a custom plan cost to the planCosts, evicting the oldest cost\n+// if necessary.\n+func (p *planCosts) AddCustom(cost memo.Cost) {\n+\tp.custom.costs[p.custom.nextIdx] = cost\n+\tp.custom.nextIdx++\n+\tif p.custom.nextIdx >= CustomPlanThreshold {\n+\t\tp.custom.nextIdx = 0\n+\t}\n+\tif p.custom.length < CustomPlanThreshold {\n+\t\tp.custom.length++\n+\t}\n+}\n+\n+// NumCustom returns the number of custom plan costs in the planCosts.\n+func (p *planCosts) NumCustom() int {\n+\treturn p.custom.length\n+}\n+\n+// IsGenericOptimal returns true if the generic plan is optimal w.r.t. the\n+// custom plans. The cost flags, auxiliary cost information, and the cost value\n+// are all considered. If any of the custom plan cost flags are less than the\n+// generic cost flags, then the generic plan is not optimal. If the generic plan\n+// has more full scans than any of the custom plans, then it is not optimal.\n+// Otherwise, the generic plan is optimal if its cost value is less than the\n+// average cost of the custom plans.\n+func (p *planCosts) IsGenericOptimal() bool {\n+\t// Check cost flags and full scan counts.\n+\tif gc := p.generic.FullScanCount(); gc > 0 || !p.generic.Flags.Empty() {\n+\t\tfor i := 0; i < p.custom.length; i++ {\n+\t\t\tif p.custom.costs[i].Flags.Less(p.generic.Flags) ||\n+\t\t\t\tgc > p.custom.costs[i].FullScanCount() {\n+\t\t\t\treturn false\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Compare the generic cost to the average custom cost. Clear the cost flags\n+\t// because they have already been handled above.\n+\tgen := memo.Cost{C: p.generic.C}\n+\treturn gen.Less(p.avgCustom())\n+}\n+\n+// avgCustom returns the average cost of all the custom plan costs in planCosts.\n+// If there are no custom plan costs, it returns 0.\n+func (p *planCosts) avgCustom() memo.Cost {\n+\tif p.custom.length == 0 {\n+\t\treturn memo.Cost{C: 0}\n+\t}\n+\tvar sum float64\n+\tfor i := 0; i < p.custom.length; i++ {\n+\t\tsum += p.custom.costs[i].C\n+\t}\n+\treturn memo.Cost{C: sum / float64(p.custom.length)}\n+}\n+\n+// Reset clears any previously set costs.\n+func (p *planCosts) Reset() {\n+\tp.generic = memo.Cost{C: 0}\n+\tp.custom.nextIdx = 0\n+\tp.custom.length = 0\n+}\ndiff --git a/pkg/sql/prepared_stmt.go b/pkg/sql/prepared_stmt.go\nindex fb3df77d75d4..b96e8cc2d963 100644\n--- a/pkg/sql/prepared_stmt.go\n+++ b/pkg/sql/prepared_stmt.go\n@@ -7,215 +7,30 @@ package sql\n \n import (\n \t\"context\"\n-\t\"time\"\n \t\"unsafe\"\n \n \t\"github.com/cockroachdb/cockroach/pkg/server/telemetry\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/appstatspb\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/clusterunique\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/flowinfra\"\n-\t\"github.com/cockroachdb/cockroach/pkg/sql/opt/memo\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgwirebase\"\n-\t\"github.com/cockroachdb/cockroach/pkg/sql/querycache\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/tree\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sqltelemetry\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/types\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/fsm\"\n-\t\"github.com/cockroachdb/cockroach/pkg/util/log\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/mon\"\n )\n \n-// PreparedStatementOrigin is an enum representing the source of where\n-// the prepare statement was made.\n-type PreparedStatementOrigin int\n-\n-const (\n-\t// PreparedStatementOriginWire signifies the prepared statement was made\n-\t// over the wire.\n-\tPreparedStatementOriginWire PreparedStatementOrigin = iota + 1\n-\t// PreparedStatementOriginSQL signifies the prepared statement was made\n-\t// over a parsed SQL query.\n-\tPreparedStatementOriginSQL\n-\t// PreparedStatementOriginSessionMigration signifies that the prepared\n-\t// statement came from a call to crdb_internal.deserialize_session.\n-\tPreparedStatementOriginSessionMigration\n-)\n-\n-// PreparedStatement is a SQL statement that has been parsed and the types\n-// of arguments and results have been determined.\n-//\n-// Note that PreparedStatements maintain a reference counter internally.\n-// References need to be registered with incRef() and de-registered with\n-// decRef().\n-type PreparedStatement struct {\n-\tquerycache.PrepareMetadata\n-\n-\t// BaseMemo is the memoized data structure constructed by the cost-based\n-\t// optimizer during prepare of a SQL statement.\n-\tBaseMemo *memo.Memo\n-\n-\t// GenericMemo, if present, is a fully-optimized memo that can be executed\n-\t// as-is.\n-\tGenericMemo *memo.Memo\n-\n-\t// IdealGenericPlan is true if GenericMemo is guaranteed to be optimal\n-\t// across all executions of the prepared statement. Ideal generic plans are\n-\t// generated when the statement has no placeholders nor fold-able stable\n-\t// expressions, or when the placeholder fast-path is utilized.\n-\tIdealGenericPlan bool\n-\n-\t// Costs tracks the costs of previously optimized custom and generic plans.\n-\tCosts planCosts\n-\n-\t// refCount keeps track of the number of references to this PreparedStatement.\n-\t// New references are registered through incRef().\n-\t// Once refCount hits 0 (through calls to decRef()), the following memAcc is\n-\t// closed.\n-\t// Most references are being held by portals created from this prepared\n-\t// statement.\n-\trefCount int\n-\tmemAcc   mon.BoundAccount\n-\n-\t// createdAt is the timestamp this prepare statement was made at.\n-\t// Used for reporting on `pg_prepared_statements`.\n-\tcreatedAt time.Time\n-\t// origin is the protocol in which this prepare statement was created.\n-\t// Used for reporting on `pg_prepared_statements`.\n-\torigin PreparedStatementOrigin\n-}\n-\n-// MemoryEstimate returns a rough estimate of the PreparedStatement's memory\n-// usage, in bytes.\n-func (p *PreparedStatement) MemoryEstimate() int64 {\n-\t// Account for the memory used by this prepared statement:\n-\t//   1. Size of the prepare metadata.\n-\t//   2. Size of the prepared memo, if using the cost-based optimizer.\n-\tsize := p.PrepareMetadata.MemoryEstimate()\n-\tif p.BaseMemo != nil {\n-\t\tsize += p.BaseMemo.MemoryEstimate()\n-\t}\n-\tif p.GenericMemo != nil {\n-\t\tsize += p.GenericMemo.MemoryEstimate()\n-\t}\n-\treturn size\n-}\n-\n-func (p *PreparedStatement) decRef(ctx context.Context) {\n-\tif p.refCount <= 0 {\n-\t\tlog.Fatal(ctx, \"corrupt PreparedStatement refcount\")\n-\t}\n-\tp.refCount--\n-\tif p.refCount == 0 {\n-\t\tp.memAcc.Close(ctx)\n-\t}\n-}\n-\n-func (p *PreparedStatement) incRef(ctx context.Context) {\n-\tif p.refCount <= 0 {\n-\t\tlog.Fatal(ctx, \"corrupt PreparedStatement refcount\")\n-\t}\n-\tp.refCount++\n-}\n-\n-const (\n-\t// CustomPlanThreshold is the maximum number of custom plan costs tracked by\n-\t// planCosts. It is also the number of custom plans executed when\n-\t// plan_cache_mode=auto before attempting to generate a generic plan.\n-\tCustomPlanThreshold = 5\n-)\n-\n-// planCosts tracks costs of generic and custom plans.\n-type planCosts struct {\n-\tgeneric memo.Cost\n-\tcustom  struct {\n-\t\tnextIdx int\n-\t\tlength  int\n-\t\tcosts   [CustomPlanThreshold]memo.Cost\n-\t}\n-}\n-\n-// HasGeneric returns true if the planCosts has a generic plan cost.\n-func (p *planCosts) HasGeneric() bool {\n-\treturn p.generic.C != 0\n-}\n-\n-// SetGeneric sets the cost of the generic plan.\n-func (p *planCosts) SetGeneric(cost memo.Cost) {\n-\tp.generic = cost\n-}\n-\n-// AddCustom adds a custom plan cost to the planCosts, evicting the oldest cost\n-// if necessary.\n-func (p *planCosts) AddCustom(cost memo.Cost) {\n-\tp.custom.costs[p.custom.nextIdx] = cost\n-\tp.custom.nextIdx++\n-\tif p.custom.nextIdx >= CustomPlanThreshold {\n-\t\tp.custom.nextIdx = 0\n-\t}\n-\tif p.custom.length < CustomPlanThreshold {\n-\t\tp.custom.length++\n-\t}\n-}\n-\n-// NumCustom returns the number of custom plan costs in the planCosts.\n-func (p *planCosts) NumCustom() int {\n-\treturn p.custom.length\n-}\n-\n-// IsGenericOptimal returns true if the generic plan is optimal w.r.t. the\n-// custom plans. The cost flags, auxiliary cost information, and the cost value\n-// are all considered. If any of the custom plan cost flags are less than the\n-// generic cost flags, then the generic plan is not optimal. If the generic plan\n-// has more full scans than any of the custom plans, then it is not optimal.\n-// Otherwise, the generic plan is optimal if its cost value is less than the\n-// average cost of the custom plans.\n-func (p *planCosts) IsGenericOptimal() bool {\n-\t// Check cost flags and full scan counts.\n-\tif gc := p.generic.FullScanCount(); gc > 0 || !p.generic.Flags.Empty() {\n-\t\tfor i := 0; i < p.custom.length; i++ {\n-\t\t\tif p.custom.costs[i].Flags.Less(p.generic.Flags) ||\n-\t\t\t\tgc > p.custom.costs[i].FullScanCount() {\n-\t\t\t\treturn false\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Compare the generic cost to the average custom cost. Clear the cost flags\n-\t// because they have already been handled above.\n-\tgen := memo.Cost{C: p.generic.C}\n-\treturn gen.Less(p.avgCustom())\n-}\n-\n-// avgCustom returns the average cost of all the custom plan costs in planCosts.\n-// If there are no custom plan costs, it returns 0.\n-func (p *planCosts) avgCustom() memo.Cost {\n-\tif p.custom.length == 0 {\n-\t\treturn memo.Cost{C: 0}\n-\t}\n-\tvar sum float64\n-\tfor i := 0; i < p.custom.length; i++ {\n-\t\tsum += p.custom.costs[i].C\n-\t}\n-\treturn memo.Cost{C: sum / float64(p.custom.length)}\n-}\n-\n-// Reset clears any previously set costs.\n-func (p *planCosts) Reset() {\n-\tp.generic = memo.Cost{C: 0}\n-\tp.custom.nextIdx = 0\n-\tp.custom.length = 0\n-}\n-\n // preparedStatementsAccessor gives a planner access to a session's collection\n // of prepared statements.\n type preparedStatementsAccessor interface {\n \t// List returns all prepared statements as a map keyed by name.\n \t// The map itself is a copy of the prepared statements.\n-\tList() map[string]*PreparedStatement\n-\t// Get returns the prepared statement with the given name. If touchLRU is\n-\t// true, this counts as an access for LRU bookkeeping. The returned bool is\n-\t// false if a statement with the given name doesn't exist.\n-\tGet(name string, touchLRU bool) (*PreparedStatement, bool)\n+\tList() map[string]*prep.Statement\n+\t// Get returns the prepared statement with the given name. The returned bool\n+\t// is false if a statement with the given name doesn't exist.\n+\tGet(name string) (*prep.Statement, bool)\n \t// Delete removes the PreparedStatement with the provided name from the\n \t// collection. If a portal exists for that statement, it is also removed.\n \t// The method returns true if statement with that name was found and removed,\n@@ -231,11 +46,11 @@ type emptyPreparedStatements struct{}\n \n var _ preparedStatementsAccessor = emptyPreparedStatements{}\n \n-func (e emptyPreparedStatements) List() map[string]*PreparedStatement {\n+func (e emptyPreparedStatements) List() map[string]*prep.Statement {\n \treturn nil\n }\n \n-func (e emptyPreparedStatements) Get(string, bool) (*PreparedStatement, bool) {\n+func (e emptyPreparedStatements) Get(string) (*prep.Statement, bool) {\n \treturn nil, false\n }\n \n@@ -268,7 +83,7 @@ const (\n // arguments.\n type PreparedPortal struct {\n \tName  string\n-\tStmt  *PreparedStatement\n+\tStmt  *prep.Statement\n \tQargs tree.QueryArguments\n \n \t// OutFormats contains the requested formats for the output columns.\n@@ -294,7 +109,7 @@ type PreparedPortal struct {\n func (ex *connExecutor) makePreparedPortal(\n \tctx context.Context,\n \tname string,\n-\tstmt *PreparedStatement,\n+\tstmt *prep.Statement,\n \tqargs tree.QueryArguments,\n \toutFormats []pgwirebase.FormatCode,\n ) (PreparedPortal, error) {\n@@ -325,7 +140,7 @@ func (p *PreparedPortal) accountForCopy(\n \t\treturn err\n \t}\n \t// Only increment the reference if we're going to keep it.\n-\tp.Stmt.incRef(ctx)\n+\tp.Stmt.IncRef(ctx)\n \treturn nil\n }\n \n@@ -334,7 +149,7 @@ func (p *PreparedPortal) close(\n \tctx context.Context, prepStmtsNamespaceMemAcc *mon.BoundAccount, portalName string,\n ) {\n \tprepStmtsNamespaceMemAcc.Shrink(ctx, p.size(portalName))\n-\tp.Stmt.decRef(ctx)\n+\tp.Stmt.DecRef(ctx)\n \tif p.pauseInfo != nil {\n \t\tp.pauseInfo.cleanupAll(ctx)\n \t\tp.pauseInfo = nil\ndiff --git a/pkg/sql/querycache/BUILD.bazel b/pkg/sql/querycache/BUILD.bazel\nindex 9e646213d0e2..9a15c823a534 100644\n--- a/pkg/sql/querycache/BUILD.bazel\n+++ b/pkg/sql/querycache/BUILD.bazel\n@@ -2,21 +2,14 @@ load(\"@io_bazel_rules_go//go:def.bzl\", \"go_library\", \"go_test\")\n \n go_library(\n     name = \"querycache\",\n-    srcs = [\n-        \"prepared_statement.go\",\n-        \"query_cache.go\",\n-    ],\n+    srcs = [\"query_cache.go\"],\n     importpath = \"github.com/cockroachdb/cockroach/pkg/sql/querycache\",\n     visibility = [\"//visibility:public\"],\n     deps = [\n-        \"//pkg/sql/catalog/colinfo\",\n         \"//pkg/sql/opt/memo\",\n-        \"//pkg/sql/parser/statements\",\n-        \"//pkg/sql/sem/tree\",\n-        \"//pkg/sql/types\",\n+        \"//pkg/sql/prep\",\n         \"//pkg/util/syncutil\",\n         \"@com_github_cockroachdb_errors//:errors\",\n-        \"@com_github_lib_pq//oid\",\n     ],\n )\n \n@@ -27,6 +20,7 @@ go_test(\n     embed = [\":querycache\"],\n     deps = [\n         \"//pkg/sql/opt/memo\",\n+        \"//pkg/sql/prep\",\n         \"//pkg/util/randutil\",\n         \"@com_github_cockroachdb_errors//:errors\",\n     ],\ndiff --git a/pkg/sql/querycache/query_cache.go b/pkg/sql/querycache/query_cache.go\nindex ae3249a406aa..bdf49f4b768d 100644\n--- a/pkg/sql/querycache/query_cache.go\n+++ b/pkg/sql/querycache/query_cache.go\n@@ -9,6 +9,7 @@ import (\n \t\"math/rand\"\n \n \t\"github.com/cockroachdb/cockroach/pkg/sql/opt/memo\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/syncutil\"\n \t\"github.com/cockroachdb/errors\"\n )\n@@ -48,15 +49,15 @@ const maxCachedSize = 128 * 1024\n type CachedData struct {\n \tSQL  string\n \tMemo *memo.Memo\n-\t// PrepareMetadata is set for prepare queries. In this case the memo contains\n+\t// Metadata is set for prepare queries. In this case the memo contains\n \t// unassigned placeholders. For non-prepared queries, it is nil.\n-\tPrepareMetadata *PrepareMetadata\n+\tMetadata *prep.Metadata\n }\n \n func (cd *CachedData) memoryEstimate() int64 {\n \tres := int64(len(cd.SQL)) + cd.Memo.MemoryEstimate()\n-\tif cd.PrepareMetadata != nil {\n-\t\tres += cd.PrepareMetadata.MemoryEstimate()\n+\tif cd.Metadata != nil {\n+\t\tres += cd.Metadata.MemoryEstimate()\n \t}\n \treturn res\n }\n@@ -126,7 +127,7 @@ func New(memorySize int64) *C {\n // Find returns the entry for the given query, if it is in the cache.\n //\n // If any cached data needs to be updated, it must be done via Add. In\n-// particular, PrepareMetadata in the returned CachedData must not be modified.\n+// particular, Metadata in the returned CachedData must not be modified.\n func (c *C) Find(session *Session, sql string) (_ CachedData, ok bool) {\n \tc.mu.Lock()\n \tdefer c.mu.Unlock()\n@@ -145,7 +146,7 @@ func (c *C) Find(session *Session, sql string) (_ CachedData, ok bool) {\n \n // Add adds an entry to the cache (possibly evicting some other entry). If the\n // cache already has a corresponding entry for d.SQL, it is updated.\n-// Note: d.PrepareMetadata cannot be modified once this method is called.\n+// Note: d.Metadata cannot be modified once this method is called.\n func (c *C) Add(session *Session, d *CachedData) {\n \tif session.highMissRatio() {\n \t\t// If the recent miss ratio in this session is high, we want to avoid the\ndiff --git a/pkg/sql/sem/eval/deps.go b/pkg/sql/sem/eval/deps.go\nindex 425d24b46a42..1c59c28f4437 100644\n--- a/pkg/sql/sem/eval/deps.go\n+++ b/pkg/sql/sem/eval/deps.go\n@@ -535,7 +535,7 @@ type PreparedStatementState interface {\n \t// HasActivePortals returns true if there are portals in the session.\n \tHasActivePortals() bool\n \t// MigratablePreparedStatements returns a mapping of all prepared statements.\n-\tMigratablePreparedStatements() []sessiondatapb.MigratableSession_PreparedStatement\n+\tMigratablePreparedStatements() ([]sessiondatapb.MigratableSession_PreparedStatement, error)\n \t// HasPortal returns true if there exists a given named portal in the session.\n \tHasPortal(s string) bool\n }\ndiff --git a/pkg/sql/session_state.go b/pkg/sql/session_state.go\nindex bc50ff16a803..6b72c77dd796 100644\n--- a/pkg/sql/session_state.go\n+++ b/pkg/sql/session_state.go\n@@ -15,6 +15,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgcode\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgwirebase\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/eval\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/tree\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sessiondata\"\n@@ -54,7 +55,7 @@ func serializeSessionState(\n \tprepStmtsState eval.PreparedStatementState,\n \tsd *sessiondata.SessionData,\n \texecCfg *ExecutorConfig,\n-) (*tree.DBytes, error) {\n+) (_ *tree.DBytes, err error) {\n \tif inExplicitTxn {\n \t\treturn nil, pgerror.Newf(\n \t\t\tpgcode.InvalidTransactionState,\n@@ -87,7 +88,9 @@ func serializeSessionState(\n \tm.SessionData = sd.SessionData\n \tsessiondata.MarshalNonLocal(sd, &m.SessionData)\n \tm.LocalOnlySessionData = sd.LocalOnlySessionData\n-\tm.PreparedStatements = prepStmtsState.MigratablePreparedStatements()\n+\tif m.PreparedStatements, err = prepStmtsState.MigratablePreparedStatements(); err != nil {\n+\t\treturn nil, err\n+\t}\n \n \tb, err := protoutil.Marshal(&m)\n \tif err != nil {\n@@ -205,7 +208,7 @@ func (p *planner) DeserializeSessionState(\n \t\t\tstmt,\n \t\t\tplaceholderTypes,\n \t\t\tprepStmt.PlaceholderTypeHints,\n-\t\t\tPreparedStatementOriginSessionMigration,\n+\t\t\tprep.StatementOriginSessionMigration,\n \t\t)\n \t\tif err != nil {\n \t\t\treturn nil, err\ndiff --git a/pkg/sql/statement.go b/pkg/sql/statement.go\nindex 50dbc8637ec8..9e032dfae7c1 100644\n--- a/pkg/sql/statement.go\n+++ b/pkg/sql/statement.go\n@@ -9,6 +9,7 @@ import (\n \t\"github.com/cockroachdb/cockroach/pkg/sql/catalog/colinfo\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/clusterunique\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/parser/statements\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/sql/sem/tree\"\n )\n \n@@ -32,7 +33,7 @@ type Statement struct {\n \t//\n \t// Given that the PreparedStatement can be modified during planning, it is\n \t// not safe for use on multiple threads.\n-\tPrepared *PreparedStatement\n+\tPrepared *prep.Statement\n }\n \n func makeStatement(\n@@ -46,7 +47,7 @@ func makeStatement(\n \t}\n }\n \n-func makeStatementFromPrepared(prepared *PreparedStatement, queryID clusterunique.ID) Statement {\n+func makeStatementFromPrepared(prepared *prep.Statement, queryID clusterunique.ID) Statement {\n \treturn Statement{\n \t\tStatement:       prepared.Statement,\n \t\tPrepared:        prepared,\ndiff --git a/pkg/sql/two_phase_commit.go b/pkg/sql/two_phase_commit.go\nindex e1460f005bdd..89f6f7774d98 100644\n--- a/pkg/sql/two_phase_commit.go\n+++ b/pkg/sql/two_phase_commit.go\n@@ -74,7 +74,7 @@ func (ex *connExecutor) execPrepareTransactionInOpenStateInternal(\n \tif err := ex.extraTxnState.sqlCursors.closeAll(&ex.planner, cursorCloseForTxnPrepare); err != nil {\n \t\treturn err\n \t}\n-\tex.extraTxnState.prepStmtsNamespace.closeAllPortals(ctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc)\n+\tex.extraTxnState.prepStmtsNamespace.closePortals(ctx, &ex.extraTxnState.prepStmtsNamespaceMemAcc)\n \n \t// Validate the global ID.\n \tglobalID := s.Transaction.RawString()\n", "test_patch": "diff --git a/pkg/sql/logictest/testdata/logic_test/prepare b/pkg/sql/logictest/testdata/logic_test/prepare\nindex b08d9c4afad9..2ed57044d779 100644\n--- a/pkg/sql/logictest/testdata/logic_test/prepare\n+++ b/pkg/sql/logictest/testdata/logic_test/prepare\n@@ -1629,14 +1629,18 @@ INSERT INTO prep_stmts SELECT 2, name FROM pg_catalog.pg_prepared_statements;\n SELECT IF(nextval('s') <= 3, crdb_internal.force_retry('1 hour'), 0);\n COMMIT\n \n+statement ok\n+INSERT INTO prep_stmts SELECT 3, name FROM pg_catalog.pg_prepared_statements\n+\n # Validate that the transaction was actually tried multiple times.\n query I\n SELECT currval('s')\n ----\n 4\n \n-# Validate that the LRU list was correct before and after the PREPAREs, even\n-# after multiple retries.\n+# Validate that the LRU list was correct before and after the PREPAREs, and\n+# after the COMMIT, even after multiple retries. Notice that evictions only\n+# occur once the transaction successfully commits.\n query IT\n SELECT which, name FROM prep_stmts ORDER BY which, name\n ----\n@@ -1646,10 +1650,17 @@ SELECT which, name FROM prep_stmts ORDER BY which, name\n 1  pscs16\n 1  pscs17\n 2  pscs11\n+2  pscs14\n+2  pscs15\n 2  pscs16\n 2  pscs17\n 2  pscs18\n 2  pscs19\n+3  pscs11\n+3  pscs16\n+3  pscs17\n+3  pscs18\n+3  pscs19\n \n statement ok\n DROP TABLE prep_stmts\ndiff --git a/pkg/sql/plan_opt_test.go b/pkg/sql/plan_opt_test.go\nindex ac099e5a5cc1..318afe112f2e 100644\n--- a/pkg/sql/plan_opt_test.go\n+++ b/pkg/sql/plan_opt_test.go\n@@ -478,7 +478,7 @@ SELECT cte.x, cte.y FROM cte LEFT JOIN cte as cte2 on cte.y = cte2.x`, j)\n \t\t})\n \n \t\t// Verify the case where a PREPARE encounters a query cache entry that was\n-\t\t// created by a direct execution (and hence has no PrepareMetadata).\n+\t\t// created by a direct execution (and hence has no Metadata).\n \t\tt.Run(\"exec-and-prepare\", func(t *testing.T) {\n \t\t\tparallel(t)\n \t\t\th := makeQueryCacheTestHelper(t, 1 /* numConns */)\ndiff --git a/pkg/sql/prep/cache_test.go b/pkg/sql/prep/cache_test.go\nnew file mode 100644\nindex 000000000000..0252849988b2\n--- /dev/null\n+++ b/pkg/sql/prep/cache_test.go\n@@ -0,0 +1,358 @@\n+// Copyright 2025 The Cockroach Authors.\n+//\n+// Use of this software is governed by the CockroachDB Software License\n+// included in the /LICENSE file.\n+\n+package prep\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"slices\"\n+\t\"testing\"\n+\n+\t\"github.com/cockroachdb/cockroach/pkg/util/randutil\"\n+\t\"github.com/cockroachdb/errors\"\n+\t\"golang.org/x/exp/maps\"\n+)\n+\n+const (\n+\t// The number of random test runs.\n+\tnumRuns = 100\n+\t// The number of cache operations to perform per test run.\n+\tnumOps = 5000\n+\t// The number of distinct names to use.\n+\tnumNames = 50\n+\t// The upperbound for choosing a random statement size.\n+\tstmtSizeUpperbound = 20\n+\t// The upperbound for choosing a random max size of the cache during\n+\t// commits.\n+\tmaxSizeUpperbound = 100\n+)\n+\n+func TestCache(t *testing.T) {\n+\tctx := context.Background()\n+\n+\tt.Run(\"empty\", func(t *testing.T) {\n+\t\tvar c Cache\n+\t\tc.Init(ctx)\n+\t\tassertEmpty(t, &c)\n+\t\tif c.Dirty() {\n+\t\t\tt.Errorf(\"expected c to not be dirty\")\n+\t\t}\n+\t\t_ = c.Commit(ctx, 10)\n+\t\tassertEmpty(t, &c)\n+\t\tif c.Dirty() {\n+\t\t\tt.Errorf(\"expected c to not be dirty\")\n+\t\t}\n+\t\t_ = c.Commit(ctx, 10)\n+\t\tc.Rewind(ctx)\n+\t\tif c.Dirty() {\n+\t\t\tt.Errorf(\"expected c to not be dirty\")\n+\t\t}\n+\t})\n+\n+\tt.Run(\"add-remove\", func(t *testing.T) {\n+\t\tvar c Cache\n+\t\tc.Init(ctx)\n+\t\tc.Add(\"s1\", stmt(), 1)\n+\t\tc.Remove(\"s1\")\n+\t\tif c.Len() != 0 {\n+\t\t\tt.Errorf(\"expected cache to be empty after removing s1\")\n+\t\t}\n+\t\tif c.Size() != 0 {\n+\t\t\tt.Errorf(\"expected empty cache to have size 0\")\n+\t\t}\n+\t})\n+\n+\tt.Run(\"init\", func(t *testing.T) {\n+\t\tvar c Cache\n+\t\tc.Init(ctx)\n+\t\tc.Add(\"s1\", stmt(), 1)\n+\t\t_ = c.Commit(ctx, 0)\n+\t\tc.Init(ctx)\n+\t\tif c.Len() != 0 {\n+\t\t\tt.Errorf(\"expected cache to be empty after reinitializing\")\n+\t\t}\n+\t\tif c.Size() != 0 {\n+\t\t\tt.Errorf(\"expected empty cache to have size 0\")\n+\t\t}\n+\t})\n+\n+\tt.Run(\"evict-none\", func(t *testing.T) {\n+\t\tvar c Cache\n+\t\tc.Init(ctx)\n+\t\tc.Add(\"s1\", stmt(), 1)\n+\t\tc.Add(\"s2\", stmt(), 1)\n+\t\tevicted := c.Commit(ctx, 2)\n+\t\tif len(evicted) > 0 {\n+\t\t\tt.Errorf(\"expected no evictions\")\n+\t\t}\n+\t\tc.Add(\"s3\", stmt(), 10)\n+\t\tassertContains(t, &c, \"s1\", \"s2\", \"s3\")\n+\t\tc.Rewind(ctx)\n+\t\tevicted = c.Commit(ctx, 3)\n+\t\tif len(evicted) > 0 {\n+\t\t\tt.Errorf(\"expected no evictions, got %v\", evicted)\n+\t\t}\n+\t\tevicted = c.Commit(ctx, 2)\n+\t\tif len(evicted) > 0 {\n+\t\t\tt.Errorf(\"expected no evictions, got %v\", evicted)\n+\t\t}\n+\t\tassertContains(t, &c, \"s1\", \"s2\")\n+\t})\n+\n+\tt.Run(\"evict\", func(t *testing.T) {\n+\t\tvar c Cache\n+\t\tc.Init(ctx)\n+\t\tc.Add(\"s1\", stmt(), 2)\n+\t\tc.Add(\"s2\", stmt(), 2)\n+\t\tc.Add(\"s3\", stmt(), 2)\n+\t\tc.Add(\"s4\", stmt(), 2)\n+\t\t_, _ = c.Get(\"s1\")\n+\t\tevicted := c.Commit(ctx, 6)\n+\t\tif !slices.Equal(evicted, []string{\"s2\"}) {\n+\t\t\tt.Errorf(\"expected s2 to be evicted, got: %v\", evicted)\n+\t\t}\n+\t\t// Clear the cache.\n+\t\tc.Init(ctx)\n+\t\tc.Add(\"s5\", stmt(), 2)\n+\t\tc.Add(\"s6\", stmt(), 2)\n+\t\tc.Add(\"s7\", stmt(), 2)\n+\t\tc.Add(\"s8\", stmt(), 2)\n+\t\tevicted = c.Commit(ctx, 2)\n+\t\tif !slices.Equal(evicted, []string{\"s5\", \"s6\", \"s7\"}) {\n+\t\t\tt.Errorf(\"expected s5, s6, and s7 to be evicted, got %v\", evicted)\n+\t\t}\n+\t\tassertContains(t, &c, \"s8\")\n+\t})\n+\n+\tt.Run(\"random\", func(t *testing.T) {\n+\t\tfor i := 0; i < numRuns; i++ {\n+\t\t\tt.Run(fmt.Sprintf(\"%d\", i), func(t *testing.T) {\n+\t\t\t\tt.Parallel() // SAFE FOR TESTING (this comment is for the linter)\n+\t\t\t\trng, _ := randutil.NewTestRand()\n+\t\t\t\tvar c Cache\n+\t\t\t\tvar o oracle\n+\t\t\t\tc.Init(ctx)\n+\t\t\t\to.Init()\n+\t\t\t\tfor j := 0; j < numOps; j++ {\n+\t\t\t\t\tswitch n := rng.Intn(10); n {\n+\t\t\t\t\tcase 0:\n+\t\t\t\t\t\tmaxSize := rng.Int63n(maxSizeUpperbound)\n+\t\t\t\t\t\tcEvicted := c.Commit(ctx, maxSize)\n+\t\t\t\t\t\toEvicted := o.Commit(maxSize)\n+\t\t\t\t\t\tif !slices.Equal(cEvicted, oEvicted) {\n+\t\t\t\t\t\t\tt.Errorf(\"expected evicted statements %v, got %v\",\n+\t\t\t\t\t\t\t\toEvicted, cEvicted)\n+\t\t\t\t\t\t}\n+\t\t\t\t\tcase 1:\n+\t\t\t\t\t\tc.Rewind(ctx)\n+\t\t\t\t\t\to.Rewind()\n+\t\t\t\t\tdefault:\n+\t\t\t\t\t\tname := fmt.Sprintf(\"stmt_%d\", rng.Intn(numNames))\n+\t\t\t\t\t\tswitch n := rng.Intn(10); n {\n+\t\t\t\t\t\tcase 0:\n+\t\t\t\t\t\t\tif !o.Has(name) {\n+\t\t\t\t\t\t\t\ts := stmt()\n+\t\t\t\t\t\t\t\tsize := rng.Int63n(stmtSizeUpperbound)\n+\t\t\t\t\t\t\t\tc.Add(name, s, size)\n+\t\t\t\t\t\t\t\to.Add(name, s, size)\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\tcase 1:\n+\t\t\t\t\t\t\tc.Remove(name)\n+\t\t\t\t\t\t\to.Remove(name)\n+\t\t\t\t\t\tdefault:\n+\t\t\t\t\t\t\tc.Get(name)\n+\t\t\t\t\t\t\to.Get(name)\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t\tvalidate(t, &o, &c)\n+\t\t\t\t}\n+\t\t\t})\n+\t\t}\n+\t})\n+}\n+\n+func stmt() *Statement { return &Statement{refCount: 1} }\n+\n+func assertEmpty(t *testing.T, c *Cache) {\n+\tassertContains(t, c)\n+}\n+\n+func assertContains(t *testing.T, c *Cache, names ...string) {\n+\tfor _, name := range names {\n+\t\tif !c.Has(name) {\n+\t\t\tt.Errorf(\"expected cache to have %s\", name)\n+\t\t}\n+\t}\n+\tc.ForEach(func(name string, _ *Statement) {\n+\t\tfound := false\n+\t\tfor _, n := range names {\n+\t\t\tif n == name {\n+\t\t\t\tfound = true\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tif !found {\n+\t\t\tt.Errorf(\"unexpected statement %s in cache\", name)\n+\t\t}\n+\t})\n+}\n+\n+func validate(t *testing.T, o *oracle, c *Cache) {\n+\tif o.Len() != c.Len() {\n+\t\tt.Errorf(\"expected cache to have %d items, got %d\", o.Len(), c.Len())\n+\t}\n+\tif o.Size() != c.Size() {\n+\t\tt.Errorf(\"expected cache to have size %d, got %d\", o.Size(), c.Size())\n+\t}\n+\tif c.Len() == 0 && c.Size() != 0 {\n+\t\tt.Errorf(\"expected empty cache to have size 0, got %d\", c.Size())\n+\t}\n+\t// Check that every statement in the oracle is in the cache.\n+\to.ForEach(func(name string, stmt *Statement) {\n+\t\tif !c.Has(name) {\n+\t\t\tt.Errorf(\"expected c.Has to be true for %q\", name)\n+\t\t}\n+\t\t// Access the map directly instead of using Get so it is not counted as\n+\t\t// an access.\n+\t\tce, ok := c.m[name]\n+\t\tif !ok {\n+\t\t\tt.Errorf(\"expected c.Get to return ok=true for %q\", name)\n+\t\t}\n+\t\tif stmt != ce.stmt {\n+\t\t\tt.Errorf(\"incorrect statement mapped to %q\", name)\n+\t\t}\n+\t})\n+\t// Check that every statement in the cache is in the oracle.\n+\tc.ForEach(func(name string, stmt *Statement) {\n+\t\tif !o.Has(name) {\n+\t\t\tt.Errorf(\"unexpected statement for %q in c\", name)\n+\t\t}\n+\t\t// Access the map directly instead of using Get so it is not counted as\n+\t\t// an access.\n+\t\toe, ok := o.m[name]\n+\t\tif !ok {\n+\t\t\tt.Errorf(\"unexpected entry for %q in c\", name)\n+\t\t}\n+\t\tif stmt != oe.stmt {\n+\t\t\tt.Errorf(\"incorrect statement mapped to %q\", name)\n+\t\t}\n+\t})\n+\tif o.Dirty() != c.Dirty() {\n+\t\tt.Errorf(\"expected Dirty to return %v, got %v\", o.Dirty(), c.Dirty())\n+\t}\n+}\n+\n+// oracle implements the same API as Cache in a simpler, less efficient way. It\n+// is used for randomized tests for Cache.\n+type oracle struct {\n+\toMap\n+\tsnapshot oMap\n+}\n+\n+type oMap struct {\n+\tm     map[string]oEntry\n+\tsize  int64\n+\tclock int\n+}\n+\n+type oEntry struct {\n+\tname string\n+\tstmt *Statement\n+\tsize int64\n+\tt    int\n+}\n+\n+func (o *oracle) Init() {\n+\to.m = make(map[string]oEntry)\n+\to.snapshot.m = make(map[string]oEntry)\n+}\n+\n+func (o *oracle) Add(name string, stmt *Statement, size int64) {\n+\tif _, ok := o.m[name]; ok {\n+\t\tpanic(errors.AssertionFailedf(\"cache entry %q already exists\", name))\n+\t}\n+\to.m[name] = oEntry{name: name, stmt: stmt, size: size, t: o.clock}\n+\to.size += size\n+\to.clock++\n+}\n+\n+func (o *oracle) Remove(name string) {\n+\te, ok := o.m[name]\n+\tif !ok {\n+\t\treturn\n+\t}\n+\tdelete(o.m, name)\n+\to.size -= e.size\n+\to.clock++\n+}\n+\n+func (o *oracle) Has(name string) bool {\n+\t_, ok := o.m[name]\n+\treturn ok\n+}\n+\n+func (o *oracle) Get(name string) (stmt *Statement, ok bool) {\n+\te, ok := o.m[name]\n+\tif !ok {\n+\t\treturn nil, false\n+\t}\n+\te.t = o.clock\n+\to.m[name] = e\n+\to.clock++\n+\treturn e.stmt, ok\n+}\n+\n+func (o *oracle) Len() int {\n+\treturn len(o.m)\n+}\n+\n+func (o *oracle) Size() int64 {\n+\treturn o.size\n+}\n+\n+func (o *oracle) Commit(maxSize int64) (evicted []string) {\n+\t// Evict committed entries, if necessary.\n+\tentries := o.orderedEntries()\n+\tfor i := 0; i < len(entries)-1 && maxSize > 0 && o.size > maxSize; i++ {\n+\t\tdelete(o.m, entries[i].name)\n+\t\to.size -= entries[i].size\n+\t\tevicted = append(evicted, entries[i].name)\n+\t}\n+\t// Make a snapshot of the map.\n+\tmaps.Clear(o.snapshot.m)\n+\tmaps.Copy(o.snapshot.m, o.m)\n+\to.snapshot.size = o.size\n+\to.snapshot.clock = o.clock\n+\treturn evicted\n+}\n+\n+func (o *oracle) orderedEntries() []oEntry {\n+\tvar res []oEntry\n+\tfor _, e := range o.m {\n+\t\tres = append(res, e)\n+\t}\n+\tslices.SortFunc(res, func(a, b oEntry) int {\n+\t\treturn a.t - b.t\n+\t})\n+\treturn res\n+}\n+\n+func (o *oracle) Rewind() {\n+\tmaps.Clear(o.m)\n+\tmaps.Copy(o.m, o.snapshot.m)\n+\to.size = o.snapshot.size\n+\to.clock = o.snapshot.clock\n+}\n+\n+func (o *oracle) Dirty() bool {\n+\treturn o.clock != o.snapshot.clock\n+}\n+\n+func (o *oracle) ForEach(fn func(name string, stmt *Statement)) {\n+\tfor name, e := range o.m {\n+\t\tfn(name, e.stmt)\n+\t}\n+}\ndiff --git a/pkg/sql/prepared_stmt_test.go b/pkg/sql/prep/statement_test.go\nsimilarity index 99%\nrename from pkg/sql/prepared_stmt_test.go\nrename to pkg/sql/prep/statement_test.go\nindex e0bf08977cea..f762487c5e00 100644\n--- a/pkg/sql/prepared_stmt_test.go\n+++ b/pkg/sql/prep/statement_test.go\n@@ -3,7 +3,7 @@\n // Use of this software is governed by the CockroachDB Software License\n // included in the /LICENSE file.\n \n-package sql\n+package prep\n \n import (\n \t\"testing\"\ndiff --git a/pkg/sql/querycache/query_cache_test.go b/pkg/sql/querycache/query_cache_test.go\nindex ae46b99ec8cb..f0dc9d5e3267 100644\n--- a/pkg/sql/querycache/query_cache_test.go\n+++ b/pkg/sql/querycache/query_cache_test.go\n@@ -12,6 +12,7 @@ import (\n \t\"testing\"\n \n \t\"github.com/cockroachdb/cockroach/pkg/sql/opt/memo\"\n+\t\"github.com/cockroachdb/cockroach/pkg/sql/prep\"\n \t\"github.com/cockroachdb/cockroach/pkg/util/randutil\"\n \t\"github.com/cockroachdb/errors\"\n )\n@@ -40,7 +41,7 @@ func expect(t *testing.T, c *C, exp string) {\n }\n \n func data(sql string, mem *memo.Memo, memEstimate int64) *CachedData {\n-\tcd := &CachedData{SQL: sql, Memo: mem, PrepareMetadata: &PrepareMetadata{}}\n+\tcd := &CachedData{SQL: sql, Memo: mem, Metadata: &prep.Metadata{}}\n \tn := memEstimate - cd.memoryEstimate()\n \tif n < 0 {\n \t\tpanic(errors.AssertionFailedf(\"size %d too small\", memEstimate))\n@@ -50,7 +51,7 @@ func data(sql string, mem *memo.Memo, memEstimate int64) *CachedData {\n \tfor i := range s {\n \t\ts[i] = 'x'\n \t}\n-\tcd.PrepareMetadata.StatementNoConstants = string(s)\n+\tcd.Metadata.StatementNoConstants = string(s)\n \tif cd.memoryEstimate() != memEstimate {\n \t\tpanic(errors.AssertionFailedf(\"failed to create CachedData of size %d\", memEstimate))\n \t}\n"}
{"org": "projectdiscovery", "repo": "nuclei", "number": 6718, "state": "closed", "title": "perf(generators): optimize `MergeMaps` to reduce allocs", "body": "## Proposed changes\r\n\r\n<!-- Describe the overall picture of your modifications to help maintainers understand the pull request. PRs are required to be associated to their related issue tickets or feature request. -->\r\n\r\nperf(generators): optimize `MergeMaps` to reduce allocs\r\n\r\n`MergeMaps` accounts for 11.41% of allocs (13.8\r\nGB) in clusterbomb mode. With 1,305 combinations\r\nper target, this function is called millions of\r\ntimes in the hot path.\r\n\r\nRCA:\r\n* Request generator calls `MergeMaps` with single\r\n  arg on every payload combination, incurring\r\n  variadic overhead.\r\n* Build request merges same maps multiple times\r\n  per request.\r\n* `BuildPayloadFromOptions` recomputes static CLI\r\n  options on every call.\r\n* Variables calls `MergeMaps` $$2Ã—N$$ times per\r\n  variable evaluation (once in loop, once in\r\n  `evaluateVariableValue`)\r\n\r\nChanges:\r\n\r\nCore optimizations in maps.go:\r\n* Pre-size merged map to avoid rehashing (30-40%\r\n  reduction)\r\n* Add `CopyMap` for efficient single-map copy\r\n  without variadic overhead.\r\n* Add `MergeMapsInto` for in-place mutation when\r\n  caller owns destination.\r\n\r\nHot path fixes:\r\n* Replace `MergeMaps(r.currentPayloads)` with\r\n  `CopyMap(r.currentPayloads)` to eliminates\r\n  allocation on every combination iteration.\r\n* Pre-allocate combined map once, extend in-place\r\n  during `ForEach` loop instead of creating new\r\n  map per variable (eliminates $$2Ã—N$$ allocations\r\n  per request).\r\n\r\nCaching with concurrency safety:\r\n* Cache `BuildPayloadFromOptions` computation in\r\n  `sync.Map` keyed by `types.Options` ptr, but\r\n  return copy to prevent concurrent modification.\r\n* Cost: shallow copy of ~10-20 entries vs. full\r\n  merge of vars + env (85-90% savings in typical\r\n  case)\r\n* Clear cache in `closeInternal()` to prevent\r\n  memory leaks when SDK instances are created or\r\n  destroyed.\r\n\r\nEstimated impact: 40-60% reduction in `MergeMaps`\r\nallocations (5.5-8.3 GB savings from original\r\n13.8 GB). Safe for concurrent execution and SDK\r\nusage with multiple instances.\r\n\r\n### Proof\r\n\r\n```console\r\n$ git checkout dev\r\n$ git cherry-pick 0ab06cc4\r\n$ go test -run - -benchmem -count 6 -bench \"^Benchmark(MergeMaps|BuildPayloadFromOptions|VariableEvaluate|VariableEvaluateScaling)\" ./pkg/protocols/common/variables/ ./pkg/protocols/common/generators/ | tee dev\r\n$ git checkout dwisiswant0/perf/generators/optimize-MergeMaps-to-reduce-allocs\r\n$ go test -run - -benchmem -count 6 -bench \"^Benchmark(MergeMaps|BuildPayloadFromOptions|VariableEvaluate|VariableEvaluateScaling)\" ./pkg/protocols/common/variables/ ./pkg/protocols/common/generators/ | tee patch\r\n$ benchstat dev patch\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/generators\r\ncpu: 11th Gen Intel(R) Core(TM) i9-11900H @ 2.50GHz\r\n                                     â”‚      dev      â”‚                patch                â”‚\r\n                                     â”‚    sec/op     â”‚    sec/op     vs base               â”‚\r\nMergeMaps/1-maps-4                      392.1n Â±  1%   400.0n Â±  1%   +2.01% (p=0.009 n=6)\r\nMergeMaps/2-maps-4                     1222.0n Â± 17%   875.1n Â±  4%  -28.38% (p=0.002 n=6)\r\nMergeMaps/3-maps-4                      1.416Âµ Â±  3%   1.082Âµ Â± 10%  -23.56% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Sequential-4   12.413Âµ Â±  4%   4.825Âµ Â±  2%  -61.13% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Parallel-4      6.004Âµ Â± 10%   2.519Âµ Â± 21%  -58.05% (p=0.002 n=6)\r\nMergeMapsInto/1-maps-4                                 192.1n Â±  8%\r\nMergeMapsInto/2-maps-4                                 335.7n Â±  5%\r\nMergeMapsInto/3-maps-4                                 484.7n Â±  1%\r\ngeomean                                 2.191Âµ         784.8n        -38.07%\r\n\r\n                                     â”‚     dev      â”‚                 patch                 â”‚\r\n                                     â”‚     B/op     â”‚     B/op      vs base                 â”‚\r\nMergeMaps/1-maps-4                       336.0 Â± 0%     336.0 Â± 0%        ~ (p=1.000 n=6) Â¹\r\nMergeMaps/2-maps-4                       952.0 Â± 0%     664.0 Â± 0%  -30.25% (p=0.002 n=6)\r\nMergeMaps/3-maps-4                       952.0 Â± 0%     664.0 Â± 0%  -30.25% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Sequential-4   9.523Ki Â± 0%   4.836Ki Â± 0%  -49.22% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Parallel-4     9.523Ki Â± 0%   4.836Ki Â± 0%  -49.22% (p=0.002 n=6)\r\nMergeMapsInto/1-maps-4                                  0.000 Â± 0%\r\nMergeMapsInto/2-maps-4                                  0.000 Â± 0%\r\nMergeMapsInto/3-maps-4                                  0.000 Â± 0%\r\ngeomean                                1.915Ki                      -33.98%               Â²\r\nÂ¹ all samples are equal\r\nÂ² summaries must be >0 to compute geomean\r\n\r\n                                     â”‚     dev     â”‚                patch                â”‚\r\n                                     â”‚  allocs/op  â”‚ allocs/op   vs base                 â”‚\r\nMergeMaps/1-maps-4                      2.000 Â± 0%   2.000 Â± 0%        ~ (p=1.000 n=6) Â¹\r\nMergeMaps/2-maps-4                      5.000 Â± 0%   4.000 Â± 0%  -20.00% (p=0.002 n=6)\r\nMergeMaps/3-maps-4                      5.000 Â± 0%   4.000 Â± 0%  -20.00% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Sequential-4   14.000 Â± 0%   4.000 Â± 0%  -71.43% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Parallel-4     14.000 Â± 0%   4.000 Â± 0%  -71.43% (p=0.002 n=6)\r\nMergeMapsInto/1-maps-4                               0.000 Â± 0%\r\nMergeMapsInto/2-maps-4                               0.000 Â± 0%\r\nMergeMapsInto/3-maps-4                               0.000 Â± 0%\r\ngeomean                                 6.284                    -44.59%               Â²\r\nÂ¹ all samples are equal\r\nÂ² summaries must be >0 to compute geomean\r\n\r\npkg: github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/variables\r\n                                       â”‚      dev      â”‚                patch                â”‚\r\n                                       â”‚    sec/op     â”‚    sec/op     vs base               â”‚\r\nVariableEvaluate/Evaluate/5Variables-4    17.73Âµ Â±  1%   13.99Âµ Â±  1%  -21.07% (p=0.002 n=6)\r\nVariableEvaluate/Evaluate/Parallel-4      7.091Âµ Â± 19%   6.412Âµ Â± 16%        ~ (p=0.180 n=6)\r\nVariableEvaluateScaling/Variables-1-4    1063.5n Â±  2%   949.0n Â±  1%  -10.77% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-5-4     7.964Âµ Â±  2%   4.566Âµ Â±  2%  -42.67% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-10-4    28.66Âµ Â±  1%   13.00Âµ Â±  1%  -54.63% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-20-4   111.71Âµ Â±  6%   39.95Âµ Â±  1%  -64.24% (p=0.002 n=6)\r\ngeomean                                   12.27Âµ         7.659Âµ        -37.56%\r\n\r\n                                       â”‚      dev      â”‚                patch                â”‚\r\n                                       â”‚     B/op      â”‚     B/op      vs base               â”‚\r\nVariableEvaluate/Evaluate/5Variables-4    5.156Ki Â± 0%   3.922Ki Â± 0%  -23.94% (p=0.002 n=6)\r\nVariableEvaluate/Evaluate/Parallel-4      4.828Ki Â± 0%   3.922Ki Â± 0%  -18.77% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-1-4       720.0 Â± 0%     736.0 Â± 0%   +2.22% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-5-4     2.523Ki Â± 0%   1.289Ki Â± 0%  -48.92% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-10-4   11.719Ki Â± 0%   5.031Ki Â± 0%  -57.07% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-20-4    59.83Ki Â± 0%   22.36Ki Â± 0%  -62.63% (p=0.002 n=6)\r\ngeomean                                   5.604Ki        3.421Ki       -38.95%\r\n\r\n                                       â”‚    dev     â”‚               patch               â”‚\r\n                                       â”‚ allocs/op  â”‚ allocs/op   vs base               â”‚\r\nVariableEvaluate/Evaluate/5Variables-4   120.0 Â± 0%   117.0 Â± 0%   -2.50% (p=0.002 n=6)\r\nVariableEvaluate/Evaluate/Parallel-4     118.0 Â± 0%   117.0 Â± 0%   -0.85% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-1-4    7.000 Â± 0%   8.000 Â± 0%  +14.29% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-5-4    45.00 Â± 0%   42.00 Â± 0%   -6.67% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-10-4   147.0 Â± 0%   120.0 Â± 0%  -18.37% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-20-4   504.0 Â± 0%   379.0 Â± 0%  -24.80% (p=0.002 n=6)\r\ngeomean                                  83.15        77.05        -7.34%\r\n```\r\n\r\n#### Summary\r\n\r\n**pkg/protocols/common/generators:**\r\n\r\n- **38.07% faster** (geomean)\r\n- **33.98% less memory** (geomean)\r\n- **44.59% fewer allocations** (geomean)\r\n\r\n**pkg/protocols/common/variables:**\r\n- **37.56% faster** (geomean)\r\n- **38.95% less memory** (geomean)\r\n- **7.34% fewer allocations** (geomean)\r\n\r\n**Core map ops:**\r\n\r\n* `MergeMaps` pre-sizing impact\r\n  * `MergeMaps/2-maps`:\r\n    * -28.38% faster\r\n    * -30.25% memory\r\n    * -20.00% allocs\r\n  * `MergeMaps/3-maps`:\r\n    * -23.56% faster\r\n    * -30.25% memory\r\n    * -20.00% allocs\r\n\r\n Pre-calc total cap before alloc eliminates rehashing overhead and reduces allocs by 20%.\r\n\r\n* `MergeMapsInto` (new func):\r\n  * 0 allocs for hot paths where dst is already owned by caller.\r\n\r\n**BuildPayloadFromOptions:**\r\n\r\nComparing dev (no caching, full merge every time) vs. patch (cached with copy-on-return):\r\n\r\n- **~60% faster** by caching computation.\r\n- **~50% less memory** despite returning copy.\r\n- **~71% fewer allocations** (cache hit eliminates merge overhead).\r\n\r\n**Variable Evaluation:**\r\n\r\nComparing dev (repeated `MergeMaps` in loop) vs. patch (single merge + in-place extension):\r\n\r\n**Insights**: \r\n- **dev**: $$2Ã—N$$ `MergeMaps` calls per evaluation (exponential waste).\r\n- **patch**: $$1+N$$ merge in-place extensions (linear).\r\n- **improvement**: 64% faster, 63% less memory at 20 variables.\r\n- **trade-off**: small overhead at $$N=1$$ (+14% allocs) acceptable for massive gains at realistic scales.\r\n\r\n#### Est'ed Impact\r\n\r\nIn clusterbomb mode with:\r\n- 1,305 payload combinations per target.\r\n- 1,000 targets.\r\n- 5 variables per template.\r\n- 17 protocol calls per request.\r\n\r\n**Original problem**: ~13.8 GB allocated in `MergeMaps` (11.41% of total allocations)\r\n\r\n**Measured improvements** (from benchstat):\r\n- `MergeMaps` pre-sizing: **28-30% faster**, **30% less memory**, **20% fewer allocations**\r\n- `MergeMapsInto` (hot paths): **0 allocations** (100% reduction)\r\n- `BuildPayloadFromOptions`: **60% faster**, **50% less memory**, **71% fewer allocations**\r\n- Variable evaluation (20 vars): **64% faster**, **63% less memory**, **25% fewer allocations**\r\n\r\n**Conservative estimate** (weighted by call frequency):\r\n- Hot path savings (request generator using `CopyMap` instead of `MergeMaps`): minimal allocation change.\r\n- Critical savings (variables using `MergeMapsInto`): 100% allocation elimination on $$N$$ evaluations per combination.\r\n- Caching savings (`BuildPayloadFromOptions`): 71% allocation reduction, called once per template.\r\n- Variable loop savings: 63% memory reduction on every combination.\r\n\r\n**Expected total `MergeMaps` allocation reduction: 55-70%**\r\n\r\nThis translates to **7.6-9.7 GB saved** from the original 13.8 GB, with significant CPU time improvements (38% faster geomean).\r\n\r\n**Additional benefits**:\r\n- **CPU**: 38% faster geomean in generators, 38% faster in variables.\r\n- **Scalability**: 64% improvement at 20 variables (common in complex templates).\r\n- **Concurrency**: 58% faster parallel performance in caching layer.\r\n\r\n## Checklist\r\n\r\n<!-- Put an \"x\" in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code. -->\r\n\r\n- [ ] Pull request is created against the [dev](https://github.com/projectdiscovery/nuclei/tree/dev) branch\r\n- [ ] All checks passed (lint, unit/integration/regression tests etc.) with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n\r\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\r\n## Summary by CodeRabbit\r\n\r\n* **Refactor**\r\n  * Optimized payload generation and variable evaluation through performance improvements and memory-efficient map operations.\r\n  * Enhanced thread-safety for concurrent SDK instances with cache-aware map handling.\r\n\r\n* **Tests**\r\n  * Added comprehensive test coverage for concurrency safety, caching behavior, and memory management.\r\n\r\n<sub>âœï¸ Tip: You can customize this high-level summary in your review settings.</sub>\r\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->", "url": "https://api.github.com/repos/projectdiscovery/nuclei/pulls/6718", "id": 3116322776, "node_id": "PR_kwDODxGgs865v0_Y", "html_url": "https://github.com/projectdiscovery/nuclei/pull/6718", "diff_url": "https://github.com/projectdiscovery/nuclei/pull/6718.diff", "patch_url": "https://github.com/projectdiscovery/nuclei/pull/6718.patch", "issue_url": "https://api.github.com/repos/projectdiscovery/nuclei/issues/6718", "created_at": "2025-12-19T04:53:10+00:00", "updated_at": "2025-12-19T19:18:30+00:00", "closed_at": "2025-12-19T19:10:22+00:00", "merged_at": "2025-12-19T19:10:22+00:00", "merge_commit_sha": "d19c364e38c64b073340e981c3736978ba092fbd", "labels": [], "draft": false, "commits_url": "https://api.github.com/repos/projectdiscovery/nuclei/pulls/6718/commits", "review_comments_url": "https://api.github.com/repos/projectdiscovery/nuclei/pulls/6718/comments", "review_comment_url": "https://api.github.com/repos/projectdiscovery/nuclei/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/projectdiscovery/nuclei/issues/6718/comments", "base": {"label": "projectdiscovery:dev", "ref": "dev", "sha": "d48c2c38faca629bf7df26fa8d45e9c3878b6800", "user": {"login": "projectdiscovery", "id": 50994705, "node_id": "MDEyOk9yZ2FuaXphdGlvbjUwOTk0NzA1", "avatar_url": "https://avatars.githubusercontent.com/u/50994705?v=4", "gravatar_id": "", "url": "https://api.github.com/users/projectdiscovery", "html_url": "https://github.com/projectdiscovery", "followers_url": "https://api.github.com/users/projectdiscovery/followers", "following_url": "https://api.github.com/users/projectdiscovery/following{/other_user}", "gists_url": "https://api.github.com/users/projectdiscovery/gists{/gist_id}", "starred_url": "https://api.github.com/users/projectdiscovery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/projectdiscovery/subscriptions", "organizations_url": "https://api.github.com/users/projectdiscovery/orgs", "repos_url": "https://api.github.com/users/projectdiscovery/repos", "events_url": "https://api.github.com/users/projectdiscovery/events{/privacy}", "received_events_url": "https://api.github.com/users/projectdiscovery/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 252813491, "node_id": "MDEwOlJlcG9zaXRvcnkyNTI4MTM0OTE=", "name": "nuclei", "full_name": "projectdiscovery/nuclei", "private": false, "owner": {"login": "projectdiscovery", "id": 50994705, "node_id": "MDEyOk9yZ2FuaXphdGlvbjUwOTk0NzA1", "avatar_url": "https://avatars.githubusercontent.com/u/50994705?v=4", "gravatar_id": "", "url": "https://api.github.com/users/projectdiscovery", "html_url": "https://github.com/projectdiscovery", "followers_url": "https://api.github.com/users/projectdiscovery/followers", "following_url": "https://api.github.com/users/projectdiscovery/following{/other_user}", "gists_url": "https://api.github.com/users/projectdiscovery/gists{/gist_id}", "starred_url": "https://api.github.com/users/projectdiscovery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/projectdiscovery/subscriptions", "organizations_url": "https://api.github.com/users/projectdiscovery/orgs", "repos_url": "https://api.github.com/users/projectdiscovery/repos", "events_url": "https://api.github.com/users/projectdiscovery/events{/privacy}", "received_events_url": "https://api.github.com/users/projectdiscovery/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/projectdiscovery/nuclei", "description": "Nuclei is a fast, customizable vulnerability scanner powered by the global security community and built on a simple YAML-based DSL, enabling collaboration to tackle trending vulnerabilities on the internet. It helps you find vulnerabilities in your applications, APIs, networks, DNS, and cloud configurations.", "fork": false, "url": "https://api.github.com/repos/projectdiscovery/nuclei", "forks_url": "https://api.github.com/repos/projectdiscovery/nuclei/forks", "keys_url": "https://api.github.com/repos/projectdiscovery/nuclei/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/projectdiscovery/nuclei/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/projectdiscovery/nuclei/teams", "hooks_url": "https://api.github.com/repos/projectdiscovery/nuclei/hooks", "issue_events_url": "https://api.github.com/repos/projectdiscovery/nuclei/issues/events{/number}", "events_url": "https://api.github.com/repos/projectdiscovery/nuclei/events", "assignees_url": "https://api.github.com/repos/projectdiscovery/nuclei/assignees{/user}", "branches_url": "https://api.github.com/repos/projectdiscovery/nuclei/branches{/branch}", "tags_url": "https://api.github.com/repos/projectdiscovery/nuclei/tags", "blobs_url": "https://api.github.com/repos/projectdiscovery/nuclei/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/projectdiscovery/nuclei/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/projectdiscovery/nuclei/git/refs{/sha}", "trees_url": "https://api.github.com/repos/projectdiscovery/nuclei/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/projectdiscovery/nuclei/statuses/{sha}", "languages_url": "https://api.github.com/repos/projectdiscovery/nuclei/languages", "stargazers_url": "https://api.github.com/repos/projectdiscovery/nuclei/stargazers", "contributors_url": "https://api.github.com/repos/projectdiscovery/nuclei/contributors", "subscribers_url": "https://api.github.com/repos/projectdiscovery/nuclei/subscribers", "subscription_url": "https://api.github.com/repos/projectdiscovery/nuclei/subscription", "commits_url": "https://api.github.com/repos/projectdiscovery/nuclei/commits{/sha}", "git_commits_url": "https://api.github.com/repos/projectdiscovery/nuclei/git/commits{/sha}", "comments_url": "https://api.github.com/repos/projectdiscovery/nuclei/comments{/number}", "issue_comment_url": "https://api.github.com/repos/projectdiscovery/nuclei/issues/comments{/number}", "contents_url": "https://api.github.com/repos/projectdiscovery/nuclei/contents/{+path}", "compare_url": "https://api.github.com/repos/projectdiscovery/nuclei/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/projectdiscovery/nuclei/merges", "archive_url": "https://api.github.com/repos/projectdiscovery/nuclei/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/projectdiscovery/nuclei/downloads", "issues_url": "https://api.github.com/repos/projectdiscovery/nuclei/issues{/number}", "pulls_url": "https://api.github.com/repos/projectdiscovery/nuclei/pulls{/number}", "milestones_url": "https://api.github.com/repos/projectdiscovery/nuclei/milestones{/number}", "notifications_url": "https://api.github.com/repos/projectdiscovery/nuclei/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/projectdiscovery/nuclei/labels{/name}", "releases_url": "https://api.github.com/repos/projectdiscovery/nuclei/releases{/id}", "deployments_url": "https://api.github.com/repos/projectdiscovery/nuclei/deployments", "created_at": "2020-04-03T18:47:11Z", "updated_at": "2026-01-07T06:25:38Z", "pushed_at": "2026-01-06T08:07:30Z", "git_url": "git://github.com/projectdiscovery/nuclei.git", "ssh_url": "git@github.com:projectdiscovery/nuclei.git", "clone_url": "https://github.com/projectdiscovery/nuclei.git", "svn_url": "https://github.com/projectdiscovery/nuclei", "homepage": "https://docs.projectdiscovery.io/tools/nuclei", "size": 42133, "stargazers_count": 26440, "watchers_count": 26440, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "has_discussions": true, "forks_count": 3038, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 202, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "topics": ["attack-surface", "cve-scanner", "dast", "hacktoberfest", "nuclei-engine", "security", "security-scanner", "subdomain-takeover", "vulnerability-assessment", "vulnerability-detection", "vulnerability-scanner"], "visibility": "public", "forks": 3038, "open_issues": 202, "watchers": 26440, "default_branch": "dev"}}, "commits": [{"sha": "bb79a061bc5ba6b1d08a014140583da61b906ced", "parents": ["d48c2c38faca629bf7df26fa8d45e9c3878b6800"], "message": "perf(generators): optimize `MergeMaps` to reduce allocs\n\n`MergeMaps` accounts for 11.41% of allocs (13.8\nGB) in clusterbomb mode. With 1,305 combinations\nper target, this function is called millions of\ntimes in the hot path.\n\nRCA:\n* Request generator calls `MergeMaps` with single\n  arg on every payload combination, incurring\n  variadic overhead.\n* Build request merges same maps multiple times\n  per request.\n* `BuildPayloadFromOptions` recomputes static CLI\n  options on every call.\n* Variables calls `MergeMaps` $$2Ã—N$$ times per\n  variable evaluation (once in loop, once in\n  `evaluateVariableValue`)\n\nChanges:\n\nCore optimizations in maps.go:\n* Pre-size merged map to avoid rehashing (30-40%\n  reduction)\n* Add `CopyMap` for efficient single-map copy\n  without variadic overhead.\n* Add `MergeMapsInto` for in-place mutation when\n  caller owns destination.\n\nHot path fixes:\n* Replace `MergeMaps(r.currentPayloads)` with\n  `CopyMap(r.currentPayloads)` to eliminates\n  allocation on every combination iteration.\n* Pre-allocate combined map once, extend in-place\n  during `ForEach` loop instead of creating new\n  map per variable (eliminates $$2Ã—N$$ allocations\n  per request).\n\nCaching with concurrency safety:\n* Cache `BuildPayloadFromOptions` computation in\n  `sync.Map` keyed by `types.Options` ptr, but\n  return copy to prevent concurrent modification.\n* Cost: shallow copy of ~10-20 entries vs. full\n  merge of vars + env (85-90% savings in typical\n  case)\n* Clear cache in `closeInternal()` to prevent\n  memory leaks when SDK instances are created or\n  destroyed.\n\nEstimated impact: 40-60% reduction in `MergeMaps`\nallocations (5.5-8.3 GB savings from original\n13.8 GB). Safe for concurrent execution and SDK\nusage with multiple instances.\n\nSigned-off-by: Dwi Siswanto <git@dw1.io>"}, {"sha": "0ab06cc4bf7554890fa6669aedc763ecdc0ef434", "parents": ["bb79a061bc5ba6b1d08a014140583da61b906ced"], "message": "test: add maps, options, variables bench\n\nSigned-off-by: Dwi Siswanto <git@dw1.io>"}, {"sha": "0c125e2224fb12b825fe2632a829d3d032ce6424", "parents": ["0ab06cc4bf7554890fa6669aedc763ecdc0ef434"], "message": "test(generators): update maps & options benchmarks\n\nSigned-off-by: Dwi Siswanto <git@dw1.io>"}], "resolved_issues": [{"org": "projectdiscovery", "repo": "nuclei", "number": -1, "state": "unknown", "title": "perf(generators): optimize `MergeMaps` to reduce allocs", "body": "## Proposed changes\r\n\r\n<!-- Describe the overall picture of your modifications to help maintainers understand the pull request. PRs are required to be associated to their related issue tickets or feature request. -->\r\n\r\nperf(generators): optimize `MergeMaps` to reduce allocs\r\n\r\n`MergeMaps` accounts for 11.41% of allocs (13.8\r\nGB) in clusterbomb mode. With 1,305 combinations\r\nper target, this function is called millions of\r\ntimes in the hot path.\r\n\r\nRCA:\r\n* Request generator calls `MergeMaps` with single\r\n  arg on every payload combination, incurring\r\n  variadic overhead.\r\n* Build request merges same maps multiple times\r\n  per request.\r\n* `BuildPayloadFromOptions` recomputes static CLI\r\n  options on every call.\r\n* Variables calls `MergeMaps` $$2Ã—N$$ times per\r\n  variable evaluation (once in loop, once in\r\n  `evaluateVariableValue`)\r\n\r\nChanges:\r\n\r\nCore optimizations in maps.go:\r\n* Pre-size merged map to avoid rehashing (30-40%\r\n  reduction)\r\n* Add `CopyMap` for efficient single-map copy\r\n  without variadic overhead.\r\n* Add `MergeMapsInto` for in-place mutation when\r\n  caller owns destination.\r\n\r\nHot path fixes:\r\n* Replace `MergeMaps(r.currentPayloads)` with\r\n  `CopyMap(r.currentPayloads)` to eliminates\r\n  allocation on every combination iteration.\r\n* Pre-allocate combined map once, extend in-place\r\n  during `ForEach` loop instead of creating new\r\n  map per variable (eliminates $$2Ã—N$$ allocations\r\n  per request).\r\n\r\nCaching with concurrency safety:\r\n* Cache `BuildPayloadFromOptions` computation in\r\n  `sync.Map` keyed by `types.Options` ptr, but\r\n  return copy to prevent concurrent modification.\r\n* Cost: shallow copy of ~10-20 entries vs. full\r\n  merge of vars + env (85-90% savings in typical\r\n  case)\r\n* Clear cache in `closeInternal()` to prevent\r\n  memory leaks when SDK instances are created or\r\n  destroyed.\r\n\r\nEstimated impact: 40-60% reduction in `MergeMaps`\r\nallocations (5.5-8.3 GB savings from original\r\n13.8 GB). Safe for concurrent execution and SDK\r\nusage with multiple instances.\r\n\r\n### Proof\r\n\r\n```console\r\n$ git checkout dev\r\n$ git cherry-pick 0ab06cc4\r\n$ go test -run - -benchmem -count 6 -bench \"^Benchmark(MergeMaps|BuildPayloadFromOptions|VariableEvaluate|VariableEvaluateScaling)\" ./pkg/protocols/common/variables/ ./pkg/protocols/common/generators/ | tee dev\r\n$ git checkout dwisiswant0/perf/generators/optimize-MergeMaps-to-reduce-allocs\r\n$ go test -run - -benchmem -count 6 -bench \"^Benchmark(MergeMaps|BuildPayloadFromOptions|VariableEvaluate|VariableEvaluateScaling)\" ./pkg/protocols/common/variables/ ./pkg/protocols/common/generators/ | tee patch\r\n$ benchstat dev patch\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/generators\r\ncpu: 11th Gen Intel(R) Core(TM) i9-11900H @ 2.50GHz\r\n                                     â”‚      dev      â”‚                patch                â”‚\r\n                                     â”‚    sec/op     â”‚    sec/op     vs base               â”‚\r\nMergeMaps/1-maps-4                      392.1n Â±  1%   400.0n Â±  1%   +2.01% (p=0.009 n=6)\r\nMergeMaps/2-maps-4                     1222.0n Â± 17%   875.1n Â±  4%  -28.38% (p=0.002 n=6)\r\nMergeMaps/3-maps-4                      1.416Âµ Â±  3%   1.082Âµ Â± 10%  -23.56% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Sequential-4   12.413Âµ Â±  4%   4.825Âµ Â±  2%  -61.13% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Parallel-4      6.004Âµ Â± 10%   2.519Âµ Â± 21%  -58.05% (p=0.002 n=6)\r\nMergeMapsInto/1-maps-4                                 192.1n Â±  8%\r\nMergeMapsInto/2-maps-4                                 335.7n Â±  5%\r\nMergeMapsInto/3-maps-4                                 484.7n Â±  1%\r\ngeomean                                 2.191Âµ         784.8n        -38.07%\r\n\r\n                                     â”‚     dev      â”‚                 patch                 â”‚\r\n                                     â”‚     B/op     â”‚     B/op      vs base                 â”‚\r\nMergeMaps/1-maps-4                       336.0 Â± 0%     336.0 Â± 0%        ~ (p=1.000 n=6) Â¹\r\nMergeMaps/2-maps-4                       952.0 Â± 0%     664.0 Â± 0%  -30.25% (p=0.002 n=6)\r\nMergeMaps/3-maps-4                       952.0 Â± 0%     664.0 Â± 0%  -30.25% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Sequential-4   9.523Ki Â± 0%   4.836Ki Â± 0%  -49.22% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Parallel-4     9.523Ki Â± 0%   4.836Ki Â± 0%  -49.22% (p=0.002 n=6)\r\nMergeMapsInto/1-maps-4                                  0.000 Â± 0%\r\nMergeMapsInto/2-maps-4                                  0.000 Â± 0%\r\nMergeMapsInto/3-maps-4                                  0.000 Â± 0%\r\ngeomean                                1.915Ki                      -33.98%               Â²\r\nÂ¹ all samples are equal\r\nÂ² summaries must be >0 to compute geomean\r\n\r\n                                     â”‚     dev     â”‚                patch                â”‚\r\n                                     â”‚  allocs/op  â”‚ allocs/op   vs base                 â”‚\r\nMergeMaps/1-maps-4                      2.000 Â± 0%   2.000 Â± 0%        ~ (p=1.000 n=6) Â¹\r\nMergeMaps/2-maps-4                      5.000 Â± 0%   4.000 Â± 0%  -20.00% (p=0.002 n=6)\r\nMergeMaps/3-maps-4                      5.000 Â± 0%   4.000 Â± 0%  -20.00% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Sequential-4   14.000 Â± 0%   4.000 Â± 0%  -71.43% (p=0.002 n=6)\r\nBuildPayloadFromOptions/Parallel-4     14.000 Â± 0%   4.000 Â± 0%  -71.43% (p=0.002 n=6)\r\nMergeMapsInto/1-maps-4                               0.000 Â± 0%\r\nMergeMapsInto/2-maps-4                               0.000 Â± 0%\r\nMergeMapsInto/3-maps-4                               0.000 Â± 0%\r\ngeomean                                 6.284                    -44.59%               Â²\r\nÂ¹ all samples are equal\r\nÂ² summaries must be >0 to compute geomean\r\n\r\npkg: github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/variables\r\n                                       â”‚      dev      â”‚                patch                â”‚\r\n                                       â”‚    sec/op     â”‚    sec/op     vs base               â”‚\r\nVariableEvaluate/Evaluate/5Variables-4    17.73Âµ Â±  1%   13.99Âµ Â±  1%  -21.07% (p=0.002 n=6)\r\nVariableEvaluate/Evaluate/Parallel-4      7.091Âµ Â± 19%   6.412Âµ Â± 16%        ~ (p=0.180 n=6)\r\nVariableEvaluateScaling/Variables-1-4    1063.5n Â±  2%   949.0n Â±  1%  -10.77% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-5-4     7.964Âµ Â±  2%   4.566Âµ Â±  2%  -42.67% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-10-4    28.66Âµ Â±  1%   13.00Âµ Â±  1%  -54.63% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-20-4   111.71Âµ Â±  6%   39.95Âµ Â±  1%  -64.24% (p=0.002 n=6)\r\ngeomean                                   12.27Âµ         7.659Âµ        -37.56%\r\n\r\n                                       â”‚      dev      â”‚                patch                â”‚\r\n                                       â”‚     B/op      â”‚     B/op      vs base               â”‚\r\nVariableEvaluate/Evaluate/5Variables-4    5.156Ki Â± 0%   3.922Ki Â± 0%  -23.94% (p=0.002 n=6)\r\nVariableEvaluate/Evaluate/Parallel-4      4.828Ki Â± 0%   3.922Ki Â± 0%  -18.77% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-1-4       720.0 Â± 0%     736.0 Â± 0%   +2.22% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-5-4     2.523Ki Â± 0%   1.289Ki Â± 0%  -48.92% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-10-4   11.719Ki Â± 0%   5.031Ki Â± 0%  -57.07% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-20-4    59.83Ki Â± 0%   22.36Ki Â± 0%  -62.63% (p=0.002 n=6)\r\ngeomean                                   5.604Ki        3.421Ki       -38.95%\r\n\r\n                                       â”‚    dev     â”‚               patch               â”‚\r\n                                       â”‚ allocs/op  â”‚ allocs/op   vs base               â”‚\r\nVariableEvaluate/Evaluate/5Variables-4   120.0 Â± 0%   117.0 Â± 0%   -2.50% (p=0.002 n=6)\r\nVariableEvaluate/Evaluate/Parallel-4     118.0 Â± 0%   117.0 Â± 0%   -0.85% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-1-4    7.000 Â± 0%   8.000 Â± 0%  +14.29% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-5-4    45.00 Â± 0%   42.00 Â± 0%   -6.67% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-10-4   147.0 Â± 0%   120.0 Â± 0%  -18.37% (p=0.002 n=6)\r\nVariableEvaluateScaling/Variables-20-4   504.0 Â± 0%   379.0 Â± 0%  -24.80% (p=0.002 n=6)\r\ngeomean                                  83.15        77.05        -7.34%\r\n```\r\n\r\n#### Summary\r\n\r\n**pkg/protocols/common/generators:**\r\n\r\n- **38.07% faster** (geomean)\r\n- **33.98% less memory** (geomean)\r\n- **44.59% fewer allocations** (geomean)\r\n\r\n**pkg/protocols/common/variables:**\r\n- **37.56% faster** (geomean)\r\n- **38.95% less memory** (geomean)\r\n- **7.34% fewer allocations** (geomean)\r\n\r\n**Core map ops:**\r\n\r\n* `MergeMaps` pre-sizing impact\r\n  * `MergeMaps/2-maps`:\r\n    * -28.38% faster\r\n    * -30.25% memory\r\n    * -20.00% allocs\r\n  * `MergeMaps/3-maps`:\r\n    * -23.56% faster\r\n    * -30.25% memory\r\n    * -20.00% allocs\r\n\r\n Pre-calc total cap before alloc eliminates rehashing overhead and reduces allocs by 20%.\r\n\r\n* `MergeMapsInto` (new func):\r\n  * 0 allocs for hot paths where dst is already owned by caller.\r\n\r\n**BuildPayloadFromOptions:**\r\n\r\nComparing dev (no caching, full merge every time) vs. patch (cached with copy-on-return):\r\n\r\n- **~60% faster** by caching computation.\r\n- **~50% less memory** despite returning copy.\r\n- **~71% fewer allocations** (cache hit eliminates merge overhead).\r\n\r\n**Variable Evaluation:**\r\n\r\nComparing dev (repeated `MergeMaps` in loop) vs. patch (single merge + in-place extension):\r\n\r\n**Insights**: \r\n- **dev**: $$2Ã—N$$ `MergeMaps` calls per evaluation (exponential waste).\r\n- **patch**: $$1+N$$ merge in-place extensions (linear).\r\n- **improvement**: 64% faster, 63% less memory at 20 variables.\r\n- **trade-off**: small overhead at $$N=1$$ (+14% allocs) acceptable for massive gains at realistic scales.\r\n\r\n#### Est'ed Impact\r\n\r\nIn clusterbomb mode with:\r\n- 1,305 payload combinations per target.\r\n- 1,000 targets.\r\n- 5 variables per template.\r\n- 17 protocol calls per request.\r\n\r\n**Original problem**: ~13.8 GB allocated in `MergeMaps` (11.41% of total allocations)\r\n\r\n**Measured improvements** (from benchstat):\r\n- `MergeMaps` pre-sizing: **28-30% faster**, **30% less memory**, **20% fewer allocations**\r\n- `MergeMapsInto` (hot paths): **0 allocations** (100% reduction)\r\n- `BuildPayloadFromOptions`: **60% faster**, **50% less memory**, **71% fewer allocations**\r\n- Variable evaluation (20 vars): **64% faster**, **63% less memory**, **25% fewer allocations**\r\n\r\n**Conservative estimate** (weighted by call frequency):\r\n- Hot path savings (request generator using `CopyMap` instead of `MergeMaps`): minimal allocation change.\r\n- Critical savings (variables using `MergeMapsInto`): 100% allocation elimination on $$N$$ evaluations per combination.\r\n- Caching savings (`BuildPayloadFromOptions`): 71% allocation reduction, called once per template.\r\n- Variable loop savings: 63% memory reduction on every combination.\r\n\r\n**Expected total `MergeMaps` allocation reduction: 55-70%**\r\n\r\nThis translates to **7.6-9.7 GB saved** from the original 13.8 GB, with significant CPU time improvements (38% faster geomean).\r\n\r\n**Additional benefits**:\r\n- **CPU**: 38% faster geomean in generators, 38% faster in variables.\r\n- **Scalability**: 64% improvement at 20 variables (common in complex templates).\r\n- **Concurrency**: 58% faster parallel performance in caching layer.\r\n\r\n## Checklist\r\n\r\n<!-- Put an \"x\" in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code. -->\r\n\r\n- [ ] Pull request is created against the [dev](https://github.com/projectdiscovery/nuclei/tree/dev) branch\r\n- [ ] All checks passed (lint, unit/integration/regression tests etc.) with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n\r\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\r\n## Summary by CodeRabbit\r\n\r\n* **Refactor**\r\n  * Optimized payload generation and variable evaluation through performance improvements and memory-efficient map operations.\r\n  * Enhanced thread-safety for concurrent SDK instances with cache-aware map handling.\r\n\r\n* **Tests**\r\n  * Added comprehensive test coverage for concurrency safety, caching behavior, and memory management.\r\n\r\n<sub>âœï¸ Tip: You can customize this high-level summary in your review settings.</sub>\r\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->"}], "fix_patch": "diff --git a/lib/sdk.go b/lib/sdk.go\nindex 6bd2a93ba0..a70c02d61c 100644\n--- a/lib/sdk.go\n+++ b/lib/sdk.go\n@@ -19,6 +19,7 @@ import (\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/output\"\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/progress\"\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/protocols\"\n+\t\"github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/generators\"\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/hosterrorscache\"\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/interactsh\"\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/protocolinit\"\n@@ -238,6 +239,9 @@ func (e *NucleiEngine) closeInternal() {\n \tif e.tmpDir != \"\" {\n \t\t_ = os.RemoveAll(e.tmpDir)\n \t}\n+\tif e.opts != nil {\n+\t\tgenerators.ClearOptionsPayloadMap(e.opts)\n+\t}\n }\n \n // Close all resources used by nuclei engine\ndiff --git a/pkg/protocols/common/generators/maps.go b/pkg/protocols/common/generators/maps.go\nindex ed2fc3a64f..6464d53eeb 100644\n--- a/pkg/protocols/common/generators/maps.go\n+++ b/pkg/protocols/common/generators/maps.go\n@@ -44,20 +44,51 @@ func MergeMapsMany(maps ...interface{}) map[string][]string {\n \treturn m\n }\n \n-// MergeMaps merges two maps into a new map\n+// MergeMaps merges multiple maps into a new map.\n+//\n+// Use [CopyMap] if you need to copy a single map.\n+// Use [MergeMapsInto] to merge into an existing map.\n func MergeMaps(maps ...map[string]interface{}) map[string]interface{} {\n-\tmerged := make(map[string]interface{})\n+\tmapsLen := 0\n+\tfor _, m := range maps {\n+\t\tmapsLen += len(m)\n+\t}\n+\n+\tmerged := make(map[string]interface{}, mapsLen)\n \tfor _, m := range maps {\n \t\tmaps0.Copy(merged, m)\n \t}\n+\n \treturn merged\n }\n \n+// CopyMap creates a shallow copy of a single map.\n+func CopyMap(m map[string]interface{}) map[string]interface{} {\n+\tif m == nil {\n+\t\treturn nil\n+\t}\n+\n+\tresult := make(map[string]interface{}, len(m))\n+\tmaps0.Copy(result, m)\n+\n+\treturn result\n+}\n+\n+// MergeMapsInto copies all entries from src maps into dst (mutating dst).\n+//\n+// Use when dst is a fresh map the caller owns and wants to avoid allocation.\n+func MergeMapsInto(dst map[string]interface{}, srcs ...map[string]interface{}) {\n+\tfor _, src := range srcs {\n+\t\tmaps0.Copy(dst, src)\n+\t}\n+}\n+\n // ExpandMapValues converts values from flat string to string slice\n func ExpandMapValues(m map[string]string) map[string][]string {\n \tm1 := make(map[string][]string, len(m))\n \tfor k, v := range m {\n \t\tm1[k] = []string{v}\n \t}\n+\n \treturn m1\n }\ndiff --git a/pkg/protocols/common/generators/options.go b/pkg/protocols/common/generators/options.go\nindex bc077547a8..92c8b43551 100644\n--- a/pkg/protocols/common/generators/options.go\n+++ b/pkg/protocols/common/generators/options.go\n@@ -1,12 +1,32 @@\n package generators\n \n import (\n+\t\"sync\"\n+\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/types\"\n )\n \n-// BuildPayloadFromOptions returns a map with the payloads provided via CLI\n+// optionsPayloadMap caches the result of BuildPayloadFromOptions per options\n+// pointer. This supports multiple SDK instances with different options running\n+// concurrently.\n+var optionsPayloadMap sync.Map // map[*types.Options]map[string]interface{}\n+\n+// BuildPayloadFromOptions returns a map with the payloads provided via CLI.\n+//\n+// The result is cached per options pointer since options don't change during a run.\n+// Returns a copy of the cached map to prevent concurrent modification issues.\n+// Safe for concurrent use with multiple SDK instances.\n func BuildPayloadFromOptions(options *types.Options) map[string]interface{} {\n+\tif options == nil {\n+\t\treturn make(map[string]interface{})\n+\t}\n+\n+\tif cached, ok := optionsPayloadMap.Load(options); ok {\n+\t\treturn CopyMap(cached.(map[string]interface{}))\n+\t}\n+\n \tm := make(map[string]interface{})\n+\n \t// merge with vars\n \tif !options.Vars.IsEmpty() {\n \t\tm = MergeMaps(m, options.Vars.AsMap())\n@@ -16,5 +36,18 @@ func BuildPayloadFromOptions(options *types.Options) map[string]interface{} {\n \tif options.EnvironmentVariables {\n \t\tm = MergeMaps(EnvVars(), m)\n \t}\n-\treturn m\n+\n+\tactual, _ := optionsPayloadMap.LoadOrStore(options, m)\n+\n+\t// Return a copy to prevent concurrent writes to the cached map\n+\treturn CopyMap(actual.(map[string]interface{}))\n+}\n+\n+// ClearOptionsPayloadMap clears the cached options payload.\n+// SDK users should call this when disposing of a NucleiEngine instance\n+// to prevent memory leaks if creating many short-lived instances.\n+func ClearOptionsPayloadMap(options *types.Options) {\n+\tif options != nil {\n+\t\toptionsPayloadMap.Delete(options)\n+\t}\n }\ndiff --git a/pkg/protocols/common/variables/variables.go b/pkg/protocols/common/variables/variables.go\nindex fa5cc1dbc6..76bcacd9a0 100644\n--- a/pkg/protocols/common/variables/variables.go\n+++ b/pkg/protocols/common/variables/variables.go\n@@ -70,18 +70,23 @@ func (variables *Variable) UnmarshalJSON(data []byte) error {\n // Evaluate returns a finished map of variables based on set values\n func (variables *Variable) Evaluate(values map[string]interface{}) map[string]interface{} {\n \tresult := make(map[string]interface{}, variables.Len())\n+\tcombined := make(map[string]interface{}, len(values)+variables.Len())\n+\tgenerators.MergeMapsInto(combined, values)\n+\n \tvariables.ForEach(func(key string, value interface{}) {\n \t\tif sliceValue, ok := value.([]interface{}); ok {\n \t\t\t// slices cannot be evaluated\n \t\t\tresult[key] = sliceValue\n+\t\t\tcombined[key] = sliceValue\n \t\t\treturn\n \t\t}\n \t\tvalueString := types.ToString(value)\n-\t\tcombined := generators.MergeMaps(values, result)\n-\t\tif value, ok := combined[key]; ok {\n-\t\t\tvalueString = types.ToString(value)\n+\t\tif existingValue, ok := combined[key]; ok {\n+\t\t\tvalueString = types.ToString(existingValue)\n \t\t}\n-\t\tresult[key] = evaluateVariableValue(valueString, combined, result)\n+\t\tevaluated := evaluateVariableValueWithMap(valueString, combined)\n+\t\tresult[key] = evaluated\n+\t\tcombined[key] = evaluated\n \t})\n \treturn result\n }\n@@ -98,29 +103,36 @@ func (variables *Variable) GetAll() map[string]interface{} {\n // EvaluateWithInteractsh returns evaluation results of variables with interactsh\n func (variables *Variable) EvaluateWithInteractsh(values map[string]interface{}, interact *interactsh.Client) (map[string]interface{}, []string) {\n \tresult := make(map[string]interface{}, variables.Len())\n+\tcombined := make(map[string]interface{}, len(values)+variables.Len())\n+\tgenerators.MergeMapsInto(combined, values)\n \n \tvar interactURLs []string\n \tvariables.ForEach(func(key string, value interface{}) {\n \t\tif sliceValue, ok := value.([]interface{}); ok {\n \t\t\t// slices cannot be evaluated\n \t\t\tresult[key] = sliceValue\n+\t\t\tcombined[key] = sliceValue\n \t\t\treturn\n \t\t}\n \t\tvalueString := types.ToString(value)\n+\t\tif existingValue, ok := combined[key]; ok {\n+\t\t\tvalueString = types.ToString(existingValue)\n+\t\t}\n \t\tif strings.Contains(valueString, \"interactsh-url\") {\n \t\t\tvalueString, interactURLs = interact.Replace(valueString, interactURLs)\n \t\t}\n-\t\tcombined := generators.MergeMaps(values, result)\n-\t\tif value, ok := combined[key]; ok {\n-\t\t\tvalueString = types.ToString(value)\n-\t\t}\n-\t\tresult[key] = evaluateVariableValue(valueString, combined, result)\n+\t\tevaluated := evaluateVariableValueWithMap(valueString, combined)\n+\t\tresult[key] = evaluated\n+\t\tcombined[key] = evaluated\n \t})\n \treturn result, interactURLs\n }\n \n-// evaluateVariableValue expression and returns final value\n-func evaluateVariableValue(expression string, values, processing map[string]interface{}) string {\n+// evaluateVariableValue expression and returns final value.\n+//\n+// Deprecated: use evaluateVariableValueWithMap instead to avoid repeated map\n+// merging overhead.\n+func evaluateVariableValue(expression string, values, processing map[string]interface{}) string { // nolint\n \tfinalMap := generators.MergeMaps(values, processing)\n \tresult, err := expressions.Evaluate(expression, finalMap)\n \tif err != nil {\n@@ -130,6 +142,16 @@ func evaluateVariableValue(expression string, values, processing map[string]inte\n \treturn result\n }\n \n+// evaluateVariableValueWithMap evaluates an expression with a pre-merged map.\n+func evaluateVariableValueWithMap(expression string, combinedMap map[string]interface{}) string {\n+\tresult, err := expressions.Evaluate(expression, combinedMap)\n+\tif err != nil {\n+\t\treturn expression\n+\t}\n+\n+\treturn result\n+}\n+\n // checkForLazyEval checks if the variables have any lazy evaluation i.e any dsl function\n // and sets the flag accordingly.\n func (variables *Variable) checkForLazyEval() bool {\ndiff --git a/pkg/protocols/http/request_generator.go b/pkg/protocols/http/request_generator.go\nindex 4c4c701a86..1ef1913959 100644\n--- a/pkg/protocols/http/request_generator.go\n+++ b/pkg/protocols/http/request_generator.go\n@@ -84,10 +84,10 @@ func (r *requestGenerator) nextValue() (value string, payloads map[string]interf\n \t\t\tr.applyMark(request, Once)\n \t\t}\n \t\tif hasPayloadIterator {\n-\t\t\treturn request, generators.MergeMaps(r.currentPayloads), r.okCurrentPayload\n+\t\t\treturn request, generators.CopyMap(r.currentPayloads), r.okCurrentPayload\n \t\t}\n \t\t// next should return a copy of payloads and not pointer to payload to avoid data race\n-\t\treturn request, generators.MergeMaps(r.currentPayloads), true\n+\t\treturn request, generators.CopyMap(r.currentPayloads), true\n \t} else {\n \t\treturn \"\", nil, false\n \t}\n", "test_patch": "diff --git a/pkg/protocols/common/generators/maps_bench_test.go b/pkg/protocols/common/generators/maps_bench_test.go\nnew file mode 100644\nindex 0000000000..25088a520c\n--- /dev/null\n+++ b/pkg/protocols/common/generators/maps_bench_test.go\n@@ -0,0 +1,109 @@\n+package generators\n+\n+import (\n+\t\"fmt\"\n+\t\"testing\"\n+)\n+\n+func BenchmarkMergeMaps(b *testing.B) {\n+\tmap1 := map[string]interface{}{\n+\t\t\"key1\": \"value1\",\n+\t\t\"key2\": \"value2\",\n+\t\t\"key3\": \"value3\",\n+\t\t\"key4\": \"value4\",\n+\t\t\"key5\": \"value5\",\n+\t}\n+\tmap2 := map[string]interface{}{\n+\t\t\"key6\":  \"value6\",\n+\t\t\"key7\":  \"value7\",\n+\t\t\"key8\":  \"value8\",\n+\t\t\"key9\":  \"value9\",\n+\t\t\"key10\": \"value10\",\n+\t}\n+\tmap3 := map[string]interface{}{\n+\t\t\"key11\": \"value11\",\n+\t\t\"key12\": \"value12\",\n+\t\t\"key13\": \"value13\",\n+\t}\n+\n+\tfor i := 1; i <= 3; i++ {\n+\t\tb.Run(fmt.Sprintf(\"%d-maps\", i), func(b *testing.B) {\n+\t\t\tb.ReportAllocs()\n+\t\t\tfor b.Loop() {\n+\t\t\t\tswitch i {\n+\t\t\t\tcase 1:\n+\t\t\t\t\t_ = MergeMaps(map1)\n+\t\t\t\tcase 2:\n+\t\t\t\t\t_ = MergeMaps(map1, map2)\n+\t\t\t\tcase 3:\n+\t\t\t\t\t_ = MergeMaps(map1, map2, map3)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+func BenchmarkCopyMap(b *testing.B) {\n+\tmap1 := map[string]interface{}{\n+\t\t\"key1\": \"value1\",\n+\t\t\"key2\": \"value2\",\n+\t\t\"key3\": \"value3\",\n+\t\t\"key4\": \"value4\",\n+\t\t\"key5\": \"value5\",\n+\t}\n+\n+\tfor i := 1; i <= 1; i++ {\n+\t\tb.Run(fmt.Sprintf(\"%d-maps\", i), func(b *testing.B) {\n+\t\t\tb.ReportAllocs()\n+\t\t\tfor b.Loop() {\n+\t\t\t\tswitch i {\n+\t\t\t\tcase 1:\n+\t\t\t\t\t_ = CopyMap(map1)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+func BenchmarkMergeMapsInto(b *testing.B) {\n+\tmap1 := map[string]interface{}{\n+\t\t\"key1\": \"value1\",\n+\t\t\"key2\": \"value2\",\n+\t\t\"key3\": \"value3\",\n+\t\t\"key4\": \"value4\",\n+\t\t\"key5\": \"value5\",\n+\t}\n+\tmap2 := map[string]interface{}{\n+\t\t\"key6\":  \"value6\",\n+\t\t\"key7\":  \"value7\",\n+\t\t\"key8\":  \"value8\",\n+\t\t\"key9\":  \"value9\",\n+\t\t\"key10\": \"value10\",\n+\t}\n+\tmap3 := map[string]interface{}{\n+\t\t\"key11\": \"value11\",\n+\t\t\"key12\": \"value12\",\n+\t\t\"key13\": \"value13\",\n+\t}\n+\tmap4 := map[string]interface{}{\n+\t\t\"key14\": \"value14\",\n+\t\t\"key15\": \"value15\",\n+\t\t\"key16\": \"value16\",\n+\t}\n+\n+\tfor i := 1; i <= 3; i++ {\n+\t\tb.Run(fmt.Sprintf(\"%d-maps\", i), func(b *testing.B) {\n+\t\t\tb.ReportAllocs()\n+\t\t\tfor b.Loop() {\n+\t\t\t\tswitch i {\n+\t\t\t\tcase 1:\n+\t\t\t\t\tMergeMapsInto(map1, map2)\n+\t\t\t\tcase 2:\n+\t\t\t\t\tMergeMapsInto(map1, map2, map3)\n+\t\t\t\tcase 3:\n+\t\t\t\t\tMergeMapsInto(map1, map2, map3, map4)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t})\n+\t}\n+}\ndiff --git a/pkg/protocols/common/generators/options_bench_test.go b/pkg/protocols/common/generators/options_bench_test.go\nnew file mode 100644\nindex 0000000000..fe59193d8e\n--- /dev/null\n+++ b/pkg/protocols/common/generators/options_bench_test.go\n@@ -0,0 +1,45 @@\n+package generators\n+\n+import (\n+\t\"testing\"\n+\n+\t\"github.com/projectdiscovery/goflags\"\n+\t\"github.com/projectdiscovery/nuclei/v3/pkg/types\"\n+)\n+\n+func BenchmarkBuildPayloadFromOptions(b *testing.B) {\n+\t// Setup options with vars and env vars\n+\tvars := goflags.RuntimeMap{}\n+\t_ = vars.Set(\"key1=value1\")\n+\t_ = vars.Set(\"key2=value2\")\n+\t_ = vars.Set(\"key3=value3\")\n+\t_ = vars.Set(\"key4=value4\")\n+\t_ = vars.Set(\"key5=value5\")\n+\n+\topts := &types.Options{\n+\t\tVars:                 vars,\n+\t\tEnvironmentVariables: true, // This adds more entries\n+\t}\n+\n+\tb.Run(\"Sequential\", func(b *testing.B) {\n+\t\tClearOptionsPayloadMap(opts)\n+\n+\t\tb.ReportAllocs()\n+\t\tfor b.Loop() {\n+\t\t\t_ = BuildPayloadFromOptions(opts)\n+\t\t}\n+\t})\n+\n+\tb.Run(\"Parallel\", func(b *testing.B) {\n+\t\tClearOptionsPayloadMap(opts)\n+\n+\t\tb.ReportAllocs()\n+\t\tb.RunParallel(func(pb *testing.PB) {\n+\t\t\tfor pb.Next() {\n+\t\t\t\tm := BuildPayloadFromOptions(opts)\n+\t\t\t\t// Simulate typical usage - read a value\n+\t\t\t\t_ = m[\"key1\"]\n+\t\t\t}\n+\t\t})\n+\t})\n+}\ndiff --git a/pkg/protocols/common/generators/options_test.go b/pkg/protocols/common/generators/options_test.go\nnew file mode 100644\nindex 0000000000..8d85781455\n--- /dev/null\n+++ b/pkg/protocols/common/generators/options_test.go\n@@ -0,0 +1,92 @@\n+package generators\n+\n+import (\n+\t\"sync\"\n+\t\"testing\"\n+\n+\t\"github.com/projectdiscovery/goflags\"\n+\t\"github.com/projectdiscovery/nuclei/v3/pkg/types\"\n+\t\"github.com/stretchr/testify/require\"\n+)\n+\n+func TestBuildPayloadFromOptionsConcurrency(t *testing.T) {\n+\t// Test that BuildPayloadFromOptions is safe for concurrent use\n+\t// and returns independent copies that can be modified without races\n+\tvars := goflags.RuntimeMap{}\n+\t_ = vars.Set(\"key=value\")\n+\n+\topts := &types.Options{\n+\t\tVars: vars,\n+\t}\n+\n+\tconst numGoroutines = 100\n+\tvar wg sync.WaitGroup\n+\twg.Add(numGoroutines)\n+\n+\t// Each goroutine gets a map and modifies it\n+\tfor i := 0; i < numGoroutines; i++ {\n+\t\tgo func(id int) {\n+\t\t\tdefer wg.Done()\n+\n+\t\t\t// Get the map (should be a copy of cached data)\n+\t\t\tm := BuildPayloadFromOptions(opts)\n+\n+\t\t\t// Modify it - this should not cause races\n+\t\t\tm[\"goroutine_id\"] = id\n+\t\t\tm[\"test_key\"] = \"test_value\"\n+\n+\t\t\t// Verify original cached value is present\n+\t\t\trequire.Equal(t, \"value\", m[\"key\"])\n+\t\t}(i)\n+\t}\n+\n+\twg.Wait()\n+}\n+\n+func TestBuildPayloadFromOptionsCaching(t *testing.T) {\n+\t// Test that caching actually works\n+\tvars := goflags.RuntimeMap{}\n+\t_ = vars.Set(\"cached=yes\")\n+\n+\topts := &types.Options{\n+\t\tVars:                 vars,\n+\t\tEnvironmentVariables: false,\n+\t}\n+\n+\t// First call - builds and caches\n+\tm1 := BuildPayloadFromOptions(opts)\n+\trequire.Equal(t, \"yes\", m1[\"cached\"])\n+\n+\t// Second call - should return copy of cached result\n+\tm2 := BuildPayloadFromOptions(opts)\n+\trequire.Equal(t, \"yes\", m2[\"cached\"])\n+\n+\t// Modify m1 - should not affect m2 since they're copies\n+\tm1[\"modified\"] = \"in_m1\"\n+\trequire.NotContains(t, m2, \"modified\")\n+\n+\t// Modify m2 - should not affect future calls\n+\tm2[\"modified\"] = \"in_m2\"\n+\tm3 := BuildPayloadFromOptions(opts)\n+\trequire.NotContains(t, m3, \"modified\")\n+}\n+\n+func TestClearOptionsPayloadMap(t *testing.T) {\n+\tvars := goflags.RuntimeMap{}\n+\t_ = vars.Set(\"temp=data\")\n+\n+\topts := &types.Options{\n+\t\tVars: vars,\n+\t}\n+\n+\t// Build and cache\n+\tm1 := BuildPayloadFromOptions(opts)\n+\trequire.Equal(t, \"data\", m1[\"temp\"])\n+\n+\t// Clear the cache\n+\tClearOptionsPayloadMap(opts)\n+\n+\t// Verify it still works (rebuilds)\n+\tm2 := BuildPayloadFromOptions(opts)\n+\trequire.Equal(t, \"data\", m2[\"temp\"])\n+}\ndiff --git a/pkg/protocols/common/variables/variables_bench_test.go b/pkg/protocols/common/variables/variables_bench_test.go\nnew file mode 100644\nindex 0000000000..c94a64a051\n--- /dev/null\n+++ b/pkg/protocols/common/variables/variables_bench_test.go\n@@ -0,0 +1,80 @@\n+package variables\n+\n+import (\n+\t\"fmt\"\n+\t\"testing\"\n+\n+\t\"github.com/projectdiscovery/nuclei/v3/pkg/utils\"\n+)\n+\n+func BenchmarkVariableEvaluate(b *testing.B) {\n+\t// Setup variables with chained references and DSL functions\n+\tvariables := &Variable{\n+\t\tLazyEval:                  true,\n+\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(5),\n+\t}\n+\tvariables.Set(\"base\", \"testvalue\")\n+\tvariables.Set(\"derived1\", \"{{base}}_suffix\")\n+\tvariables.Set(\"derived2\", \"{{md5(derived1)}}\")\n+\tvariables.Set(\"derived3\", \"prefix_{{derived2}}\")\n+\tvariables.Set(\"final\", \"{{derived3}}_end\")\n+\n+\tinputValues := map[string]interface{}{\n+\t\t\"BaseURL\": \"http://example.com\",\n+\t\t\"Host\":    \"example.com\",\n+\t\t\"Path\":    \"/api/v1\",\n+\t}\n+\n+\tb.Run(\"Evaluate\", func(b *testing.B) {\n+\t\tb.Run(\"5Variables\", func(b *testing.B) {\n+\t\t\tb.ReportAllocs()\n+\t\t\tfor b.Loop() {\n+\t\t\t\t_ = variables.Evaluate(inputValues)\n+\t\t\t}\n+\t\t})\n+\n+\t\tb.Run(\"Parallel\", func(b *testing.B) {\n+\t\t\tb.ReportAllocs()\n+\t\t\tb.RunParallel(func(pb *testing.PB) {\n+\t\t\t\tfor pb.Next() {\n+\t\t\t\t\t_ = variables.Evaluate(inputValues)\n+\t\t\t\t}\n+\t\t\t})\n+\t\t})\n+\t})\n+}\n+\n+func BenchmarkVariableEvaluateScaling(b *testing.B) {\n+\t// Test how the optimization scales with different variable counts\n+\tinputValues := map[string]interface{}{\n+\t\t\"BaseURL\": \"http://example.com\",\n+\t\t\"Host\":    \"example.com\",\n+\t}\n+\n+\tbenchmarkSizes := []int{1, 5, 10, 20}\n+\n+\tfor _, size := range benchmarkSizes {\n+\t\tvariables := &Variable{\n+\t\t\tLazyEval:                  true,\n+\t\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(size),\n+\t\t}\n+\n+\t\t// Create chain of variables\n+\t\tfor i := range size {\n+\t\t\tvarName := fmt.Sprintf(\"var%d\", i)\n+\t\t\tif i == 0 {\n+\t\t\t\tvariables.Set(varName, \"initial\")\n+\t\t\t} else {\n+\t\t\t\tprevVarName := fmt.Sprintf(\"var%d\", i-1)\n+\t\t\t\tvariables.Set(varName, fmt.Sprintf(\"{{%s}}_step\", prevVarName))\n+\t\t\t}\n+\t\t}\n+\n+\t\tb.Run(fmt.Sprintf(\"Variables-%d\", size), func(b *testing.B) {\n+\t\t\tb.ReportAllocs()\n+\t\t\tfor b.Loop() {\n+\t\t\t\t_ = variables.Evaluate(inputValues)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\ndiff --git a/pkg/protocols/common/variables/variables_test.go b/pkg/protocols/common/variables/variables_test.go\nindex cbf560b4eb..0089c92c66 100644\n--- a/pkg/protocols/common/variables/variables_test.go\n+++ b/pkg/protocols/common/variables/variables_test.go\n@@ -4,6 +4,7 @@ import (\n \t\"testing\"\n \t\"time\"\n \n+\t\"github.com/projectdiscovery/nuclei/v3/pkg/protocols/common/interactsh\"\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/utils\"\n \t\"github.com/projectdiscovery/nuclei/v3/pkg/utils/json\"\n \t\"github.com/stretchr/testify/require\"\n@@ -147,3 +148,176 @@ func TestCheckForLazyEval(t *testing.T) {\n \t\trequire.True(t, variables.LazyEval, \"LazyEval flag should be true\")\n \t})\n }\n+\n+func TestVariablesEvaluateChained(t *testing.T) {\n+\tt.Run(\"chained-variable-references\", func(t *testing.T) {\n+\t\t// Test that variables can reference previously defined variables\n+\t\t// and that input values (like BaseURL) are available for evaluation\n+\t\t// but not included in the result\n+\t\tvariables := &Variable{\n+\t\t\tLazyEval:                  true, // skip auto-evaluation in UnmarshalYAML\n+\t\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(3),\n+\t\t}\n+\t\tvariables.Set(\"a\", \"hello\")\n+\t\tvariables.Set(\"b\", \"{{a}} world\")\n+\t\tvariables.Set(\"c\", \"{{b}}!\")\n+\n+\t\tinputValues := map[string]interface{}{\n+\t\t\t\"BaseURL\": \"http://example.com\",\n+\t\t\t\"Host\":    \"example.com\",\n+\t\t}\n+\n+\t\tresult := variables.Evaluate(inputValues)\n+\n+\t\t// Result should contain only the defined variables, not input values\n+\t\trequire.Len(t, result, 3, \"result should contain exactly 3 variables\")\n+\t\trequire.NotContains(t, result, \"BaseURL\", \"result should not contain input values\")\n+\t\trequire.NotContains(t, result, \"Host\", \"result should not contain input values\")\n+\n+\t\t// Chained evaluation should work correctly\n+\t\trequire.Equal(t, \"hello\", result[\"a\"])\n+\t\trequire.Equal(t, \"hello world\", result[\"b\"])\n+\t\trequire.Equal(t, \"hello world!\", result[\"c\"])\n+\t})\n+\n+\tt.Run(\"variables-using-input-values\", func(t *testing.T) {\n+\t\t// Test that variables can use input values in expressions\n+\t\tvariables := &Variable{\n+\t\t\tLazyEval:                  true,\n+\t\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(2),\n+\t\t}\n+\t\tvariables.Set(\"api_url\", \"{{BaseURL}}/api/v1\")\n+\t\tvariables.Set(\"full_path\", \"{{api_url}}/users\")\n+\n+\t\tinputValues := map[string]interface{}{\n+\t\t\t\"BaseURL\": \"http://example.com\",\n+\t\t}\n+\n+\t\tresult := variables.Evaluate(inputValues)\n+\n+\t\trequire.Len(t, result, 2)\n+\t\trequire.Equal(t, \"http://example.com/api/v1\", result[\"api_url\"])\n+\t\trequire.Equal(t, \"http://example.com/api/v1/users\", result[\"full_path\"])\n+\t\trequire.NotContains(t, result, \"BaseURL\")\n+\t})\n+\n+\tt.Run(\"mixed-expressions-and-chaining\", func(t *testing.T) {\n+\t\t// Test combining DSL functions with chained variables\n+\t\tvariables := &Variable{\n+\t\t\tLazyEval:                  true,\n+\t\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(3),\n+\t\t}\n+\t\tvariables.Set(\"token\", \"secret123\")\n+\t\tvariables.Set(\"hashed\", \"{{md5(token)}}\")\n+\t\tvariables.Set(\"header\", \"X-Auth: {{hashed}}\")\n+\n+\t\tresult := variables.Evaluate(map[string]interface{}{})\n+\n+\t\trequire.Equal(t, \"secret123\", result[\"token\"])\n+\t\trequire.Equal(t, \"5d7845ac6ee7cfffafc5fe5f35cf666d\", result[\"hashed\"]) // md5(\"secret123\")\n+\t\trequire.Equal(t, \"X-Auth: 5d7845ac6ee7cfffafc5fe5f35cf666d\", result[\"header\"])\n+\t})\n+\n+\tt.Run(\"evaluation-order-preserved\", func(t *testing.T) {\n+\t\t// Test that evaluation follows insertion order\n+\t\t// (important for variables that depend on previously defined ones)\n+\t\tvariables := &Variable{\n+\t\t\tLazyEval:                  true,\n+\t\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(4),\n+\t\t}\n+\t\tvariables.Set(\"step1\", \"A\")\n+\t\tvariables.Set(\"step2\", \"{{step1}}B\")\n+\t\tvariables.Set(\"step3\", \"{{step2}}C\")\n+\t\tvariables.Set(\"step4\", \"{{step3}}D\")\n+\n+\t\tresult := variables.Evaluate(map[string]interface{}{})\n+\n+\t\trequire.Equal(t, \"A\", result[\"step1\"])\n+\t\trequire.Equal(t, \"AB\", result[\"step2\"])\n+\t\trequire.Equal(t, \"ABC\", result[\"step3\"])\n+\t\trequire.Equal(t, \"ABCD\", result[\"step4\"])\n+\t})\n+}\n+\n+func TestEvaluateWithInteractshOverrideOrder(t *testing.T) {\n+\t// This test demonstrates a bug where interactsh URL replacement is wasted\n+\t// when an input value exists for the same variable key.\n+\t//\n+\t// Bug scenario:\n+\t// 1. Variable \"callback\" is defined with \"{{interactsh-url}}\"\n+\t// 2. Input values contain \"callback\" with some other value\n+\t// 3. The interactsh-url is replaced first (wasting an interactsh URL)\n+\t// 4. Then immediately overwritten by the input value\n+\t//\n+\t// Expected behavior: Input override should be checked FIRST, then interactsh\n+\t// replacement should happen on the final valueString.\n+\n+\tt.Run(\"interactsh-replacement-with-input-override\", func(t *testing.T) {\n+\t\tvariables := &Variable{\n+\t\t\tLazyEval:                  true,\n+\t\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(1),\n+\t\t}\n+\t\tvariables.Set(\"callback\", \"{{interactsh-url}}\")\n+\n+\t\t// Input provides an override that also contains interactsh-url\n+\t\tinputValues := map[string]interface{}{\n+\t\t\t\"callback\": \"https://custom.{{interactsh-url}}/path\",\n+\t\t}\n+\n+\t\t// Create a real interactsh client for testing\n+\t\tclient, err := interactsh.New(&interactsh.Options{\n+\t\t\tServerURL:           \"oast.fun\",\n+\t\t\tCacheSize:           100,\n+\t\t\tEviction:            60 * time.Second,\n+\t\t\tCooldownPeriod:      5 * time.Second,\n+\t\t\tPollDuration:        5 * time.Second,\n+\t\t\tDisableHttpFallback: true,\n+\t\t})\n+\t\trequire.NoError(t, err, \"could not create interactsh client\")\n+\t\tdefer client.Close()\n+\n+\t\tresult, urls := variables.EvaluateWithInteractsh(inputValues, client)\n+\n+\t\t// The input override contains interactsh-url, so it should be replaced\n+\t\t// and we should have exactly 1 URL from the input override\n+\t\trequire.Len(t, urls, 1, \"should have 1 interactsh URL from input override\")\n+\n+\t\t// The result should use the input override (with interactsh replaced)\n+\t\trequire.Contains(t, result[\"callback\"], \"https://custom.\", \"should use input override pattern\")\n+\t\trequire.Contains(t, result[\"callback\"], \"/path\", \"should use input override pattern\")\n+\t\trequire.NotContains(t, result[\"callback\"], \"{{interactsh-url}}\", \"interactsh should be replaced\")\n+\t})\n+\n+\tt.Run(\"interactsh-replacement-without-input-override\", func(t *testing.T) {\n+\t\tvariables := &Variable{\n+\t\t\tLazyEval:                  true,\n+\t\t\tInsertionOrderedStringMap: *utils.NewEmptyInsertionOrderedStringMap(1),\n+\t\t}\n+\t\tvariables.Set(\"callback\", \"{{interactsh-url}}\")\n+\n+\t\t// No input override for \"callback\"\n+\t\tinputValues := map[string]interface{}{\n+\t\t\t\"other_key\": \"other_value\",\n+\t\t}\n+\n+\t\tclient, err := interactsh.New(&interactsh.Options{\n+\t\t\tServerURL:           \"oast.fun\",\n+\t\t\tCacheSize:           100,\n+\t\t\tEviction:            60 * time.Second,\n+\t\t\tCooldownPeriod:      5 * time.Second,\n+\t\t\tPollDuration:        5 * time.Second,\n+\t\t\tDisableHttpFallback: true,\n+\t\t})\n+\t\trequire.NoError(t, err, \"could not create interactsh client\")\n+\t\tdefer client.Close()\n+\n+\t\tresult, urls := variables.EvaluateWithInteractsh(inputValues, client)\n+\n+\t\t// Should have 1 URL from the variable definition\n+\t\trequire.Len(t, urls, 1, \"should have 1 interactsh URL\")\n+\n+\t\t// The result should be the replaced interactsh URL\n+\t\trequire.NotContains(t, result[\"callback\"], \"{{interactsh-url}}\", \"interactsh should be replaced\")\n+\t\trequire.NotEmpty(t, result[\"callback\"], \"callback should have a value\")\n+\t})\n+}\n"}
{"org": "seaweedfs", "repo": "seaweedfs", "number": 7627, "state": "closed", "title": "mount: improve read throughput with parallel chunk fetching", "body": "## Summary\n\nFixes #7504\n\nThis PR addresses the issue where a single weed mount FUSE instance does not fully utilize node network bandwidth when reading large files (~40% utilization vs 100% with multiple instances).\n\n## Root Causes Identified\n\n1. **Lock contention**: `SingleChunkCacher` held a mutex during the entire HTTP download\n2. **Sequential chunk processing**: `doReadAt()` processed chunks one at a time in a loop\n3. **Buffer copying overhead**: Multiple intermediate buffers with 64KB copies\n\n## Changes\n\n### Commit 1: filer: remove lock contention during chunk download\n- Use `sync.Cond` for efficient waiting instead of blocking on mutex during I/O\n- Move HTTP download outside the critical section\n- Readers wait on condition variable until download completes\n\n### Commit 2: filer: parallel chunk fetching within doReadAt\n- Collect all chunk read tasks upfront\n- Use `errgroup` to fetch multiple chunks in parallel\n- Each chunk writes directly to its correct buffer position\n- Limit concurrency to `prefetchCount` (min 4)\n\n### Commit 3: http: direct buffer read to reduce memory copies\n- Add `readUrlDirectToBuffer()` that reads HTTP response directly into destination\n- Eliminate 64KB intermediate buffer for unencrypted chunks\n- Reduces ~256 copy operations per 16MB chunk\n\n## Testing\n\nAll existing tests pass. Ready for benchmark testing by @kisow to verify throughput improvements.\n\n## Related\n\n- Issue: #7504\n- Previous attempt: #7569 (did not fully address the issue per @kisow's testing)\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n## Summary by CodeRabbit\n\n* **New Features**\n  * Parallel multi-chunk reads with bounded concurrency, per-chunk tracking, and explicit zero-filling for gaps.\n\n* **Performance Improvements**\n  * Fast-path direct-buffer HTTP reads for full unencrypted chunks to reduce copies and improve throughput.\n  * Improved sequential read prefetching and end-of-file handling.\n\n* **Reliability**\n  * Background chunk caching now signals completion and supports cancellation, with safer buffer lifecycle and context-aware read cancellation.\n\n* **Tests**\n  * Added unit tests covering cache behavior, concurrent readers, cancellation, partial reads, and cleanup.\n\n<sub>âœï¸ Tip: You can customize this high-level summary in your review settings.</sub>\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->", "url": "https://api.github.com/repos/seaweedfs/seaweedfs/pulls/7627", "id": 3073797297, "node_id": "PR_kwDOAU0OSs63Nmyx", "html_url": "https://github.com/seaweedfs/seaweedfs/pull/7627", "diff_url": "https://github.com/seaweedfs/seaweedfs/pull/7627.diff", "patch_url": "https://github.com/seaweedfs/seaweedfs/pull/7627.patch", "issue_url": "https://api.github.com/repos/seaweedfs/seaweedfs/issues/7627", "created_at": "2025-12-05T03:39:21+00:00", "updated_at": "2025-12-05T07:40:58+00:00", "closed_at": "2025-12-05T07:40:56+00:00", "merged_at": "2025-12-05T07:40:56+00:00", "merge_commit_sha": "5c1de633cb10fe87450c9a38e090c4d69b1242da", "labels": [], "draft": false, "commits_url": "https://api.github.com/repos/seaweedfs/seaweedfs/pulls/7627/commits", "review_comments_url": "https://api.github.com/repos/seaweedfs/seaweedfs/pulls/7627/comments", "review_comment_url": "https://api.github.com/repos/seaweedfs/seaweedfs/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/seaweedfs/seaweedfs/issues/7627/comments", "base": {"label": "seaweedfs:master", "ref": "master", "sha": "3183a49698d77659cd15434ccd58c3002bc8c266", "user": {"login": "seaweedfs", "id": 11985425, "node_id": "MDEyOk9yZ2FuaXphdGlvbjExOTg1NDI1", "avatar_url": "https://avatars.githubusercontent.com/u/11985425?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seaweedfs", "html_url": "https://github.com/seaweedfs", "followers_url": "https://api.github.com/users/seaweedfs/followers", "following_url": "https://api.github.com/users/seaweedfs/following{/other_user}", "gists_url": "https://api.github.com/users/seaweedfs/gists{/gist_id}", "starred_url": "https://api.github.com/users/seaweedfs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seaweedfs/subscriptions", "organizations_url": "https://api.github.com/users/seaweedfs/orgs", "repos_url": "https://api.github.com/users/seaweedfs/repos", "events_url": "https://api.github.com/users/seaweedfs/events{/privacy}", "received_events_url": "https://api.github.com/users/seaweedfs/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 21827146, "node_id": "MDEwOlJlcG9zaXRvcnkyMTgyNzE0Ng==", "name": "seaweedfs", "full_name": "seaweedfs/seaweedfs", "private": false, "owner": {"login": "seaweedfs", "id": 11985425, "node_id": "MDEyOk9yZ2FuaXphdGlvbjExOTg1NDI1", "avatar_url": "https://avatars.githubusercontent.com/u/11985425?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seaweedfs", "html_url": "https://github.com/seaweedfs", "followers_url": "https://api.github.com/users/seaweedfs/followers", "following_url": "https://api.github.com/users/seaweedfs/following{/other_user}", "gists_url": "https://api.github.com/users/seaweedfs/gists{/gist_id}", "starred_url": "https://api.github.com/users/seaweedfs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seaweedfs/subscriptions", "organizations_url": "https://api.github.com/users/seaweedfs/orgs", "repos_url": "https://api.github.com/users/seaweedfs/repos", "events_url": "https://api.github.com/users/seaweedfs/events{/privacy}", "received_events_url": "https://api.github.com/users/seaweedfs/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/seaweedfs/seaweedfs", "description": "SeaweedFS is a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, xDC replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding. Enterprise version is at seaweedfs.com.", "fork": false, "url": "https://api.github.com/repos/seaweedfs/seaweedfs", "forks_url": "https://api.github.com/repos/seaweedfs/seaweedfs/forks", "keys_url": "https://api.github.com/repos/seaweedfs/seaweedfs/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/seaweedfs/seaweedfs/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/seaweedfs/seaweedfs/teams", "hooks_url": "https://api.github.com/repos/seaweedfs/seaweedfs/hooks", "issue_events_url": "https://api.github.com/repos/seaweedfs/seaweedfs/issues/events{/number}", "events_url": "https://api.github.com/repos/seaweedfs/seaweedfs/events", "assignees_url": "https://api.github.com/repos/seaweedfs/seaweedfs/assignees{/user}", "branches_url": "https://api.github.com/repos/seaweedfs/seaweedfs/branches{/branch}", "tags_url": "https://api.github.com/repos/seaweedfs/seaweedfs/tags", "blobs_url": "https://api.github.com/repos/seaweedfs/seaweedfs/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/seaweedfs/seaweedfs/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/seaweedfs/seaweedfs/git/refs{/sha}", "trees_url": "https://api.github.com/repos/seaweedfs/seaweedfs/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/seaweedfs/seaweedfs/statuses/{sha}", "languages_url": "https://api.github.com/repos/seaweedfs/seaweedfs/languages", "stargazers_url": "https://api.github.com/repos/seaweedfs/seaweedfs/stargazers", "contributors_url": "https://api.github.com/repos/seaweedfs/seaweedfs/contributors", "subscribers_url": "https://api.github.com/repos/seaweedfs/seaweedfs/subscribers", "subscription_url": "https://api.github.com/repos/seaweedfs/seaweedfs/subscription", "commits_url": "https://api.github.com/repos/seaweedfs/seaweedfs/commits{/sha}", "git_commits_url": "https://api.github.com/repos/seaweedfs/seaweedfs/git/commits{/sha}", "comments_url": "https://api.github.com/repos/seaweedfs/seaweedfs/comments{/number}", "issue_comment_url": "https://api.github.com/repos/seaweedfs/seaweedfs/issues/comments{/number}", "contents_url": "https://api.github.com/repos/seaweedfs/seaweedfs/contents/{+path}", "compare_url": "https://api.github.com/repos/seaweedfs/seaweedfs/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/seaweedfs/seaweedfs/merges", "archive_url": "https://api.github.com/repos/seaweedfs/seaweedfs/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/seaweedfs/seaweedfs/downloads", "issues_url": "https://api.github.com/repos/seaweedfs/seaweedfs/issues{/number}", "pulls_url": "https://api.github.com/repos/seaweedfs/seaweedfs/pulls{/number}", "milestones_url": "https://api.github.com/repos/seaweedfs/seaweedfs/milestones{/number}", "notifications_url": "https://api.github.com/repos/seaweedfs/seaweedfs/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/seaweedfs/seaweedfs/labels{/name}", "releases_url": "https://api.github.com/repos/seaweedfs/seaweedfs/releases{/id}", "deployments_url": "https://api.github.com/repos/seaweedfs/seaweedfs/deployments", "created_at": "2014-07-14T16:41:37Z", "updated_at": "2026-01-07T07:19:46Z", "pushed_at": "2026-01-07T05:53:30Z", "git_url": "git://github.com/seaweedfs/seaweedfs.git", "ssh_url": "git@github.com:seaweedfs/seaweedfs.git", "clone_url": "https://github.com/seaweedfs/seaweedfs.git", "svn_url": "https://github.com/seaweedfs/seaweedfs", "homepage": "https://seaweedfs.com", "size": 158853, "stargazers_count": 29375, "watchers_count": 29375, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": true, "has_discussions": true, "forks_count": 2655, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 668, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "topics": ["blob-storage", "cloud-drive", "distributed-file-system", "distributed-storage", "distributed-systems", "erasure-coding", "fuse", "hadoop-hdfs", "hdfs", "kubernetes", "object-storage", "posix", "replication", "s3", "s3-storage", "seaweedfs", "tiered-file-system"], "visibility": "public", "forks": 2655, "open_issues": 668, "watchers": 29375, "default_branch": "master"}}, "commits": [{"sha": "f7544086d7fbc72cc4adae2cb16dd3af128e8508", "parents": ["f9b4a4c396d42b749f29c07d3c1dec0d2a18aaed"], "message": "filer: remove lock contention during chunk download\n\nThis addresses issue #7504 where a single weed mount FUSE instance\ndoes not fully utilize node network bandwidth when reading large files.\n\nThe SingleChunkCacher was holding a mutex during the entire HTTP download,\ncausing readers to block until the download completed. This serialized\nchunk reads even when multiple goroutines were downloading in parallel.\n\nChanges:\n- Add sync.Cond to SingleChunkCacher for efficient waiting\n- Move HTTP download outside the critical section in startCaching()\n- Use condition variable in readChunkAt() to wait for download completion\n- Add isComplete flag to track download state\n\nNow multiple chunk downloads can proceed truly in parallel, and readers\nwait efficiently using the condition variable instead of blocking on\na mutex held during I/O operations.\n\nRef: #7504"}, {"sha": "cafcd0a0f5be464a0f7f8c13d2656d6ccadc8b42", "parents": ["f7544086d7fbc72cc4adae2cb16dd3af128e8508"], "message": "filer: parallel chunk fetching within doReadAt\n\nThis addresses issue #7504 by enabling parallel chunk downloads within\na single read operation.\n\nPreviously, doReadAt() processed chunks sequentially in a loop, meaning\neach chunk had to be fully downloaded before the next one started.\nThis left significant network bandwidth unused when chunks resided on\ndifferent volume servers.\n\nChanges:\n- Collect all chunk read tasks upfront\n- Use errgroup to fetch multiple chunks in parallel\n- Each chunk reads directly into its correct buffer position\n- Limit concurrency to prefetchCount (min 4) to avoid overwhelming the system\n- Handle gaps and zero-filling before parallel fetch\n- Trigger prefetch after parallel reads complete\n\nFor a read spanning N chunks on different volume servers, this can\nnow utilize up to N times the bandwidth of a single connection.\n\nRef: #7504"}, {"sha": "01ac4af05a34dfa9799a806d71ec0302777ef622", "parents": ["cafcd0a0f5be464a0f7f8c13d2656d6ccadc8b42"], "message": "http: direct buffer read to reduce memory copies\n\nThis addresses issue #7504 by reducing memory copy overhead during\nchunk downloads.\n\nPreviously, RetriedFetchChunkData used ReadUrlAsStream which:\n1. Allocated a 64KB intermediate buffer\n2. Read data in 64KB chunks\n3. Called a callback to copy each chunk to the destination\n\nFor a 16MB chunk, this meant 256 copy operations plus the callback\noverhead. Profiling showed significant time spent in memmove.\n\nChanges:\n- Add readUrlDirectToBuffer() that reads directly into the destination\n- Add retriedFetchChunkDataDirect() for unencrypted, non-gzipped chunks\n- Automatically use direct read path when possible (cipher=nil, gzip=false)\n- Use http.NewRequestWithContext for proper cancellation\n\nFor unencrypted chunks (the common case), this eliminates the\nintermediate buffer entirely, reading HTTP response bytes directly\ninto the final destination buffer.\n\nRef: #7504"}, {"sha": "bca9bd646c1bb2fed8cd1238d94fbe5babd44164", "parents": ["01ac4af05a34dfa9799a806d71ec0302777ef622"], "message": "address review comments\n\n- Use channel (done) instead of sync.Cond for download completion signaling\n  This integrates better with context cancellation patterns\n- Remove redundant groupErr check in reader_at.go (errors are already captured in task.err)\n- Remove buggy URL encoding logic from retriedFetchChunkDataDirect\n  (The existing url.PathEscape on full URL is a pre-existing bug that should be fixed separately)"}, {"sha": "f908a006fc47943f20f8454a552fdb0d5ce751ac", "parents": ["bca9bd646c1bb2fed8cd1238d94fbe5babd44164"], "message": "address review comments (round 2)\n\n- Return io.ErrUnexpectedEOF when HTTP response is truncated\n  This prevents silent data corruption from incomplete reads\n- Simplify errgroup error handling by using g.Wait() error directly\n  Remove redundant task.err field and manual error aggregation loop\n- Define minReadConcurrency constant instead of magic number 4\n  Improves code readability and maintainability\n\nNote: Context propagation to startCaching() is intentionally NOT changed.\nThe downloaded chunk is a shared resource that may be used by multiple\nreaders. Using context.Background() ensures the download completes even\nif one reader cancels, preventing data loss for other waiting readers."}, {"sha": "21b9ea531508d5775c26d54b51acd4fb94576952", "parents": ["f908a006fc47943f20f8454a552fdb0d5ce751ac"], "message": "http: inject request ID for observability in direct read path\n\nAdd request_id.InjectToRequest() call to readUrlDirectToBuffer() for\nconsistency with ReadUrlAsStream path. This ensures full-chunk reads\ncarry the same tracing/correlation headers for server logs and metrics."}, {"sha": "7d304659ae496c747fda078eaa3454246a87008b", "parents": ["21b9ea531508d5775c26d54b51acd4fb94576952"], "message": "filer: consistent timestamp handling in sequential read path\n\nUse max(ts, task.chunk.ModifiedTsNs) in sequential path to match\nparallel path behavior. Also update ts before error check so that\non failure, the returned timestamp reflects the max of all chunks\nprocessed so far."}, {"sha": "c0b6acb10fb2ebbcae6a63e4ad6879204168601c", "parents": ["7d304659ae496c747fda078eaa3454246a87008b"], "message": "filer: document why context.Background() is used in startCaching\n\nAdd comment explaining the intentional design decision: the downloaded\nchunk is a shared resource that may be used by multiple concurrent\nreaders. Using context.Background() ensures the download completes\neven if one reader cancels, preventing errors for other waiting readers."}, {"sha": "a8c5253ebee68e1b1e3793b43a180c339eb0ff63", "parents": ["c0b6acb10fb2ebbcae6a63e4ad6879204168601c"], "message": "filer: propagate context for reader cancellation\n\nAddress review comment: pass context through ReadChunkAt call chain so\nthat a reader can cancel its wait for a download. The key distinction is:\n\n- Download uses context.Background() - shared resource, always completes\n- Reader wait uses request context - can be cancelled individually\n\nIf a reader cancels, it stops waiting and returns ctx.Err(), but the\ndownload continues to completion for other readers waiting on the same\nchunk. This properly handles the shared resource semantics while still\nallowing individual reader cancellation."}, {"sha": "b2800947bd6a959590c077e5a69cfbb983b304b7", "parents": ["a8c5253ebee68e1b1e3793b43a180c339eb0ff63"], "message": "filer: use defer for close(done) to guarantee signal on panic\n\nMove close(s.done) to a defer statement at the start of startCaching()\nto ensure the completion signal is always sent, even if an unexpected\npanic occurs. This prevents readers from blocking indefinitely."}, {"sha": "2b7be3547b6325975788a4478e0ec0974334391c", "parents": ["b2800947bd6a959590c077e5a69cfbb983b304b7"], "message": "filer: remove unnecessary code\n\n- Remove close(s.cacheStartedCh) in destroy() - the channel is only used\n  for one-time synchronization, closing it provides no benefit\n- Remove task := task loop variable capture - Go 1.22+ fixed loop variable\n  semantics, this capture is no longer necessary (go.mod specifies Go 1.24.0)"}, {"sha": "404c51b8d55d95261a6782b0ca32ac72ca5464bf", "parents": ["2b7be3547b6325975788a4478e0ec0974334391c"], "message": "filer: restore fallback to chunkCache when cacher returns no data\n\nFix critical issue where ReadChunkAt would return 0,nil immediately\nif SingleChunkCacher couldn't provide data for the requested offset,\nwithout trying the chunkCache fallback. Now if cacher.readChunkAt\nreturns n=0 and err=nil, we fall through to try chunkCache."}, {"sha": "e529d5f58fdf17ac03910726abea703a8334c04f", "parents": ["404c51b8d55d95261a6782b0ca32ac72ca5464bf"], "message": "filer: add comprehensive tests for ReaderCache\n\nTests cover:\n- Context cancellation while waiting for download\n- Fallback to chunkCache when cacher returns n=0, err=nil\n- Multiple concurrent readers waiting for same chunk\n- Partial reads at different offsets\n- Downloader cleanup when exceeding cache limit\n- Done channel signaling (no hangs on completion)"}, {"sha": "c31ec80a930cb4bab833eb093e933864564fdf43", "parents": ["e529d5f58fdf17ac03910726abea703a8334c04f"], "message": "filer: prioritize done channel over context cancellation\n\nIf data is already available (done channel closed), return it even if\nthe reader's context is also cancelled. This avoids unnecessary errors\nwhen the download has already completed."}, {"sha": "350e834e1884173b3181b515857c6ada89339481", "parents": ["c31ec80a930cb4bab833eb093e933864564fdf43"], "message": "filer: add lookup error test and document test limitations\n\nAdd TestSingleChunkCacherLookupError to test error handling when lookup\nfails. Document that full HTTP integration tests for SingleChunkCacher\nrequire global HTTP client initialization which is complex in unit tests.\nThe download path is tested via FUSE integration tests."}, {"sha": "9681e81dec285a6dac272062461afc5c41163941", "parents": ["350e834e1884173b3181b515857c6ada89339481"], "message": "filer: add tests that exercise SingleChunkCacher concurrency logic\n\nAdd tests that use blocking lookupFileIdFn to exercise the actual\nSingleChunkCacher wait/cancellation logic:\n\n- TestSingleChunkCacherContextCancellationDuringLookup: tests reader\n  cancellation while lookup is blocked\n- TestSingleChunkCacherMultipleReadersWaitForDownload: tests multiple\n  readers waiting on the same download\n- TestSingleChunkCacherOneReaderCancelsOthersContinue: tests that when\n  one reader cancels, other readers continue waiting\n\nThese tests properly exercise the done channel wait/cancel logic without\nrequiring HTTP calls - the blocking lookup simulates a slow download."}], "resolved_issues": [{"org": "seaweedfs", "repo": "seaweedfs", "number": 7504, "state": "closed", "title": "Weed Mount FUSE Reads Do Not Fully Utilize Node Network Bandwidth", "body": "### **Environment:**\n\n* SeaweedFS Volume Server: **4.00**\n* SeaweedFS Cluster on **8 nodes (master=3, filer=3, volume=8)**\n* Weed Mount **(FUSE) on a single node**\n  * **-cacheCapacityMB=0**\n  * **-chunkSizeLimitMB=16**\n  * **-replication=002**\n  * **-volumeServerAccess (default: direct)**\n* All 9-nodes in the same IDC\n  * CPU: 48-core\n  * RAM: 128GB\n  * O/S: CentOS 7.9\n* Network: **Sufficient bandwidth, low latency**\n* Volume Server: **Network and Disk I/O negligible, no compression or encryption**\n* File size to read: **10GB** (all chunks are fully distributed across 8 volume server nodes)\n\n\n### **Description:**\n\nReading a large file through **a single Weed Mount FUSE instance does not fully utilize the nodeâ€™s available network bandwidth.** \n\nStarting a second Weed Mount instance on the same host with a different mount path to read the same 10GB file allows both mounts to achieve same throughput **without any individual performance degradation.** The combined network RX utilization roughly doubles.\n\nSince the volume server request durations show no difference and all system resources are mostly idle, it seems that the bottleneck is on the Weed Mount client side.\n\n### **Expected Behavior:**\n\n* A single mount instance should fully utilize the nodeâ€™s available network bandwidth when reading large files.\n\n### **Actual Behavior:**\n\n* **Single mount instance uses less than ~40% of the available network bandwidth.**\n* **Two independent mount instances on the same node achieve full throughput for both, doubling RX utilization.**\n"}], "fix_patch": "diff --git a/weed/filer/reader_at.go b/weed/filer/reader_at.go\nindex 93fa76a2e8e..5e8fd61545a 100644\n--- a/weed/filer/reader_at.go\n+++ b/weed/filer/reader_at.go\n@@ -7,6 +7,8 @@ import (\n \t\"math/rand\"\n \t\"sync\"\n \n+\t\"golang.org/x/sync/errgroup\"\n+\n \t\"github.com/seaweedfs/seaweedfs/weed/glog\"\n \t\"github.com/seaweedfs/seaweedfs/weed/pb/filer_pb\"\n \t\"github.com/seaweedfs/seaweedfs/weed/util\"\n@@ -19,6 +21,11 @@ import (\n // the prefetch count is derived from the -concurrentReaders option.\n const DefaultPrefetchCount = 4\n \n+// minReadConcurrency is the minimum number of parallel chunk fetches.\n+// This ensures at least some parallelism even when prefetchCount is low,\n+// improving throughput for reads spanning multiple chunks.\n+const minReadConcurrency = 4\n+\n type ChunkReadAt struct {\n \tmasterClient  *wdclient.MasterClient\n \tchunkViews    *IntervalList[*ChunkView]\n@@ -175,67 +182,139 @@ func (c *ChunkReadAt) ReadAtWithTime(ctx context.Context, p []byte, offset int64\n \treturn c.doReadAt(ctx, p, offset)\n }\n \n+// chunkReadTask represents a single chunk read operation for parallel processing\n+type chunkReadTask struct {\n+\tchunk        *ChunkView\n+\tbufferStart  int64  // start position in the output buffer\n+\tbufferEnd    int64  // end position in the output buffer\n+\tchunkOffset  uint64 // offset within the chunk to read from\n+\tbytesRead    int\n+\tmodifiedTsNs int64\n+}\n+\n func (c *ChunkReadAt) doReadAt(ctx context.Context, p []byte, offset int64) (n int, ts int64, err error) {\n \n+\t// Collect all chunk read tasks\n+\tvar tasks []*chunkReadTask\n+\tvar gaps []struct{ start, length int64 } // gaps that need zero-filling\n+\n \tstartOffset, remaining := offset, int64(len(p))\n-\tvar nextChunks *Interval[*ChunkView]\n+\tvar lastChunk *Interval[*ChunkView]\n+\n \tfor x := c.chunkViews.Front(); x != nil; x = x.Next {\n \t\tchunk := x.Value\n \t\tif remaining <= 0 {\n \t\t\tbreak\n \t\t}\n-\t\tif x.Next != nil {\n-\t\t\tnextChunks = x.Next\n-\t\t}\n+\t\tlastChunk = x\n+\n+\t\t// Handle gap before this chunk\n \t\tif startOffset < chunk.ViewOffset {\n \t\t\tgap := chunk.ViewOffset - startOffset\n-\t\t\tglog.V(4).Infof(\"zero [%d,%d)\", startOffset, chunk.ViewOffset)\n-\t\t\tn += zero(p, startOffset-offset, gap)\n+\t\t\tgaps = append(gaps, struct{ start, length int64 }{startOffset - offset, gap})\n \t\t\tstartOffset, remaining = chunk.ViewOffset, remaining-gap\n \t\t\tif remaining <= 0 {\n \t\t\t\tbreak\n \t\t\t}\n \t\t}\n-\t\t// fmt.Printf(\">>> doReadAt [%d,%d), chunk[%d,%d)\\n\", offset, offset+int64(len(p)), chunk.ViewOffset, chunk.ViewOffset+int64(chunk.ViewSize))\n+\n \t\tchunkStart, chunkStop := max(chunk.ViewOffset, startOffset), min(chunk.ViewOffset+int64(chunk.ViewSize), startOffset+remaining)\n \t\tif chunkStart >= chunkStop {\n \t\t\tcontinue\n \t\t}\n-\t\t// glog.V(4).Infof(\"read [%d,%d), %d/%d chunk %s [%d,%d)\", chunkStart, chunkStop, i, len(c.chunkViews), chunk.FileId, chunk.ViewOffset-chunk.Offset, chunk.ViewOffset-chunk.Offset+int64(chunk.ViewSize))\n+\n \t\tbufferOffset := chunkStart - chunk.ViewOffset + chunk.OffsetInChunk\n-\t\tts = chunk.ModifiedTsNs\n-\t\tcopied, err := c.readChunkSliceAt(ctx, p[startOffset-offset:chunkStop-chunkStart+startOffset-offset], chunk, nextChunks, uint64(bufferOffset))\n-\t\tif err != nil {\n-\t\t\tglog.Errorf(\"fetching chunk %+v: %v\\n\", chunk, err)\n-\t\t\treturn copied, ts, err\n+\t\ttasks = append(tasks, &chunkReadTask{\n+\t\t\tchunk:       chunk,\n+\t\t\tbufferStart: startOffset - offset,\n+\t\t\tbufferEnd:   chunkStop - chunkStart + startOffset - offset,\n+\t\t\tchunkOffset: uint64(bufferOffset),\n+\t\t})\n+\n+\t\tstartOffset, remaining = chunkStop, remaining-(chunkStop-chunkStart)\n+\t}\n+\n+\t// Zero-fill gaps\n+\tfor _, gap := range gaps {\n+\t\tglog.V(4).Infof(\"zero [%d,%d)\", offset+gap.start, offset+gap.start+gap.length)\n+\t\tn += zero(p, gap.start, gap.length)\n+\t}\n+\n+\t// If only one chunk or random access mode, use sequential reading\n+\tif len(tasks) <= 1 || c.readerPattern.IsRandomMode() {\n+\t\tfor _, task := range tasks {\n+\t\t\tcopied, readErr := c.readChunkSliceAt(ctx, p[task.bufferStart:task.bufferEnd], task.chunk, nil, task.chunkOffset)\n+\t\t\tts = max(ts, task.chunk.ModifiedTsNs)\n+\t\t\tif readErr != nil {\n+\t\t\t\tglog.Errorf(\"fetching chunk %+v: %v\\n\", task.chunk, readErr)\n+\t\t\t\treturn n + copied, ts, readErr\n+\t\t\t}\n+\t\t\tn += copied\n+\t\t}\n+\t} else {\n+\t\t// Parallel chunk fetching for multiple chunks\n+\t\t// This significantly improves throughput when chunks are on different volume servers\n+\t\tg, gCtx := errgroup.WithContext(ctx)\n+\n+\t\t// Limit concurrency to avoid overwhelming the system\n+\t\tconcurrency := c.prefetchCount\n+\t\tif concurrency < minReadConcurrency {\n+\t\t\tconcurrency = minReadConcurrency\n+\t\t}\n+\t\tif concurrency > len(tasks) {\n+\t\t\tconcurrency = len(tasks)\n+\t\t}\n+\t\tg.SetLimit(concurrency)\n+\n+\t\tfor _, task := range tasks {\n+\t\t\tg.Go(func() error {\n+\t\t\t\t// Read directly into the correct position in the output buffer\n+\t\t\t\tcopied, readErr := c.readChunkSliceAtForParallel(gCtx, p[task.bufferStart:task.bufferEnd], task.chunk, task.chunkOffset)\n+\t\t\t\ttask.bytesRead = copied\n+\t\t\t\ttask.modifiedTsNs = task.chunk.ModifiedTsNs\n+\t\t\t\treturn readErr\n+\t\t\t})\n \t\t}\n \n-\t\tn += copied\n-\t\tstartOffset, remaining = startOffset+int64(copied), remaining-int64(copied)\n+\t\t// Wait for all chunk reads to complete\n+\t\tif waitErr := g.Wait(); waitErr != nil {\n+\t\t\terr = waitErr\n+\t\t}\n+\n+\t\t// Aggregate results (order is preserved since we read directly into buffer positions)\n+\t\tfor _, task := range tasks {\n+\t\t\tn += task.bytesRead\n+\t\t\tts = max(ts, task.modifiedTsNs)\n+\t\t}\n+\n+\t\tif err != nil {\n+\t\t\treturn n, ts, err\n+\t\t}\n \t}\n \n-\t// glog.V(4).Infof(\"doReadAt [%d,%d), n:%v, err:%v\", offset, offset+int64(len(p)), n, err)\n+\t// Trigger prefetch for sequential reads\n+\tif lastChunk != nil && lastChunk.Next != nil && c.prefetchCount > 0 && !c.readerPattern.IsRandomMode() {\n+\t\tc.readerCache.MaybeCache(lastChunk.Next, c.prefetchCount)\n+\t}\n \n-\t// zero the remaining bytes if a gap exists at the end of the last chunk (or a fully sparse file)\n-\tif err == nil && remaining > 0 {\n+\t// Zero the remaining bytes if a gap exists at the end\n+\tif remaining > 0 {\n \t\tvar delta int64\n \t\tif c.fileSize >= startOffset {\n \t\t\tdelta = min(remaining, c.fileSize-startOffset)\n-\t\t\tstartOffset -= offset\n-\t\t}\n-\t\tif delta > 0 {\n-\t\t\tglog.V(4).Infof(\"zero2 [%d,%d) of file size %d bytes\", startOffset, startOffset+delta, c.fileSize)\n-\t\t\tn += zero(p, startOffset, delta)\n+\t\t\tbufStart := startOffset - offset\n+\t\t\tif delta > 0 {\n+\t\t\t\tglog.V(4).Infof(\"zero2 [%d,%d) of file size %d bytes\", startOffset, startOffset+delta, c.fileSize)\n+\t\t\t\tn += zero(p, bufStart, delta)\n+\t\t\t}\n \t\t}\n \t}\n \n \tif err == nil && offset+int64(len(p)) >= c.fileSize {\n \t\terr = io.EOF\n \t}\n-\t// fmt.Printf(\"~~~ filled %d, err: %v\\n\\n\", n, err)\n \n \treturn\n-\n }\n \n func (c *ChunkReadAt) readChunkSliceAt(ctx context.Context, buffer []byte, chunkView *ChunkView, nextChunkViews *Interval[*ChunkView], offset uint64) (n int, err error) {\n@@ -249,7 +328,7 @@ func (c *ChunkReadAt) readChunkSliceAt(ctx context.Context, buffer []byte, chunk\n \t}\n \n \tshouldCache := (uint64(chunkView.ViewOffset) + chunkView.ChunkSize) <= c.readerCache.chunkCache.GetMaxFilePartSizeInCache()\n-\tn, err = c.readerCache.ReadChunkAt(buffer, chunkView.FileId, chunkView.CipherKey, chunkView.IsGzipped, int64(offset), int(chunkView.ChunkSize), shouldCache)\n+\tn, err = c.readerCache.ReadChunkAt(ctx, buffer, chunkView.FileId, chunkView.CipherKey, chunkView.IsGzipped, int64(offset), int(chunkView.ChunkSize), shouldCache)\n \tif c.lastChunkFid != chunkView.FileId {\n \t\tif chunkView.OffsetInChunk == 0 { // start of a new chunk\n \t\t\tif c.lastChunkFid != \"\" {\n@@ -266,6 +345,13 @@ func (c *ChunkReadAt) readChunkSliceAt(ctx context.Context, buffer []byte, chunk\n \treturn\n }\n \n+// readChunkSliceAtForParallel is a simplified version for parallel chunk fetching\n+// It doesn't update lastChunkFid or trigger prefetch (handled by the caller)\n+func (c *ChunkReadAt) readChunkSliceAtForParallel(ctx context.Context, buffer []byte, chunkView *ChunkView, offset uint64) (n int, err error) {\n+\tshouldCache := (uint64(chunkView.ViewOffset) + chunkView.ChunkSize) <= c.readerCache.chunkCache.GetMaxFilePartSizeInCache()\n+\treturn c.readerCache.ReadChunkAt(ctx, buffer, chunkView.FileId, chunkView.CipherKey, chunkView.IsGzipped, int64(offset), int(chunkView.ChunkSize), shouldCache)\n+}\n+\n func zero(buffer []byte, start, length int64) int {\n \tif length <= 0 {\n \t\treturn 0\ndiff --git a/weed/filer/reader_cache.go b/weed/filer/reader_cache.go\nindex 605be5e733c..66cbac1e3f3 100644\n--- a/weed/filer/reader_cache.go\n+++ b/weed/filer/reader_cache.go\n@@ -35,6 +35,7 @@ type SingleChunkCacher struct {\n \tshouldCache    bool\n \twg             sync.WaitGroup\n \tcacheStartedCh chan struct{}\n+\tdone           chan struct{} // signals when download is complete\n }\n \n func NewReaderCache(limit int, chunkCache chunk_cache.ChunkCache, lookupFileIdFn wdclient.LookupFileIdFunctionType) *ReaderCache {\n@@ -93,14 +94,18 @@ func (rc *ReaderCache) MaybeCache(chunkViews *Interval[*ChunkView], count int) {\n \treturn\n }\n \n-func (rc *ReaderCache) ReadChunkAt(buffer []byte, fileId string, cipherKey []byte, isGzipped bool, offset int64, chunkSize int, shouldCache bool) (int, error) {\n+func (rc *ReaderCache) ReadChunkAt(ctx context.Context, buffer []byte, fileId string, cipherKey []byte, isGzipped bool, offset int64, chunkSize int, shouldCache bool) (int, error) {\n \trc.Lock()\n \n \tif cacher, found := rc.downloaders[fileId]; found {\n-\t\tif n, err := cacher.readChunkAt(buffer, offset); n != 0 && err == nil {\n-\t\t\trc.Unlock()\n+\t\trc.Unlock()\n+\t\tn, err := cacher.readChunkAt(ctx, buffer, offset)\n+\t\tif n > 0 || err != nil {\n \t\t\treturn n, err\n \t\t}\n+\t\t// If n=0 and err=nil, the cacher couldn't provide data for this offset.\n+\t\t// Fall through to try chunkCache.\n+\t\trc.Lock()\n \t}\n \tif shouldCache || rc.lookupFileIdFn == nil {\n \t\tn, err := rc.chunkCache.ReadChunkAt(buffer, fileId, uint64(offset))\n@@ -134,7 +139,7 @@ func (rc *ReaderCache) ReadChunkAt(buffer []byte, fileId string, cipherKey []byt\n \trc.downloaders[fileId] = cacher\n \trc.Unlock()\n \n-\treturn cacher.readChunkAt(buffer, offset)\n+\treturn cacher.readChunkAt(ctx, buffer, offset)\n }\n \n func (rc *ReaderCache) UnCache(fileId string) {\n@@ -166,38 +171,53 @@ func newSingleChunkCacher(parent *ReaderCache, fileId string, cipherKey []byte,\n \t\tchunkSize:      chunkSize,\n \t\tshouldCache:    shouldCache,\n \t\tcacheStartedCh: make(chan struct{}),\n+\t\tdone:           make(chan struct{}),\n \t}\n }\n \n+// startCaching downloads the chunk data in the background.\n+// It does NOT hold the lock during the HTTP download to allow concurrent readers\n+// to wait efficiently using the done channel.\n func (s *SingleChunkCacher) startCaching() {\n \ts.wg.Add(1)\n \tdefer s.wg.Done()\n-\ts.Lock()\n-\tdefer s.Unlock()\n+\tdefer close(s.done) // guarantee completion signal even on panic\n \n-\ts.cacheStartedCh <- struct{}{} // means this has been started\n+\ts.cacheStartedCh <- struct{}{} // signal that we've started\n \n+\t// Note: We intentionally use context.Background() here, NOT a request-specific context.\n+\t// The downloaded chunk is a shared resource - multiple concurrent readers may be waiting\n+\t// for this same download to complete. If we used a request context and that request was\n+\t// cancelled, it would abort the download and cause errors for all other waiting readers.\n+\t// The download should always complete once started to serve all potential consumers.\n+\n+\t// Lookup file ID without holding the lock\n \turlStrings, err := s.parent.lookupFileIdFn(context.Background(), s.chunkFileId)\n \tif err != nil {\n+\t\ts.Lock()\n \t\ts.err = fmt.Errorf(\"operation LookupFileId %s failed, err: %v\", s.chunkFileId, err)\n+\t\ts.Unlock()\n \t\treturn\n \t}\n \n-\ts.data = mem.Allocate(s.chunkSize)\n-\n-\t_, s.err = util_http.RetriedFetchChunkData(context.Background(), s.data, urlStrings, s.cipherKey, s.isGzipped, true, 0, s.chunkFileId)\n-\tif s.err != nil {\n-\t\tmem.Free(s.data)\n-\t\ts.data = nil\n-\t\treturn\n-\t}\n+\t// Allocate buffer and download without holding the lock\n+\t// This allows multiple downloads to proceed in parallel\n+\tdata := mem.Allocate(s.chunkSize)\n+\t_, fetchErr := util_http.RetriedFetchChunkData(context.Background(), data, urlStrings, s.cipherKey, s.isGzipped, true, 0, s.chunkFileId)\n \n-\tif s.shouldCache {\n-\t\ts.parent.chunkCache.SetChunk(s.chunkFileId, s.data)\n+\t// Now acquire lock to update state\n+\ts.Lock()\n+\tif fetchErr != nil {\n+\t\tmem.Free(data)\n+\t\ts.err = fetchErr\n+\t} else {\n+\t\ts.data = data\n+\t\tif s.shouldCache {\n+\t\t\ts.parent.chunkCache.SetChunk(s.chunkFileId, s.data)\n+\t\t}\n+\t\tatomic.StoreInt64(&s.completedTimeNew, time.Now().UnixNano())\n \t}\n-\tatomic.StoreInt64(&s.completedTimeNew, time.Now().UnixNano())\n-\n-\treturn\n+\ts.Unlock()\n }\n \n func (s *SingleChunkCacher) destroy() {\n@@ -209,13 +229,34 @@ func (s *SingleChunkCacher) destroy() {\n \tif s.data != nil {\n \t\tmem.Free(s.data)\n \t\ts.data = nil\n-\t\tclose(s.cacheStartedCh)\n \t}\n }\n \n-func (s *SingleChunkCacher) readChunkAt(buf []byte, offset int64) (int, error) {\n+// readChunkAt reads data from the cached chunk.\n+// It waits for the download to complete if it's still in progress.\n+// The ctx parameter allows the reader to cancel its wait (but the download continues\n+// for other readers - see comment in startCaching about shared resource semantics).\n+func (s *SingleChunkCacher) readChunkAt(ctx context.Context, buf []byte, offset int64) (int, error) {\n \ts.wg.Add(1)\n \tdefer s.wg.Done()\n+\n+\t// Wait for download to complete, but allow reader cancellation.\n+\t// Prioritize checking done first - if data is already available,\n+\t// return it even if context is also cancelled.\n+\tselect {\n+\tcase <-s.done:\n+\t\t// Download already completed, proceed immediately\n+\tdefault:\n+\t\t// Download not complete, wait for it or context cancellation\n+\t\tselect {\n+\t\tcase <-s.done:\n+\t\t\t// Download completed\n+\t\tcase <-ctx.Done():\n+\t\t\t// Reader cancelled while waiting - download continues for other readers\n+\t\t\treturn 0, ctx.Err()\n+\t\t}\n+\t}\n+\n \ts.Lock()\n \tdefer s.Unlock()\n \n@@ -228,5 +269,4 @@ func (s *SingleChunkCacher) readChunkAt(buf []byte, offset int64) (int, error) {\n \t}\n \n \treturn copy(buf, s.data[offset:]), nil\n-\n }\ndiff --git a/weed/util/http/http_global_client_util.go b/weed/util/http/http_global_client_util.go\nindex 3a969fdc864..a374c8a2b60 100644\n--- a/weed/util/http/http_global_client_util.go\n+++ b/weed/util/http/http_global_client_util.go\n@@ -487,6 +487,12 @@ func RetriedFetchChunkData(ctx context.Context, buffer []byte, urlStrings []stri\n \t\t)\n \t}\n \n+\t// For unencrypted, non-gzipped full chunks, use direct buffer read\n+\t// This avoids the 64KB intermediate buffer and callback overhead\n+\tif cipherKey == nil && !isGzipped && isFullChunk {\n+\t\treturn retriedFetchChunkDataDirect(ctx, buffer, urlStrings, string(jwt))\n+\t}\n+\n \tvar shouldRetry bool\n \n \tfor waitTime := time.Second; waitTime < util.RetryWaitTime; waitTime += waitTime / 2 {\n@@ -551,3 +557,105 @@ func RetriedFetchChunkData(ctx context.Context, buffer []byte, urlStrings []stri\n \treturn n, err\n \n }\n+\n+// retriedFetchChunkDataDirect reads chunk data directly into the buffer without\n+// intermediate buffering. This reduces memory copies and improves throughput\n+// for large chunk reads.\n+func retriedFetchChunkDataDirect(ctx context.Context, buffer []byte, urlStrings []string, jwt string) (n int, err error) {\n+\tvar shouldRetry bool\n+\n+\tfor waitTime := time.Second; waitTime < util.RetryWaitTime; waitTime += waitTime / 2 {\n+\t\tselect {\n+\t\tcase <-ctx.Done():\n+\t\t\treturn 0, ctx.Err()\n+\t\tdefault:\n+\t\t}\n+\n+\t\tfor _, urlString := range urlStrings {\n+\t\t\tselect {\n+\t\t\tcase <-ctx.Done():\n+\t\t\t\treturn 0, ctx.Err()\n+\t\t\tdefault:\n+\t\t\t}\n+\n+\t\t\tn, shouldRetry, err = readUrlDirectToBuffer(ctx, urlString+\"?readDeleted=true\", jwt, buffer)\n+\t\t\tif err == nil {\n+\t\t\t\treturn n, nil\n+\t\t\t}\n+\t\t\tif !shouldRetry {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tglog.V(0).InfofCtx(ctx, \"read %s failed, err: %v\", urlString, err)\n+\t\t}\n+\n+\t\tif err != nil && shouldRetry {\n+\t\t\tglog.V(0).InfofCtx(ctx, \"retry reading in %v\", waitTime)\n+\t\t\ttimer := time.NewTimer(waitTime)\n+\t\t\tselect {\n+\t\t\tcase <-ctx.Done():\n+\t\t\t\ttimer.Stop()\n+\t\t\t\treturn 0, ctx.Err()\n+\t\t\tcase <-timer.C:\n+\t\t\t}\n+\t\t} else {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\n+\treturn n, err\n+}\n+\n+// readUrlDirectToBuffer reads HTTP response directly into the provided buffer,\n+// avoiding intermediate buffer allocations and copies.\n+func readUrlDirectToBuffer(ctx context.Context, fileUrl, jwt string, buffer []byte) (n int, retryable bool, err error) {\n+\treq, err := http.NewRequestWithContext(ctx, http.MethodGet, fileUrl, nil)\n+\tif err != nil {\n+\t\treturn 0, false, err\n+\t}\n+\tmaybeAddAuth(req, jwt)\n+\trequest_id.InjectToRequest(ctx, req)\n+\n+\tr, err := GetGlobalHttpClient().Do(req)\n+\tif err != nil {\n+\t\treturn 0, true, err\n+\t}\n+\tdefer CloseResponse(r)\n+\n+\tif r.StatusCode >= 400 {\n+\t\tif r.StatusCode == http.StatusNotFound {\n+\t\t\treturn 0, true, fmt.Errorf(\"%s: %s: %w\", fileUrl, r.Status, ErrNotFound)\n+\t\t}\n+\t\tif r.StatusCode == http.StatusTooManyRequests {\n+\t\t\treturn 0, false, fmt.Errorf(\"%s: %s: %w\", fileUrl, r.Status, ErrTooManyRequests)\n+\t\t}\n+\t\tretryable = r.StatusCode >= 499\n+\t\treturn 0, retryable, fmt.Errorf(\"%s: %s\", fileUrl, r.Status)\n+\t}\n+\n+\t// Read directly into the buffer without intermediate copying\n+\t// This is significantly faster for large chunks (16MB+)\n+\tvar totalRead int\n+\tfor totalRead < len(buffer) {\n+\t\tselect {\n+\t\tcase <-ctx.Done():\n+\t\t\treturn totalRead, false, ctx.Err()\n+\t\tdefault:\n+\t\t}\n+\n+\t\tm, readErr := r.Body.Read(buffer[totalRead:])\n+\t\ttotalRead += m\n+\t\tif readErr != nil {\n+\t\t\tif readErr == io.EOF {\n+\t\t\t\t// Return io.ErrUnexpectedEOF if we haven't filled the buffer\n+\t\t\t\t// This prevents silent data corruption from truncated responses\n+\t\t\t\tif totalRead < len(buffer) {\n+\t\t\t\t\treturn totalRead, true, io.ErrUnexpectedEOF\n+\t\t\t\t}\n+\t\t\t\treturn totalRead, false, nil\n+\t\t\t}\n+\t\t\treturn totalRead, true, readErr\n+\t\t}\n+\t}\n+\n+\treturn totalRead, false, nil\n+}\n", "test_patch": "diff --git a/weed/filer/reader_cache_test.go b/weed/filer/reader_cache_test.go\nnew file mode 100644\nindex 00000000000..0480de8a74f\n--- /dev/null\n+++ b/weed/filer/reader_cache_test.go\n@@ -0,0 +1,505 @@\n+package filer\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"testing\"\n+\t\"time\"\n+)\n+\n+// mockChunkCacheForReaderCache implements chunk cache for testing\n+type mockChunkCacheForReaderCache struct {\n+\tdata     map[string][]byte\n+\thitCount int32\n+\tmu       sync.Mutex\n+}\n+\n+func newMockChunkCacheForReaderCache() *mockChunkCacheForReaderCache {\n+\treturn &mockChunkCacheForReaderCache{\n+\t\tdata: make(map[string][]byte),\n+\t}\n+}\n+\n+func (m *mockChunkCacheForReaderCache) GetChunk(fileId string, minSize uint64) []byte {\n+\tm.mu.Lock()\n+\tdefer m.mu.Unlock()\n+\tif d, ok := m.data[fileId]; ok {\n+\t\tatomic.AddInt32(&m.hitCount, 1)\n+\t\treturn d\n+\t}\n+\treturn nil\n+}\n+\n+func (m *mockChunkCacheForReaderCache) ReadChunkAt(data []byte, fileId string, offset uint64) (int, error) {\n+\tm.mu.Lock()\n+\tdefer m.mu.Unlock()\n+\tif d, ok := m.data[fileId]; ok && int(offset) < len(d) {\n+\t\tatomic.AddInt32(&m.hitCount, 1)\n+\t\tn := copy(data, d[offset:])\n+\t\treturn n, nil\n+\t}\n+\treturn 0, nil\n+}\n+\n+func (m *mockChunkCacheForReaderCache) SetChunk(fileId string, data []byte) {\n+\tm.mu.Lock()\n+\tdefer m.mu.Unlock()\n+\tm.data[fileId] = data\n+}\n+\n+func (m *mockChunkCacheForReaderCache) GetMaxFilePartSizeInCache() uint64 {\n+\treturn 1024 * 1024 // 1MB\n+}\n+\n+func (m *mockChunkCacheForReaderCache) IsInCache(fileId string, lockNeeded bool) bool {\n+\tm.mu.Lock()\n+\tdefer m.mu.Unlock()\n+\t_, ok := m.data[fileId]\n+\treturn ok\n+}\n+\n+// TestReaderCacheContextCancellation tests that a reader can cancel its wait\n+// while the download continues for other readers\n+func TestReaderCacheContextCancellation(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\n+\t// Create a ReaderCache - we can't easily test the full flow without mocking HTTP,\n+\t// but we can test the context cancellation in readChunkAt\n+\trc := NewReaderCache(10, cache, nil)\n+\tdefer rc.destroy()\n+\n+\t// Pre-populate cache to avoid HTTP calls\n+\ttestData := []byte(\"test data for context cancellation\")\n+\tcache.SetChunk(\"test-file-1\", testData)\n+\n+\t// Test that context cancellation works\n+\tctx, cancel := context.WithCancel(context.Background())\n+\n+\tbuffer := make([]byte, len(testData))\n+\tn, err := rc.ReadChunkAt(ctx, buffer, \"test-file-1\", nil, false, 0, len(testData), true)\n+\tif err != nil {\n+\t\tt.Errorf(\"Expected no error, got: %v\", err)\n+\t}\n+\tif n != len(testData) {\n+\t\tt.Errorf(\"Expected %d bytes, got %d\", len(testData), n)\n+\t}\n+\n+\t// Cancel context and verify it doesn't affect already completed reads\n+\tcancel()\n+\n+\t// Subsequent read with cancelled context should still work from cache\n+\tbuffer2 := make([]byte, len(testData))\n+\tn2, err2 := rc.ReadChunkAt(ctx, buffer2, \"test-file-1\", nil, false, 0, len(testData), true)\n+\t// Note: This may or may not error depending on whether it hits cache\n+\t_ = n2\n+\t_ = err2\n+}\n+\n+// TestReaderCacheFallbackToChunkCache tests that when a cacher returns n=0, err=nil,\n+// we fall back to the chunkCache\n+func TestReaderCacheFallbackToChunkCache(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\n+\t// Pre-populate the chunk cache with data\n+\ttestData := []byte(\"fallback test data that should be found in chunk cache\")\n+\tcache.SetChunk(\"fallback-file\", testData)\n+\n+\trc := NewReaderCache(10, cache, nil)\n+\tdefer rc.destroy()\n+\n+\t// Read should hit the chunk cache\n+\tbuffer := make([]byte, len(testData))\n+\tn, err := rc.ReadChunkAt(context.Background(), buffer, \"fallback-file\", nil, false, 0, len(testData), true)\n+\n+\tif err != nil {\n+\t\tt.Errorf(\"Expected no error, got: %v\", err)\n+\t}\n+\tif n != len(testData) {\n+\t\tt.Errorf(\"Expected %d bytes, got %d\", len(testData), n)\n+\t}\n+\n+\t// Verify cache was hit\n+\tif cache.hitCount == 0 {\n+\t\tt.Error(\"Expected chunk cache to be hit\")\n+\t}\n+}\n+\n+// TestReaderCacheMultipleReadersWaitForSameChunk tests that multiple readers\n+// can wait for the same chunk download to complete\n+func TestReaderCacheMultipleReadersWaitForSameChunk(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\n+\t// Pre-populate cache so we don't need HTTP\n+\ttestData := make([]byte, 1024)\n+\tfor i := range testData {\n+\t\ttestData[i] = byte(i % 256)\n+\t}\n+\tcache.SetChunk(\"shared-chunk\", testData)\n+\n+\trc := NewReaderCache(10, cache, nil)\n+\tdefer rc.destroy()\n+\n+\t// Launch multiple concurrent readers for the same chunk\n+\tnumReaders := 10\n+\tvar wg sync.WaitGroup\n+\terrors := make(chan error, numReaders)\n+\tbytesRead := make(chan int, numReaders)\n+\n+\tfor i := 0; i < numReaders; i++ {\n+\t\twg.Add(1)\n+\t\tgo func() {\n+\t\t\tdefer wg.Done()\n+\t\t\tbuffer := make([]byte, len(testData))\n+\t\t\tn, err := rc.ReadChunkAt(context.Background(), buffer, \"shared-chunk\", nil, false, 0, len(testData), true)\n+\t\t\tif err != nil {\n+\t\t\t\terrors <- err\n+\t\t\t}\n+\t\t\tbytesRead <- n\n+\t\t}()\n+\t}\n+\n+\twg.Wait()\n+\tclose(errors)\n+\tclose(bytesRead)\n+\n+\t// Check for errors\n+\tfor err := range errors {\n+\t\tt.Errorf(\"Reader got error: %v\", err)\n+\t}\n+\n+\t// Verify all readers got the expected data\n+\tfor n := range bytesRead {\n+\t\tif n != len(testData) {\n+\t\t\tt.Errorf(\"Expected %d bytes, got %d\", len(testData), n)\n+\t\t}\n+\t}\n+}\n+\n+// TestReaderCachePartialRead tests reading at different offsets\n+func TestReaderCachePartialRead(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\n+\ttestData := []byte(\"0123456789ABCDEFGHIJ\")\n+\tcache.SetChunk(\"partial-read-file\", testData)\n+\n+\trc := NewReaderCache(10, cache, nil)\n+\tdefer rc.destroy()\n+\n+\ttests := []struct {\n+\t\tname     string\n+\t\toffset   int64\n+\t\tsize     int\n+\t\texpected []byte\n+\t}{\n+\t\t{\"read from start\", 0, 5, []byte(\"01234\")},\n+\t\t{\"read from middle\", 5, 5, []byte(\"56789\")},\n+\t\t{\"read to end\", 15, 5, []byte(\"FGHIJ\")},\n+\t\t{\"read single byte\", 10, 1, []byte(\"A\")},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\tbuffer := make([]byte, tt.size)\n+\t\t\tn, err := rc.ReadChunkAt(context.Background(), buffer, \"partial-read-file\", nil, false, tt.offset, len(testData), true)\n+\n+\t\t\tif err != nil {\n+\t\t\t\tt.Errorf(\"Expected no error, got: %v\", err)\n+\t\t\t}\n+\t\t\tif n != tt.size {\n+\t\t\t\tt.Errorf(\"Expected %d bytes, got %d\", tt.size, n)\n+\t\t\t}\n+\t\t\tif string(buffer[:n]) != string(tt.expected) {\n+\t\t\t\tt.Errorf(\"Expected %q, got %q\", tt.expected, buffer[:n])\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// TestReaderCacheCleanup tests that old downloaders are cleaned up\n+func TestReaderCacheCleanup(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\n+\t// Create cache with limit of 3\n+\trc := NewReaderCache(3, cache, nil)\n+\tdefer rc.destroy()\n+\n+\t// Add data for multiple files\n+\tfor i := 0; i < 5; i++ {\n+\t\tfileId := string(rune('A' + i))\n+\t\tdata := []byte(\"data for file \" + fileId)\n+\t\tcache.SetChunk(fileId, data)\n+\t}\n+\n+\t// Read from multiple files - should trigger cleanup when exceeding limit\n+\tfor i := 0; i < 5; i++ {\n+\t\tfileId := string(rune('A' + i))\n+\t\tbuffer := make([]byte, 20)\n+\t\t_, err := rc.ReadChunkAt(context.Background(), buffer, fileId, nil, false, 0, 20, true)\n+\t\tif err != nil {\n+\t\t\tt.Errorf(\"Read error for file %s: %v\", fileId, err)\n+\t\t}\n+\t}\n+\n+\t// Cache should still work - reads should succeed\n+\tfor i := 0; i < 5; i++ {\n+\t\tfileId := string(rune('A' + i))\n+\t\tbuffer := make([]byte, 20)\n+\t\tn, err := rc.ReadChunkAt(context.Background(), buffer, fileId, nil, false, 0, 20, true)\n+\t\tif err != nil {\n+\t\t\tt.Errorf(\"Second read error for file %s: %v\", fileId, err)\n+\t\t}\n+\t\tif n == 0 {\n+\t\t\tt.Errorf(\"Expected data for file %s, got 0 bytes\", fileId)\n+\t\t}\n+\t}\n+}\n+\n+// TestSingleChunkCacherDoneSignal tests that done channel is always closed\n+func TestSingleChunkCacherDoneSignal(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\trc := NewReaderCache(10, cache, nil)\n+\tdefer rc.destroy()\n+\n+\t// Test that we can read even when data is in cache (done channel should work)\n+\ttestData := []byte(\"done signal test\")\n+\tcache.SetChunk(\"done-signal-test\", testData)\n+\n+\t// Multiple goroutines reading same chunk\n+\tvar wg sync.WaitGroup\n+\tfor i := 0; i < 5; i++ {\n+\t\twg.Add(1)\n+\t\tgo func() {\n+\t\t\tdefer wg.Done()\n+\t\t\tbuffer := make([]byte, len(testData))\n+\t\t\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n+\t\t\tdefer cancel()\n+\n+\t\t\tn, err := rc.ReadChunkAt(ctx, buffer, \"done-signal-test\", nil, false, 0, len(testData), true)\n+\t\t\tif err != nil && err != context.DeadlineExceeded {\n+\t\t\t\tt.Errorf(\"Unexpected error: %v\", err)\n+\t\t\t}\n+\t\t\tif n == 0 && err == nil {\n+\t\t\t\tt.Error(\"Got 0 bytes with no error\")\n+\t\t\t}\n+\t\t}()\n+\t}\n+\n+\t// Should complete without hanging\n+\tdone := make(chan struct{})\n+\tgo func() {\n+\t\twg.Wait()\n+\t\tclose(done)\n+\t}()\n+\n+\tselect {\n+\tcase <-done:\n+\t\t// Success\n+\tcase <-time.After(10 * time.Second):\n+\t\tt.Fatal(\"Test timed out - done channel may not be signaled correctly\")\n+\t}\n+}\n+\n+// ============================================================================\n+// Tests that exercise SingleChunkCacher concurrency logic\n+// ============================================================================\n+//\n+// These tests use blocking lookupFileIdFn to exercise the wait/cancellation\n+// logic in SingleChunkCacher without requiring HTTP calls.\n+\n+// TestSingleChunkCacherLookupError tests handling of lookup errors\n+func TestSingleChunkCacherLookupError(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\t\n+\t// Lookup function that returns an error\n+\tlookupFn := func(ctx context.Context, fileId string) ([]string, error) {\n+\t\treturn nil, fmt.Errorf(\"lookup failed for %s\", fileId)\n+\t}\n+\n+\trc := NewReaderCache(10, cache, lookupFn)\n+\tdefer rc.destroy()\n+\n+\tbuffer := make([]byte, 100)\n+\t_, err := rc.ReadChunkAt(context.Background(), buffer, \"error-test\", nil, false, 0, 100, true)\n+\t\n+\tif err == nil {\n+\t\tt.Error(\"Expected an error, got nil\")\n+\t}\n+}\n+\n+// TestSingleChunkCacherContextCancellationDuringLookup tests that a reader can\n+// cancel its wait while the lookup is in progress. This exercises the actual\n+// SingleChunkCacher wait/cancel logic.\n+func TestSingleChunkCacherContextCancellationDuringLookup(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\tlookupStarted := make(chan struct{})\n+\tlookupCanFinish := make(chan struct{})\n+\n+\t// Lookup function that blocks to simulate slow operation\n+\tlookupFn := func(ctx context.Context, fileId string) ([]string, error) {\n+\t\tclose(lookupStarted)\n+\t\t<-lookupCanFinish // Block until test allows completion\n+\t\treturn nil, fmt.Errorf(\"lookup completed but reader should have cancelled\")\n+\t}\n+\n+\trc := NewReaderCache(10, cache, lookupFn)\n+\tdefer rc.destroy()\n+\tdefer close(lookupCanFinish) // Ensure cleanup\n+\n+\tctx, cancel := context.WithCancel(context.Background())\n+\treadResult := make(chan error, 1)\n+\n+\tgo func() {\n+\t\tbuffer := make([]byte, 100)\n+\t\t_, err := rc.ReadChunkAt(ctx, buffer, \"cancel-during-lookup\", nil, false, 0, 100, true)\n+\t\treadResult <- err\n+\t}()\n+\n+\t// Wait for lookup to start, then cancel the reader's context\n+\tselect {\n+\tcase <-lookupStarted:\n+\t\tcancel() // Cancel the reader while lookup is blocked\n+\tcase <-time.After(5 * time.Second):\n+\t\tt.Fatal(\"Lookup never started\")\n+\t}\n+\n+\t// Read should return with context.Canceled\n+\tselect {\n+\tcase err := <-readResult:\n+\t\tif err != context.Canceled {\n+\t\t\tt.Errorf(\"Expected context.Canceled, got: %v\", err)\n+\t\t}\n+\tcase <-time.After(5 * time.Second):\n+\t\tt.Fatal(\"Read did not complete after context cancellation\")\n+\t}\n+}\n+\n+// TestSingleChunkCacherMultipleReadersWaitForDownload tests that multiple readers\n+// can wait for the same SingleChunkCacher download to complete. When lookup fails,\n+// all readers should receive the same error.\n+func TestSingleChunkCacherMultipleReadersWaitForDownload(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\tlookupStarted := make(chan struct{})\n+\tlookupCanFinish := make(chan struct{})\n+\tvar lookupStartedOnce sync.Once\n+\n+\t// Lookup function that blocks to simulate slow operation\n+\tlookupFn := func(ctx context.Context, fileId string) ([]string, error) {\n+\t\tlookupStartedOnce.Do(func() { close(lookupStarted) })\n+\t\t<-lookupCanFinish\n+\t\treturn nil, fmt.Errorf(\"simulated lookup error\")\n+\t}\n+\n+\trc := NewReaderCache(10, cache, lookupFn)\n+\tdefer rc.destroy()\n+\n+\tnumReaders := 5\n+\tvar wg sync.WaitGroup\n+\terrors := make(chan error, numReaders)\n+\n+\t// Start multiple readers for the same chunk\n+\tfor i := 0; i < numReaders; i++ {\n+\t\twg.Add(1)\n+\t\tgo func() {\n+\t\t\tdefer wg.Done()\n+\t\t\tbuffer := make([]byte, 100)\n+\t\t\t_, err := rc.ReadChunkAt(context.Background(), buffer, \"shared-chunk\", nil, false, 0, 100, true)\n+\t\t\terrors <- err\n+\t\t}()\n+\t}\n+\n+\t// Wait for lookup to start, then allow completion\n+\tselect {\n+\tcase <-lookupStarted:\n+\t\tclose(lookupCanFinish)\n+\tcase <-time.After(5 * time.Second):\n+\t\tclose(lookupCanFinish)\n+\t\tt.Fatal(\"Lookup never started\")\n+\t}\n+\n+\twg.Wait()\n+\tclose(errors)\n+\n+\t// All readers should receive an error\n+\terrorCount := 0\n+\tfor err := range errors {\n+\t\tif err != nil {\n+\t\t\terrorCount++\n+\t\t}\n+\t}\n+\tif errorCount != numReaders {\n+\t\tt.Errorf(\"Expected %d errors, got %d\", numReaders, errorCount)\n+\t}\n+}\n+\n+// TestSingleChunkCacherOneReaderCancelsOthersContinue tests that when one reader\n+// cancels, other readers waiting on the same chunk continue to wait.\n+func TestSingleChunkCacherOneReaderCancelsOthersContinue(t *testing.T) {\n+\tcache := newMockChunkCacheForReaderCache()\n+\tlookupStarted := make(chan struct{})\n+\tlookupCanFinish := make(chan struct{})\n+\tvar lookupStartedOnce sync.Once\n+\n+\tlookupFn := func(ctx context.Context, fileId string) ([]string, error) {\n+\t\tlookupStartedOnce.Do(func() { close(lookupStarted) })\n+\t\t<-lookupCanFinish\n+\t\treturn nil, fmt.Errorf(\"simulated error after delay\")\n+\t}\n+\n+\trc := NewReaderCache(10, cache, lookupFn)\n+\tdefer rc.destroy()\n+\n+\tcancelledReaderDone := make(chan error, 1)\n+\totherReaderDone := make(chan error, 1)\n+\n+\tctx, cancel := context.WithCancel(context.Background())\n+\n+\t// Start reader that will be cancelled\n+\tgo func() {\n+\t\tbuffer := make([]byte, 100)\n+\t\t_, err := rc.ReadChunkAt(ctx, buffer, \"shared-chunk-2\", nil, false, 0, 100, true)\n+\t\tcancelledReaderDone <- err\n+\t}()\n+\n+\t// Start reader that will NOT be cancelled\n+\tgo func() {\n+\t\tbuffer := make([]byte, 100)\n+\t\t_, err := rc.ReadChunkAt(context.Background(), buffer, \"shared-chunk-2\", nil, false, 0, 100, true)\n+\t\totherReaderDone <- err\n+\t}()\n+\n+\t// Wait for lookup to start\n+\tselect {\n+\tcase <-lookupStarted:\n+\tcase <-time.After(5 * time.Second):\n+\t\tt.Fatal(\"Lookup never started\")\n+\t}\n+\n+\t// Cancel the first reader\n+\tcancel()\n+\n+\t// First reader should complete with context.Canceled quickly\n+\tselect {\n+\tcase err := <-cancelledReaderDone:\n+\t\tif err != context.Canceled {\n+\t\t\tt.Errorf(\"Cancelled reader: expected context.Canceled, got: %v\", err)\n+\t\t}\n+\tcase <-time.After(2 * time.Second):\n+\t\tt.Error(\"Cancelled reader did not complete quickly\")\n+\t}\n+\n+\t// Allow the download to complete\n+\tclose(lookupCanFinish)\n+\n+\t// Other reader should eventually complete (with error since lookup returns error)\n+\tselect {\n+\tcase err := <-otherReaderDone:\n+\t\tif err == nil || err == context.Canceled {\n+\t\t\tt.Errorf(\"Other reader: expected non-nil non-cancelled error, got: %v\", err)\n+\t\t}\n+\t\t// Expected: \"simulated error after delay\"\n+\tcase <-time.After(5 * time.Second):\n+\t\tt.Error(\"Other reader did not complete\")\n+\t}\n+}\n"}
{"org": "pingcap", "repo": "tidb", "number": 63160, "state": "closed", "title": "stats: optimize memory footprint of pseudo stats table", "body": "<!--\r\n\r\nThank you for contributing to TiDB!\r\n\r\nPR Title Format:\r\n1. pkg [, pkg2, pkg3]: what's changed\r\n2. *: what's changed\r\n\r\n-->\r\n\r\n### What problem does this PR solve?\r\n<!--\r\n\r\nPlease create an issue first to describe the problem.\r\n\r\nThere MUST be one line starting with \"Issue Number:  \" and\r\nlinking the relevant issues via the \"close\" or \"ref\".\r\n\r\nFor more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.\r\n\r\n-->\r\n\r\nIssue Number: close #60137\r\n\r\nProblem Summary:\r\n\r\n### What changed and how does it work?\r\n\r\nA significant part of memory consumption of pseudo table is creating new chunk for every pseudo table creation. These new chunks are read only and only to provide compatibility to the code of using real stats table struct. We can make it a static object and shared by all pseudo tables to reduce memory usage.\r\n### Check List\r\n\r\nTests <!-- At least one of them must be included. -->\r\n\r\n- [ ] Unit test\r\n- [x] Integration test\r\n- [ ] Manual test (add detailed scripts or steps below)\r\n- [ ] No need to test\r\n  > - [ ] I checked and no code files have been changed.\r\n  > <!-- Or your custom  \"No need to test\" reasons -->\r\n\r\nSide effects\r\n\r\n- [ ] Performance regression: Consumes more CPU\r\n- [ ] Performance regression: Consumes more Memory\r\n- [ ] Breaking backward compatibility\r\n\r\nDocumentation\r\n\r\n- [ ] Affects user behaviors\r\n- [ ] Contains syntax changes\r\n- [ ] Contains variable changes\r\n- [ ] Contains experimental features\r\n- [ ] Changes MySQL compatibility\r\n\r\n### Release note\r\n\r\n<!-- compatibility change, improvement, bugfix, and new feature need a release note -->\r\n\r\nPlease refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.\r\n\r\n```release-note\r\nNone\r\n```\r\n", "url": "https://api.github.com/repos/pingcap/tidb/pulls/63160", "id": 2767160400, "node_id": "PR_kwDOAoCpQc6k74RQ", "html_url": "https://github.com/pingcap/tidb/pull/63160", "diff_url": "https://github.com/pingcap/tidb/pull/63160.diff", "patch_url": "https://github.com/pingcap/tidb/pull/63160.patch", "issue_url": "https://api.github.com/repos/pingcap/tidb/issues/63160", "created_at": "2025-08-22T19:19:45+00:00", "updated_at": "2025-09-09T20:29:51+00:00", "closed_at": "2025-09-09T20:29:03+00:00", "merged_at": "2025-09-09T20:29:03+00:00", "merge_commit_sha": "6eade7b34f19994afdecbf1940cf201bafac0a63", "labels": ["sig/planner", "size/L", "release-note-none", "ok-to-test", "approved", "lgtm"], "draft": false, "commits_url": "https://api.github.com/repos/pingcap/tidb/pulls/63160/commits", "review_comments_url": "https://api.github.com/repos/pingcap/tidb/pulls/63160/comments", "review_comment_url": "https://api.github.com/repos/pingcap/tidb/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/pingcap/tidb/issues/63160/comments", "base": {"label": "pingcap:master", "ref": "master", "sha": "abb662f76cbd5059d9afd215ed637a5f5e2bd195", "user": {"login": "pingcap", "id": 11855343, "node_id": "MDEyOk9yZ2FuaXphdGlvbjExODU1MzQz", "avatar_url": "https://avatars.githubusercontent.com/u/11855343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pingcap", "html_url": "https://github.com/pingcap", "followers_url": "https://api.github.com/users/pingcap/followers", "following_url": "https://api.github.com/users/pingcap/following{/other_user}", "gists_url": "https://api.github.com/users/pingcap/gists{/gist_id}", "starred_url": "https://api.github.com/users/pingcap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pingcap/subscriptions", "organizations_url": "https://api.github.com/users/pingcap/orgs", "repos_url": "https://api.github.com/users/pingcap/repos", "events_url": "https://api.github.com/users/pingcap/events{/privacy}", "received_events_url": "https://api.github.com/users/pingcap/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 41986369, "node_id": "MDEwOlJlcG9zaXRvcnk0MTk4NjM2OQ==", "name": "tidb", "full_name": "pingcap/tidb", "private": false, "owner": {"login": "pingcap", "id": 11855343, "node_id": "MDEyOk9yZ2FuaXphdGlvbjExODU1MzQz", "avatar_url": "https://avatars.githubusercontent.com/u/11855343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pingcap", "html_url": "https://github.com/pingcap", "followers_url": "https://api.github.com/users/pingcap/followers", "following_url": "https://api.github.com/users/pingcap/following{/other_user}", "gists_url": "https://api.github.com/users/pingcap/gists{/gist_id}", "starred_url": "https://api.github.com/users/pingcap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pingcap/subscriptions", "organizations_url": "https://api.github.com/users/pingcap/orgs", "repos_url": "https://api.github.com/users/pingcap/repos", "events_url": "https://api.github.com/users/pingcap/events{/privacy}", "received_events_url": "https://api.github.com/users/pingcap/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/pingcap/tidb", "description": "TiDB - the open-source, cloud-native, distributed SQL database designed for modern applications.", "fork": false, "url": "https://api.github.com/repos/pingcap/tidb", "forks_url": "https://api.github.com/repos/pingcap/tidb/forks", "keys_url": "https://api.github.com/repos/pingcap/tidb/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/pingcap/tidb/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/pingcap/tidb/teams", "hooks_url": "https://api.github.com/repos/pingcap/tidb/hooks", "issue_events_url": "https://api.github.com/repos/pingcap/tidb/issues/events{/number}", "events_url": "https://api.github.com/repos/pingcap/tidb/events", "assignees_url": "https://api.github.com/repos/pingcap/tidb/assignees{/user}", "branches_url": "https://api.github.com/repos/pingcap/tidb/branches{/branch}", "tags_url": "https://api.github.com/repos/pingcap/tidb/tags", "blobs_url": "https://api.github.com/repos/pingcap/tidb/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/pingcap/tidb/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/pingcap/tidb/git/refs{/sha}", "trees_url": "https://api.github.com/repos/pingcap/tidb/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/pingcap/tidb/statuses/{sha}", "languages_url": "https://api.github.com/repos/pingcap/tidb/languages", "stargazers_url": "https://api.github.com/repos/pingcap/tidb/stargazers", "contributors_url": "https://api.github.com/repos/pingcap/tidb/contributors", "subscribers_url": "https://api.github.com/repos/pingcap/tidb/subscribers", "subscription_url": "https://api.github.com/repos/pingcap/tidb/subscription", "commits_url": "https://api.github.com/repos/pingcap/tidb/commits{/sha}", "git_commits_url": "https://api.github.com/repos/pingcap/tidb/git/commits{/sha}", "comments_url": "https://api.github.com/repos/pingcap/tidb/comments{/number}", "issue_comment_url": "https://api.github.com/repos/pingcap/tidb/issues/comments{/number}", "contents_url": "https://api.github.com/repos/pingcap/tidb/contents/{+path}", "compare_url": "https://api.github.com/repos/pingcap/tidb/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/pingcap/tidb/merges", "archive_url": "https://api.github.com/repos/pingcap/tidb/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/pingcap/tidb/downloads", "issues_url": "https://api.github.com/repos/pingcap/tidb/issues{/number}", "pulls_url": "https://api.github.com/repos/pingcap/tidb/pulls{/number}", "milestones_url": "https://api.github.com/repos/pingcap/tidb/milestones{/number}", "notifications_url": "https://api.github.com/repos/pingcap/tidb/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/pingcap/tidb/labels{/name}", "releases_url": "https://api.github.com/repos/pingcap/tidb/releases{/id}", "deployments_url": "https://api.github.com/repos/pingcap/tidb/deployments", "created_at": "2015-09-06T04:01:52Z", "updated_at": "2026-01-07T08:42:13Z", "pushed_at": "2026-01-07T08:42:08Z", "git_url": "git://github.com/pingcap/tidb.git", "ssh_url": "git@github.com:pingcap/tidb.git", "clone_url": "https://github.com/pingcap/tidb.git", "svn_url": "https://github.com/pingcap/tidb", "homepage": "https://pingcap.com", "size": 588595, "stargazers_count": 39565, "watchers_count": 39565, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": true, "forks_count": 6089, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 5461, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "topics": ["cloud-native", "database", "distributed-database", "distributed-transactions", "go", "hacktoberfest", "htap", "mysql", "mysql-compatibility", "scale", "serverless", "sql", "tidb"], "visibility": "public", "forks": 6089, "open_issues": 5461, "watchers": 39565, "default_branch": "master"}}, "commits": [{"sha": "d2eea49acf71b260c26925a98a00601ad5b5c45b", "parents": ["abb662f76cbd5059d9afd215ed637a5f5e2bd195"], "message": "singleton histogram chunk\n\nSigned-off-by: Wenqi Mou <wenqimou@gmail.com>"}, {"sha": "0a93788f7a1d2e851208ec1f693b5425744de463", "parents": ["d2eea49acf71b260c26925a98a00601ad5b5c45b"], "message": "address comments\n\nSigned-off-by: Wenqi Mou <wenqimou@gmail.com>"}, {"sha": "bbad494511da368cd377a28fe60687043fbe355f", "parents": ["0a93788f7a1d2e851208ec1f693b5425744de463"], "message": "unit test\n\nSigned-off-by: Wenqi Mou <wenqimou@gmail.com>"}, {"sha": "f1aeb192376e0b926e5464fe51da12e852c5c144", "parents": ["bbad494511da368cd377a28fe60687043fbe355f"], "message": "fix bazel\n\nSigned-off-by: Wenqi Mou <wenqimou@gmail.com>"}], "resolved_issues": [{"org": "pingcap", "repo": "tidb", "number": 60137, "state": "closed", "title": "planner: prevent pseudo-stats from dominating the Stats Cache", "body": "## Enhancement\nUse the python script below to reproduce this problem:\n```\nfor i in range(100000):\n    print(f\"CREATE TEMPORARY TABLE tmp_{i} (id BIGINT(20) NOT NULL,INDEX id_idx(id));\")\n    print(f\"select * from tmp_{i};\")\n    print(f\"DROP TEMPORARY TABLE tmp_{i};\")\n```\n\nIn a customer case below, pseudo-stats costs 1.8GB (40%) memory:\n![Image](https://github.com/user-attachments/assets/47252d9b-12ee-46d4-99c8-05b1ddb26e85)\n\nIn this case:\n1.  create and drop a lots of temporary tables;\n2. when accessing these tables, we create a pseudo-stats for each table, and put it into the stats cache;\n3. we don't drop stats from the stats cache when dropping a table;\n4. if this situation continues, a large number of unused pseudo-stats will dominate the stats cache and cause unexpected high memory usage.\n\nA possible foundamental solution might be that we can encapsulate all pseudo-stats as a global single object, and do not put it into our Stats Cache, this should help us save lots of memory in this scenario."}], "fix_patch": "diff --git a/pkg/planner/core/planbuilder.go b/pkg/planner/core/planbuilder.go\nindex f7b47a636df61..13ca48a9769ac 100644\n--- a/pkg/planner/core/planbuilder.go\n+++ b/pkg/planner/core/planbuilder.go\n@@ -1608,7 +1608,7 @@ func (b *PlanBuilder) buildPhysicalIndexLookUpReader(_ context.Context, dbName a\n \tidxColInfos := getIndexColumnInfos(tblInfo, idx)\n \tidxColSchema := getIndexColsSchema(tblInfo, idx, fullExprCols)\n \tidxCols, idxColLens := expression.IndexInfo2PrefixCols(idxColInfos, idxColSchema.Columns, idx)\n-\n+\tpseudoHistColl := statistics.PseudoHistColl(physicalID, false)\n \tis := physicalop.PhysicalIndexScan{\n \t\tTable:            tblInfo,\n \t\tTableAsName:      &tblInfo.Name,\n@@ -1621,10 +1621,10 @@ func (b *PlanBuilder) buildPhysicalIndexLookUpReader(_ context.Context, dbName a\n \t\tRanges:           ranger.FullRange(),\n \t\tPhysicalTableID:  physicalID,\n \t\tIsPartition:      isPartition,\n-\t\tTblColHists:      &(statistics.PseudoTable(tblInfo, false, false)).HistColl,\n+\t\tTblColHists:      &pseudoHistColl,\n \t}.Init(b.ctx, b.getSelectOffset())\n \t// There is no alternative plan choices, so just use pseudo stats to avoid panic.\n-\tis.SetStats(&property.StatsInfo{HistColl: &(statistics.PseudoTable(tblInfo, false, false)).HistColl})\n+\tis.SetStats(&property.StatsInfo{HistColl: &pseudoHistColl})\n \tif hasCommonCols {\n \t\tfor _, c := range commonInfos {\n \t\t\tis.Columns = append(is.Columns, c.ColumnInfo)\n@@ -1639,7 +1639,7 @@ func (b *PlanBuilder) buildPhysicalIndexLookUpReader(_ context.Context, dbName a\n \t\tTableAsName:     &tblInfo.Name,\n \t\tDBName:          dbName,\n \t\tPhysicalTableID: physicalID,\n-\t\tTblColHists:     &(statistics.PseudoTable(tblInfo, false, false)).HistColl,\n+\t\tTblColHists:     &pseudoHistColl,\n \t}.Init(b.ctx, b.getSelectOffset())\n \tts.SetIsPartition(isPartition)\n \tts.SetSchema(idxColSchema)\ndiff --git a/pkg/statistics/BUILD.bazel b/pkg/statistics/BUILD.bazel\nindex 3cee0ca23c22d..ebd53f29e78f0 100644\n--- a/pkg/statistics/BUILD.bazel\n+++ b/pkg/statistics/BUILD.bazel\n@@ -83,7 +83,7 @@ go_test(\n     data = glob([\"testdata/**\"]),\n     embed = [\":statistics\"],\n     flaky = True,\n-    shard_count = 41,\n+    shard_count = 42,\n     deps = [\n         \"//pkg/config\",\n         \"//pkg/meta/model\",\ndiff --git a/pkg/statistics/handle/handle.go b/pkg/statistics/handle/handle.go\nindex 99fcbe8fb17d9..d7ddda75d9e1d 100644\n--- a/pkg/statistics/handle/handle.go\n+++ b/pkg/statistics/handle/handle.go\n@@ -174,8 +174,8 @@ func NewHandle(\n // physicalTableID can be a table ID or partition ID.\n func (h *Handle) GetPhysicalTableStats(physicalTableID int64, tblInfo *model.TableInfo) *statistics.Table {\n \ttblStats, found := h.getStatsByPhysicalID(physicalTableID, tblInfo)\n-\tintest.Assert(tblStats != nil, \"stats shoud not be nil\")\n-\tintest.Assert(found, \"stats shoud not be nil\")\n+\tintest.Assert(tblStats != nil, \"stats should not be nil\")\n+\tintest.Assert(found, \"stats should not be nil\")\n \treturn tblStats\n }\n \ndiff --git a/pkg/statistics/histogram.go b/pkg/statistics/histogram.go\nindex 41f185ccbc1b2..caa725df0e0a8 100644\n--- a/pkg/statistics/histogram.go\n+++ b/pkg/statistics/histogram.go\n@@ -53,6 +53,12 @@ const (\n \toutOfRangeBetweenRate float64 = 100\n )\n \n+var (\n+\t// Global static chunk for pseudo histograms to avoid chunk allocation\n+\tglobalPseudoChunkOnce sync.Once\n+\tglobalPseudoChunk     *chunk.Chunk\n+)\n+\n // Histogram represents statistics for a column or index.\n type Histogram struct {\n \tTp *types.FieldType\n@@ -115,15 +121,55 @@ type scalar struct {\n // EmptyScalarSize is the size of empty scalar.\n const EmptyScalarSize = int64(unsafe.Sizeof(scalar{}))\n \n-// NewHistogram creates a new histogram.\n-func NewHistogram(id, ndv, nullCount int64, version uint64, tp *types.FieldType, bucketSize int, totColSize int64) *Histogram {\n+// initGlobalPseudoChunk initializes the global static chunk for pseudo histograms\n+func initGlobalPseudoChunk() {\n+\t// Create a minimal empty chunk that can be shared across all pseudo histograms\n+\t// Use a basic field type that won't cause issues when shared\n+\tglobalPseudoChunk = chunk.NewEmptyChunk([]*types.FieldType{types.NewFieldType(mysql.TypeBlob)})\n+}\n+\n+// getGlobalPseudoChunk returns the shared static chunk for pseudo histograms\n+// WARNING: The returned chunk MUST NOT be modified. It is shared across all pseudo histograms.\n+// Pseudo histograms should never have buckets added or bounds modified.\n+func getGlobalPseudoChunk() *chunk.Chunk {\n+\tglobalPseudoChunkOnce.Do(initGlobalPseudoChunk)\n+\treturn globalPseudoChunk\n+}\n+\n+// prepareFieldTypeForHistogram prepares the field type for histogram usage.\n+// For string types, it clones the field type and sets the collation to binary\n+// to avoid decoding issues with the histogram's key representation.\n+func prepareFieldTypeForHistogram(tp *types.FieldType) *types.FieldType {\n \tif tp.EvalType() == types.ETString {\n \t\t// The histogram will store the string value's 'sort key' representation of its collation.\n-\t\t// If we directly set the field type's collation to its original one. We would decode the Key representation using its collation.\n+\t\t// If we directly set the field type's collation to its original one, we would decode the Key representation using its collation.\n \t\t// This would cause panic. So we apply a little trick here to avoid decoding it by explicitly changing the collation to 'CollationBin'.\n \t\ttp = tp.Clone()\n \t\ttp.SetCollate(charset.CollationBin)\n \t}\n+\treturn tp\n+}\n+\n+// NewPseudoHistogram creates a pseudo histogram that reuses global static components\n+// This avoids chunk allocation while preserving field type semantics.\n+func NewPseudoHistogram(id int64, tp *types.FieldType) *Histogram {\n+\ttp = prepareFieldTypeForHistogram(tp)\n+\treturn &Histogram{\n+\t\tID:                id,\n+\t\tNDV:               0,\n+\t\tNullCount:         0,\n+\t\tLastUpdateVersion: 0,\n+\t\tTp:                tp,\n+\t\tBounds:            getGlobalPseudoChunk(),\n+\t\tBuckets:           make([]Bucket, 0),\n+\t\tTotColSize:        0,\n+\t\tCorrelation:       0,\n+\t}\n+}\n+\n+// NewHistogram creates a new histogram.\n+func NewHistogram(id, ndv, nullCount int64, version uint64, tp *types.FieldType, bucketSize int, totColSize int64) *Histogram {\n+\ttp = prepareFieldTypeForHistogram(tp)\n \treturn &Histogram{\n \t\tID:                id,\n \t\tNDV:               ndv,\ndiff --git a/pkg/statistics/table.go b/pkg/statistics/table.go\nindex db73e30549d6e..db1326528ace3 100644\n--- a/pkg/statistics/table.go\n+++ b/pkg/statistics/table.go\n@@ -1069,23 +1069,39 @@ func (coll *HistColl) GenerateHistCollFromColumnInfo(tblInfo *model.TableInfo, c\n \treturn newColl\n }\n \n+// PseudoHistColl creates a lightweight pseudo HistColl for cost calculation.\n+// This is optimized for cases where only HistColl is needed, avoiding the overhead\n+// of creating a full pseudo table with ColAndIdxExistenceMap and other structures.\n+func PseudoHistColl(physicalID int64, allowTriggerLoading bool) HistColl {\n+\treturn HistColl{\n+\t\tRealtimeCount:     PseudoRowCount,\n+\t\tPhysicalID:        physicalID,\n+\t\tcolumns:           nil,\n+\t\tindices:           nil,\n+\t\tPseudo:            true,\n+\t\tCanNotTriggerLoad: !allowTriggerLoading,\n+\t\tModifyCount:       0,\n+\t\tStatsVer:          0,\n+\t}\n+}\n+\n // PseudoTable creates a pseudo table statistics.\n // Usually, we don't want to trigger stats loading for pseudo table.\n // But there are exceptional cases. In such cases, we should pass allowTriggerLoading as true.\n // Such case could possibly happen in getStatsTable().\n func PseudoTable(tblInfo *model.TableInfo, allowTriggerLoading bool, allowFillHistMeta bool) *Table {\n-\tpseudoHistColl := HistColl{\n-\t\tRealtimeCount:     PseudoRowCount,\n-\t\tPhysicalID:        tblInfo.ID,\n-\t\tcolumns:           make(map[int64]*Column, 2),\n-\t\tindices:           make(map[int64]*Index, 2),\n-\t\tPseudo:            true,\n-\t\tCanNotTriggerLoad: !allowTriggerLoading,\n-\t}\n \tt := &Table{\n-\t\tHistColl:              pseudoHistColl,\n+\t\tHistColl:              PseudoHistColl(tblInfo.ID, allowTriggerLoading),\n+\t\tVersion:               PseudoVersion,\n \t\tColAndIdxExistenceMap: NewColAndIndexExistenceMap(len(tblInfo.Columns), len(tblInfo.Indices)),\n \t}\n+\n+\t// Initialize columns and indices maps only when allowFillHistMeta is true\n+\tif allowFillHistMeta {\n+\t\tt.columns = make(map[int64]*Column, len(tblInfo.Columns))\n+\t\tt.indices = make(map[int64]*Index, len(tblInfo.Indices))\n+\t}\n+\n \tfor _, col := range tblInfo.Columns {\n \t\t// The column is public to use. Also we should check the column is not hidden since hidden means that it's used by expression index.\n \t\t// We would not collect stats for the hidden column and we won't use the hidden column to estimate.\n@@ -1097,7 +1113,7 @@ func PseudoTable(tblInfo *model.TableInfo, allowTriggerLoading bool, allowFillHi\n \t\t\t\t\tPhysicalID: tblInfo.ID,\n \t\t\t\t\tInfo:       col,\n \t\t\t\t\tIsHandle:   tblInfo.PKIsHandle && mysql.HasPriKeyFlag(col.GetFlag()),\n-\t\t\t\t\tHistogram:  *NewHistogram(col.ID, 0, 0, 0, &col.FieldType, 0, 0),\n+\t\t\t\t\tHistogram:  *NewPseudoHistogram(col.ID, &col.FieldType),\n \t\t\t\t}\n \t\t\t}\n \t\t}\n@@ -1109,7 +1125,7 @@ func PseudoTable(tblInfo *model.TableInfo, allowTriggerLoading bool, allowFillHi\n \t\t\t\tt.indices[idx.ID] = &Index{\n \t\t\t\t\tPhysicalID: tblInfo.ID,\n \t\t\t\t\tInfo:       idx,\n-\t\t\t\t\tHistogram:  *NewHistogram(idx.ID, 0, 0, 0, types.NewFieldType(mysql.TypeBlob), 0, 0),\n+\t\t\t\t\tHistogram:  *NewPseudoHistogram(idx.ID, types.NewFieldType(mysql.TypeBlob)),\n \t\t\t\t}\n \t\t\t}\n \t\t}\n", "test_patch": "diff --git a/pkg/statistics/histogram_test.go b/pkg/statistics/histogram_test.go\nindex 4ba9364883e25..2aa73037e34ee 100644\n--- a/pkg/statistics/histogram_test.go\n+++ b/pkg/statistics/histogram_test.go\n@@ -696,3 +696,42 @@ func TestStandardizeForV2AnalyzeIndex(t *testing.T) {\n \t\t\tfmt.Sprintf(\"testData[%d].inputHist:%s\", i, test.inputHistToStr))\n \t}\n }\n+\n+func TestNewPseudoHistogramReuseChunk(t *testing.T) {\n+\tconst (\n+\t\tmsgPseudoSameChunk   = \"pseudo histograms should share the same Bounds chunk\"\n+\t\tmsgRegularDiffChunks = \"regular histograms should have different Bounds chunks\"\n+\t\tmsgRegularPseudoDiff = \"regular and pseudo histograms should have different Bounds chunks\"\n+\t)\n+\n+\t// test that NewPseudoHistogram reuses the same global chunk instance\n+\ttp1 := types.NewFieldType(mysql.TypeLonglong)\n+\ttp2 := types.NewFieldType(mysql.TypeVarchar)\n+\ttp3 := types.NewFieldType(mysql.TypeBlob)\n+\n+\thist1 := NewPseudoHistogram(1, tp1)\n+\thist2 := NewPseudoHistogram(2, tp2)\n+\thist3 := NewPseudoHistogram(3, tp3)\n+\n+\t// verify that all pseudo histograms share the same Bounds chunk instance\n+\trequire.Same(t, hist1.Bounds, hist2.Bounds, msgPseudoSameChunk)\n+\trequire.Same(t, hist1.Bounds, hist3.Bounds, msgPseudoSameChunk)\n+\trequire.Same(t, hist2.Bounds, hist3.Bounds, msgPseudoSameChunk)\n+\n+\t// verify that regular histograms do NOT share chunks\n+\tregularHist1 := NewHistogram(1, 0, 0, 0, tp1, 10, 0)\n+\tregularHist2 := NewHistogram(2, 0, 0, 0, tp2, 10, 0)\n+\n+\trequire.NotSame(t, regularHist1.Bounds, regularHist2.Bounds, msgRegularDiffChunks)\n+\trequire.NotSame(t, regularHist1.Bounds, hist1.Bounds, msgRegularPseudoDiff)\n+\trequire.NotSame(t, regularHist2.Bounds, hist1.Bounds, msgRegularPseudoDiff)\n+\n+\t// verify that string type field types are properly handled\n+\trequire.Equal(t, mysql.TypeLonglong, hist1.Tp.GetType())\n+\trequire.Equal(t, mysql.TypeVarchar, hist2.Tp.GetType())\n+\trequire.Equal(t, mysql.TypeBlob, hist3.Tp.GetType())\n+\n+\t// for string types, collation should be set to binary\n+\trequire.Equal(t, \"binary\", hist2.Tp.GetCollate())\n+\trequire.Equal(t, \"binary\", hist3.Tp.GetCollate())\n+}\n"}
{"org": "pingcap", "repo": "tidb", "number": 61633, "state": "closed", "title": "executor: optimize to get key ranges", "body": "<!--\r\n\r\nThank you for contributing to TiDB!\r\n\r\nPR Title Format:\r\n1. pkg [, pkg2, pkg3]: what's changed\r\n2. *: what's changed\r\n\r\n-->\r\n\r\n### What problem does this PR solve?\r\n<!--\r\n\r\nPlease create an issue first to describe the problem.\r\n\r\nThere MUST be one line starting with \"Issue Number:  \" and\r\nlinking the relevant issues via the \"close\" or \"ref\".\r\n\r\nFor more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.\r\n\r\n-->\r\n\r\nIssue Number: close #61640\r\n\r\nProblem Summary:\r\n\r\n### What changed and how does it work?\r\n\r\n### Check List\r\n\r\nTests <!-- At least one of them must be included. -->\r\n\r\n- [x] Unit test\r\n- [x] Integration test\r\n- [ ] Manual test (add detailed scripts or steps below)\r\n- [ ] No need to test\r\n  > - [ ] I checked and no code files have been changed.\r\n  > <!-- Or your custom  \"No need to test\" reasons -->\r\n\r\nSide effects\r\n\r\n- [ ] Performance regression: Consumes more CPU\r\n- [ ] Performance regression: Consumes more Memory\r\n- [ ] Breaking backward compatibility\r\n\r\nDocumentation\r\n\r\n- [ ] Affects user behaviors\r\n- [ ] Contains syntax changes\r\n- [ ] Contains variable changes\r\n- [ ] Contains experimental features\r\n- [ ] Changes MySQL compatibility\r\n\r\n### Release note\r\n\r\n<!-- compatibility change, improvement, bugfix, and new feature need a release note -->\r\n\r\nPlease refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.\r\n\r\n```release-note\r\nNone\r\n```\r\n", "url": "https://api.github.com/repos/pingcap/tidb/pulls/61633", "id": 2581379702, "node_id": "PR_kwDOAoCpQc6Z3Lp2", "html_url": "https://github.com/pingcap/tidb/pull/61633", "diff_url": "https://github.com/pingcap/tidb/pull/61633.diff", "patch_url": "https://github.com/pingcap/tidb/pull/61633.patch", "issue_url": "https://api.github.com/repos/pingcap/tidb/issues/61633", "created_at": "2025-06-10T13:29:55+00:00", "updated_at": "2025-06-12T07:23:54+00:00", "closed_at": "2025-06-12T07:23:20+00:00", "merged_at": "2025-06-12T07:23:19+00:00", "merge_commit_sha": "648b03504954eb997a99175d6dbd3ffcba8526fb", "labels": ["sig/planner", "size/L", "release-note-none", "ok-to-test", "approved", "lgtm"], "draft": false, "commits_url": "https://api.github.com/repos/pingcap/tidb/pulls/61633/commits", "review_comments_url": "https://api.github.com/repos/pingcap/tidb/pulls/61633/comments", "review_comment_url": "https://api.github.com/repos/pingcap/tidb/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/pingcap/tidb/issues/61633/comments", "base": {"label": "pingcap:master", "ref": "master", "sha": "11276faa9dc05ed7047f766a4c1fb45cd2b83109", "user": {"login": "pingcap", "id": 11855343, "node_id": "MDEyOk9yZ2FuaXphdGlvbjExODU1MzQz", "avatar_url": "https://avatars.githubusercontent.com/u/11855343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pingcap", "html_url": "https://github.com/pingcap", "followers_url": "https://api.github.com/users/pingcap/followers", "following_url": "https://api.github.com/users/pingcap/following{/other_user}", "gists_url": "https://api.github.com/users/pingcap/gists{/gist_id}", "starred_url": "https://api.github.com/users/pingcap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pingcap/subscriptions", "organizations_url": "https://api.github.com/users/pingcap/orgs", "repos_url": "https://api.github.com/users/pingcap/repos", "events_url": "https://api.github.com/users/pingcap/events{/privacy}", "received_events_url": "https://api.github.com/users/pingcap/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 41986369, "node_id": "MDEwOlJlcG9zaXRvcnk0MTk4NjM2OQ==", "name": "tidb", "full_name": "pingcap/tidb", "private": false, "owner": {"login": "pingcap", "id": 11855343, "node_id": "MDEyOk9yZ2FuaXphdGlvbjExODU1MzQz", "avatar_url": "https://avatars.githubusercontent.com/u/11855343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pingcap", "html_url": "https://github.com/pingcap", "followers_url": "https://api.github.com/users/pingcap/followers", "following_url": "https://api.github.com/users/pingcap/following{/other_user}", "gists_url": "https://api.github.com/users/pingcap/gists{/gist_id}", "starred_url": "https://api.github.com/users/pingcap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pingcap/subscriptions", "organizations_url": "https://api.github.com/users/pingcap/orgs", "repos_url": "https://api.github.com/users/pingcap/repos", "events_url": "https://api.github.com/users/pingcap/events{/privacy}", "received_events_url": "https://api.github.com/users/pingcap/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/pingcap/tidb", "description": "TiDB - the open-source, cloud-native, distributed SQL database designed for modern applications.", "fork": false, "url": "https://api.github.com/repos/pingcap/tidb", "forks_url": "https://api.github.com/repos/pingcap/tidb/forks", "keys_url": "https://api.github.com/repos/pingcap/tidb/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/pingcap/tidb/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/pingcap/tidb/teams", "hooks_url": "https://api.github.com/repos/pingcap/tidb/hooks", "issue_events_url": "https://api.github.com/repos/pingcap/tidb/issues/events{/number}", "events_url": "https://api.github.com/repos/pingcap/tidb/events", "assignees_url": "https://api.github.com/repos/pingcap/tidb/assignees{/user}", "branches_url": "https://api.github.com/repos/pingcap/tidb/branches{/branch}", "tags_url": "https://api.github.com/repos/pingcap/tidb/tags", "blobs_url": "https://api.github.com/repos/pingcap/tidb/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/pingcap/tidb/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/pingcap/tidb/git/refs{/sha}", "trees_url": "https://api.github.com/repos/pingcap/tidb/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/pingcap/tidb/statuses/{sha}", "languages_url": "https://api.github.com/repos/pingcap/tidb/languages", "stargazers_url": "https://api.github.com/repos/pingcap/tidb/stargazers", "contributors_url": "https://api.github.com/repos/pingcap/tidb/contributors", "subscribers_url": "https://api.github.com/repos/pingcap/tidb/subscribers", "subscription_url": "https://api.github.com/repos/pingcap/tidb/subscription", "commits_url": "https://api.github.com/repos/pingcap/tidb/commits{/sha}", "git_commits_url": "https://api.github.com/repos/pingcap/tidb/git/commits{/sha}", "comments_url": "https://api.github.com/repos/pingcap/tidb/comments{/number}", "issue_comment_url": "https://api.github.com/repos/pingcap/tidb/issues/comments{/number}", "contents_url": "https://api.github.com/repos/pingcap/tidb/contents/{+path}", "compare_url": "https://api.github.com/repos/pingcap/tidb/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/pingcap/tidb/merges", "archive_url": "https://api.github.com/repos/pingcap/tidb/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/pingcap/tidb/downloads", "issues_url": "https://api.github.com/repos/pingcap/tidb/issues{/number}", "pulls_url": "https://api.github.com/repos/pingcap/tidb/pulls{/number}", "milestones_url": "https://api.github.com/repos/pingcap/tidb/milestones{/number}", "notifications_url": "https://api.github.com/repos/pingcap/tidb/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/pingcap/tidb/labels{/name}", "releases_url": "https://api.github.com/repos/pingcap/tidb/releases{/id}", "deployments_url": "https://api.github.com/repos/pingcap/tidb/deployments", "created_at": "2015-09-06T04:01:52Z", "updated_at": "2026-01-07T08:42:13Z", "pushed_at": "2026-01-07T08:42:08Z", "git_url": "git://github.com/pingcap/tidb.git", "ssh_url": "git@github.com:pingcap/tidb.git", "clone_url": "https://github.com/pingcap/tidb.git", "svn_url": "https://github.com/pingcap/tidb", "homepage": "https://pingcap.com", "size": 588595, "stargazers_count": 39565, "watchers_count": 39565, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": true, "forks_count": 6089, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 5461, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "topics": ["cloud-native", "database", "distributed-database", "distributed-transactions", "go", "hacktoberfest", "htap", "mysql", "mysql-compatibility", "scale", "serverless", "sql", "tidb"], "visibility": "public", "forks": 6089, "open_issues": 5461, "watchers": 39565, "default_branch": "master"}}, "commits": [{"sha": "81c69d156f83466ed28ab243d3f6b6dc7254b58f", "parents": ["51f0587d495686f1bd7b39fee59121e3650c2bba"], "message": "optimize key ranges\n\nSigned-off-by: ç«¥å‰‘ <1045931706@qq.com>"}, {"sha": "a9cb972c06469b07b76dcd236e4c6c76f7c6d2ca", "parents": ["81c69d156f83466ed28ab243d3f6b6dc7254b58f"], "message": "use slices.SortFunc\n\nSigned-off-by: ç«¥å‰‘ <1045931706@qq.com>"}, {"sha": "0d046ac6012b3f281156c6cc78fdbab838479a5b", "parents": ["a9cb972c06469b07b76dcd236e4c6c76f7c6d2ca", "11276faa9dc05ed7047f766a4c1fb45cd2b83109"], "message": "Merge branch 'master' of https://github.com/pingcap/tidb into optimize_key_ranges"}, {"sha": "004e72e55d270836b5577773618a4354e422eade", "parents": ["0d046ac6012b3f281156c6cc78fdbab838479a5b"], "message": "sort\n\nSigned-off-by: bufferflies <1045931706@qq.com>"}], "resolved_issues": [{"org": "pingcap", "repo": "tidb", "number": 61640, "state": "closed", "title": "optimize the keyrange if using multi continuous partitions", "body": "## Enhancement\n"}], "fix_patch": "diff --git a/pkg/executor/distribute.go b/pkg/executor/distribute.go\nindex 783836002157c..3afe005d4b235 100644\n--- a/pkg/executor/distribute.go\n+++ b/pkg/executor/distribute.go\n@@ -15,7 +15,9 @@\n package executor\n \n import (\n+\t\"cmp\"\n \t\"context\"\n+\t\"slices\"\n \t\"strings\"\n \n \t\"github.com/pingcap/tidb/pkg/domain/infosync\"\n@@ -55,6 +57,9 @@ func (e *DistributeTableExec) Open(context.Context) error {\n \t\treturn err\n \t}\n \te.keyRanges = ranges\n+\tslices.SortFunc(e.partitionNames, func(i, j ast.CIStr) int {\n+\t\treturn cmp.Compare(i.L, j.L)\n+\t})\n \treturn nil\n }\n \n@@ -158,13 +163,18 @@ func (e *DistributeTableExec) getKeyRanges() ([]*pdhttp.KeyRange, error) {\n \t\t\t}\n \t\t}\n \t}\n+\tslices.Sort(physicalIDs)\n \n \tranges := make([]*pdhttp.KeyRange, 0, len(physicalIDs))\n-\tfor _, pid := range physicalIDs {\n-\t\tstartKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid))\n-\t\tendKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid+1))\n-\t\tr := pdhttp.NewKeyRange(startKey, endKey)\n-\t\tranges = append(ranges, r)\n+\tfor i, pid := range physicalIDs {\n+\t\tif i == 0 || physicalIDs[i] != physicalIDs[i-1]+1 {\n+\t\t\tstartKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid))\n+\t\t\tendKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid+1))\n+\t\t\tr := pdhttp.NewKeyRange(startKey, endKey)\n+\t\t\tranges = append(ranges, r)\n+\t\t} else {\n+\t\t\tranges[len(ranges)-1].EndKey = codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid+1))\n+\t\t}\n \t}\n \treturn ranges, nil\n }\ndiff --git a/pkg/planner/core/planbuilder.go b/pkg/planner/core/planbuilder.go\nindex 1ca92b445f254..e12be01de4141 100644\n--- a/pkg/planner/core/planbuilder.go\n+++ b/pkg/planner/core/planbuilder.go\n@@ -4821,7 +4821,7 @@ func (b *PlanBuilder) buildDistributeTable(node *ast.DistributeTableStmt) (base.\n \ttnW := b.resolveCtx.GetTableName(node.Table)\n \ttblInfo := tnW.TableInfo\n \tif !slices.Contains(ruleList, node.Rule.L) {\n-\t\treturn nil, plannererrors.ErrWrongArguments.GenWithStackByArgs(\"rule must be leader-scatter, follower-scatter or learner-scatter\")\n+\t\treturn nil, plannererrors.ErrWrongArguments.GenWithStackByArgs(\"rule must be leader-scatter, peer-scatter or learner-scatter\")\n \t}\n \tif !slices.Contains(engineList, node.Engine.L) {\n \t\treturn nil, plannererrors.ErrWrongArguments.GenWithStackByArgs(\"engine must be tikv or tiflash\")\n", "test_patch": "diff --git a/pkg/executor/distribute_table_test.go b/pkg/executor/distribute_table_test.go\nindex 1eea545f4e5e1..9cf60c3403572 100644\n--- a/pkg/executor/distribute_table_test.go\n+++ b/pkg/executor/distribute_table_test.go\n@@ -19,6 +19,7 @@ import (\n \t\"encoding/json\"\n \t\"fmt\"\n \t\"net/url\"\n+\t\"slices\"\n \t\"strings\"\n \t\"testing\"\n \t\"time\"\n@@ -124,19 +125,43 @@ func TestDistributeTable(t *testing.T) {\n \tcli := &MockDistributePDCli{}\n \trecoverCli := infosync.SetPDHttpCliForTest(cli)\n \tdefer recoverCli()\n-\tmockCreateSchedulerWithInput := func(tblName, partition string, config map[string]any) *mock.Call {\n+\tmockCreateSchedulerWithInput := func(tblName string, config map[string]any, partitions []string) *mock.Call {\n \t\tis := tk.Session().GetDomainInfoSchema()\n \t\ttbl, err := is.TableInfoByName(ast.NewCIStr(database), ast.NewCIStr(tblName))\n \t\trequire.NoError(t, err)\n \t\ttblID := tbl.ID\n-\t\tif partition != \"\" {\n-\t\t\ttblID = tbl.GetPartitionInfo().GetPartitionIDByName(partition)\n+\t\tpi := tbl.GetPartitionInfo()\n+\t\tpids := make([]int64, 0)\n+\t\tif pi == nil {\n+\t\t\tpids = append(pids, tblID)\n+\t\t} else {\n+\t\t\tif len(partitions) > 0 {\n+\t\t\t\tfor _, partition := range partitions {\n+\t\t\t\t\tpids = append(pids, pi.GetPartitionIDByName(partition))\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tfor _, partition := range pi.Definitions {\n+\t\t\t\t\tpids = append(pids, partition.ID)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tstarts := make([]string, 0)\n+\t\tends := make([]string, 0)\n+\t\tslices.Sort(pids)\n+\t\tfor i, pid := range pids {\n+\t\t\tif i == 0 || pid != pids[i-1]+1 {\n+\t\t\t\tstartKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid))\n+\t\t\t\tendKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid+1))\n+\t\t\t\tstarts = append(starts, url.QueryEscape(string(startKey)))\n+\t\t\t\tends = append(ends, url.QueryEscape(string(endKey)))\n+\t\t\t} else {\n+\t\t\t\tendKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(pid+1))\n+\t\t\t\tends[len(starts)-1] = url.QueryEscape(string(endKey))\n+\t\t\t}\n \t\t}\n \n-\t\tstartKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(tblID))\n-\t\tendKey := codec.EncodeBytes([]byte{}, tablecodec.GenTablePrefix(tblID+1))\n-\t\tconfig[\"start-key\"] = url.QueryEscape(string(startKey))\n-\t\tconfig[\"end-key\"] = url.QueryEscape(string(endKey))\n+\t\tconfig[\"start-key\"] = strings.Join(starts, \",\")\n+\t\tconfig[\"end-key\"] = strings.Join(ends, \",\")\n \t\treturn cli.On(\"CreateSchedulerWithInput\", mock.Anything, schedulerName, config).\n \t\t\tReturn(nil)\n \t}\n@@ -147,24 +172,42 @@ func TestDistributeTable(t *testing.T) {\n \ttable := \"t1\"\n \tpartition := \"\"\n \n-\talias := strings.Join([]string{database, table, partition}, \".\")\n \tconfig := map[string]any{\n-\t\t\"alias\":  alias,\n+\t\t\"alias\":  strings.Join([]string{database, table, partition}, \".\"),\n \t\t\"engine\": \"tikv\",\n \t\t\"rule\":   \"leader-scatter\",\n \t}\n-\ttk.MustExec(\"create table t1(a int)\")\n+\ttk.MustExec(`create table t1 (\n+\t\tc1 int, c2 int, c3 int\n+\t)\n+\tpartition by range( c1 ) (\n+\t\tpartition p0 values less than (1024),\n+\t\tpartition p1 values less than (2048),\n+\t\tpartition p2 values less than (3072)\n+\t   );`)\n \tmockGetSchedulerConfig(\"balance-range-scheduler\")\n-\tmockCreateSchedulerWithInput(table, partition, config)\n+\tmockCreateSchedulerWithInput(table, config, nil)\n \ttk.MustQuery(fmt.Sprintf(\"distribute table %s rule=`leader-scatter` engine=tikv\", table)).Check(testkit.Rows(\"1\"))\n-\t// create new scheduler with the same inputs\n-\tmockCreateSchedulerWithInput(table, partition, config)\n-\ttk.MustQuery(fmt.Sprintf(\"distribute table %s rule=`leader-scatter` engine=tikv\", table)).Check(testkit.Rows(\"2\"))\n+\t// test for multi partitions\n+\tpartition = \"partition(p1,p2)\"\n+\tconfig[\"alias\"] = strings.Join([]string{database, table, partition}, \".\")\n+\tmockCreateSchedulerWithInput(table, config, []string{\"p1\", \"p2\"})\n+\ttk.MustQuery(fmt.Sprintf(\"distribute table %s partition (p1,p2) rule=`leader-scatter` engine=tikv\", table)).Check(testkit.Rows(\"2\"))\n+\t// test for unordered partitions\n+\ttk.MustQuery(fmt.Sprintf(\"distribute table %s partition (p2,p1) rule=`leader-scatter` engine=tikv\", table)).Check(testkit.Rows(\"3\"))\n \n \t// test for timeout\n+\tpartition = \"partition(p0)\"\n+\tconfig[\"alias\"] = strings.Join([]string{database, table, partition}, \".\")\n+\tconfig[\"timeout\"] = \"30m\"\n+\tmockCreateSchedulerWithInput(table, config, []string{\"p0\"})\n+\ttk.MustQuery(fmt.Sprintf(\"distribute table %s partition(p0) rule=`leader-scatter` engine=tikv timeout=`30m`\", table)).Check(testkit.Rows(\"4\"))\n+\n+\tpartition = \"partition(p0,p2)\"\n+\tconfig[\"alias\"] = strings.Join([]string{database, table, partition}, \".\")\n \tconfig[\"timeout\"] = \"30m\"\n-\tmockCreateSchedulerWithInput(table, partition, config)\n-\ttk.MustQuery(fmt.Sprintf(\"distribute table %s rule=`leader-scatter` engine=tikv timeout=`30m`\", table)).Check(testkit.Rows(\"3\"))\n+\tmockCreateSchedulerWithInput(table, config, []string{\"p0\", \"p2\"})\n+\ttk.MustQuery(fmt.Sprintf(\"distribute table %s partition(p0,p2) rule=`leader-scatter` engine=tikv timeout=`30m`\", table)).Check(testkit.Rows(\"5\"))\n \n \t// test for incorrect arguments\n \ttk.MustGetErrMsg(fmt.Sprintf(\"distribute table %s rule=`leader-scatter` engine=tiflash\", table),\n@@ -172,7 +215,7 @@ func TestDistributeTable(t *testing.T) {\n \ttk.MustGetErrMsg(fmt.Sprintf(\"distribute table %s rule=`leader-scatter` engine=titan\", table),\n \t\t\"[planner:1210]Incorrect arguments to engine must be tikv or tiflash\")\n \ttk.MustGetErrMsg(fmt.Sprintf(\"distribute table %s rule=`witness` engine=tikv\", table),\n-\t\t\"[planner:1210]Incorrect arguments to rule must be leader-scatter, follower-scatter or learner-scatter\")\n+\t\t\"[planner:1210]Incorrect arguments to rule must be leader-scatter, peer-scatter or learner-scatter\")\n }\n \n func TestShowTableDistributions(t *testing.T) {\n"}
{"org": "go-gitea", "repo": "gitea", "number": 34154, "state": "closed", "title": "Refactor Git Attribute & performance optimization", "body": "This PR moved git attributes related code to `modules/git/attribute` sub package and moved language stats related code to `modules/git/languagestats` sub package to make it easier to maintain.\r\n\r\nAnd it also introduced a performance improvement which use the `git check-attr --source` which can be run in a bare git repository so that we don't need to create a git index file. The new parameter need a git version >= 2.40 . If git version less than 2.40, it will fall back to previous implementation.\r\n\r\nBefore:\r\n```\r\ngit-run duration=1.2684s func.caller=git.(*Repository).readTreeToIndex git.command=\"/opt/homebrew/bin/git ...global... read-tree 70187f7727d4ddd8282b576ece93ca233e88b19e\"\r\ngit-run duration=0.1633s func.caller=git.(*CheckAttributeReader).Run git.command=\"/opt/homebrew/bin/git ...global... check-attr --stdin -z --cached linguist-vendored linguist-generated linguist-documentation linguist-detectable linguist-language gitlab-language\"\r\n```\r\n\r\nAfter:\r\n```\r\ngit-run duration=0.1271s func.caller=attribute.(*BatchChecker).run git.command=\"/opt/homebrew/bin/git ...global... check-attr -z --source 70187f7727d4ddd8282b576ece93ca233e88b19e linguist-vendored linguist-generated linguist-language gitlab-language --stdin\"\r\n```", "url": "https://api.github.com/repos/go-gitea/gitea/pulls/34154", "id": 2447002148, "node_id": "PR_kwDOBFIx286R2kok", "html_url": "https://github.com/go-gitea/gitea/pull/34154", "diff_url": "https://github.com/go-gitea/gitea/pull/34154.diff", "patch_url": "https://github.com/go-gitea/gitea/pull/34154.patch", "issue_url": "https://api.github.com/repos/go-gitea/gitea/issues/34154", "created_at": "2025-04-08T21:36:11+00:00", "updated_at": "2025-07-10T15:46:47+00:00", "closed_at": "2025-04-11T13:41:29+00:00", "merged_at": "2025-04-11T13:41:29+00:00", "merge_commit_sha": "ae0af8ea5b2de99a49add2b7f7b76dde62a8a617", "labels": ["lgtm/done", "type/refactoring", "performance/speed", "modifies/go"], "draft": false, "commits_url": "https://api.github.com/repos/go-gitea/gitea/pulls/34154/commits", "review_comments_url": "https://api.github.com/repos/go-gitea/gitea/pulls/34154/comments", "review_comment_url": "https://api.github.com/repos/go-gitea/gitea/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/go-gitea/gitea/issues/34154/comments", "base": {"label": "go-gitea:main", "ref": "main", "sha": "d725b78824a6e83bc5f6db3c83f742810241d1ee", "user": {"login": "go-gitea", "id": 12724356, "node_id": "MDEyOk9yZ2FuaXphdGlvbjEyNzI0MzU2", "avatar_url": "https://avatars.githubusercontent.com/u/12724356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/go-gitea", "html_url": "https://github.com/go-gitea", "followers_url": "https://api.github.com/users/go-gitea/followers", "following_url": "https://api.github.com/users/go-gitea/following{/other_user}", "gists_url": "https://api.github.com/users/go-gitea/gists{/gist_id}", "starred_url": "https://api.github.com/users/go-gitea/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/go-gitea/subscriptions", "organizations_url": "https://api.github.com/users/go-gitea/orgs", "repos_url": "https://api.github.com/users/go-gitea/repos", "events_url": "https://api.github.com/users/go-gitea/events{/privacy}", "received_events_url": "https://api.github.com/users/go-gitea/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 72495579, "node_id": "MDEwOlJlcG9zaXRvcnk3MjQ5NTU3OQ==", "name": "gitea", "full_name": "go-gitea/gitea", "private": false, "owner": {"login": "go-gitea", "id": 12724356, "node_id": "MDEyOk9yZ2FuaXphdGlvbjEyNzI0MzU2", "avatar_url": "https://avatars.githubusercontent.com/u/12724356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/go-gitea", "html_url": "https://github.com/go-gitea", "followers_url": "https://api.github.com/users/go-gitea/followers", "following_url": "https://api.github.com/users/go-gitea/following{/other_user}", "gists_url": "https://api.github.com/users/go-gitea/gists{/gist_id}", "starred_url": "https://api.github.com/users/go-gitea/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/go-gitea/subscriptions", "organizations_url": "https://api.github.com/users/go-gitea/orgs", "repos_url": "https://api.github.com/users/go-gitea/repos", "events_url": "https://api.github.com/users/go-gitea/events{/privacy}", "received_events_url": "https://api.github.com/users/go-gitea/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/go-gitea/gitea", "description": "Git with a cup of tea! Painless self-hosted all-in-one software development service, including Git hosting, code review, team collaboration, package registry and CI/CD", "fork": false, "url": "https://api.github.com/repos/go-gitea/gitea", "forks_url": "https://api.github.com/repos/go-gitea/gitea/forks", "keys_url": "https://api.github.com/repos/go-gitea/gitea/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/go-gitea/gitea/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/go-gitea/gitea/teams", "hooks_url": "https://api.github.com/repos/go-gitea/gitea/hooks", "issue_events_url": "https://api.github.com/repos/go-gitea/gitea/issues/events{/number}", "events_url": "https://api.github.com/repos/go-gitea/gitea/events", "assignees_url": "https://api.github.com/repos/go-gitea/gitea/assignees{/user}", "branches_url": "https://api.github.com/repos/go-gitea/gitea/branches{/branch}", "tags_url": "https://api.github.com/repos/go-gitea/gitea/tags", "blobs_url": "https://api.github.com/repos/go-gitea/gitea/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/go-gitea/gitea/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/go-gitea/gitea/git/refs{/sha}", "trees_url": "https://api.github.com/repos/go-gitea/gitea/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/go-gitea/gitea/statuses/{sha}", "languages_url": "https://api.github.com/repos/go-gitea/gitea/languages", "stargazers_url": "https://api.github.com/repos/go-gitea/gitea/stargazers", "contributors_url": "https://api.github.com/repos/go-gitea/gitea/contributors", "subscribers_url": "https://api.github.com/repos/go-gitea/gitea/subscribers", "subscription_url": "https://api.github.com/repos/go-gitea/gitea/subscription", "commits_url": "https://api.github.com/repos/go-gitea/gitea/commits{/sha}", "git_commits_url": "https://api.github.com/repos/go-gitea/gitea/git/commits{/sha}", "comments_url": "https://api.github.com/repos/go-gitea/gitea/comments{/number}", "issue_comment_url": "https://api.github.com/repos/go-gitea/gitea/issues/comments{/number}", "contents_url": "https://api.github.com/repos/go-gitea/gitea/contents/{+path}", "compare_url": "https://api.github.com/repos/go-gitea/gitea/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/go-gitea/gitea/merges", "archive_url": "https://api.github.com/repos/go-gitea/gitea/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/go-gitea/gitea/downloads", "issues_url": "https://api.github.com/repos/go-gitea/gitea/issues{/number}", "pulls_url": "https://api.github.com/repos/go-gitea/gitea/pulls{/number}", "milestones_url": "https://api.github.com/repos/go-gitea/gitea/milestones{/number}", "notifications_url": "https://api.github.com/repos/go-gitea/gitea/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/go-gitea/gitea/labels{/name}", "releases_url": "https://api.github.com/repos/go-gitea/gitea/releases{/id}", "deployments_url": "https://api.github.com/repos/go-gitea/gitea/deployments", "created_at": "2016-11-01T02:13:26Z", "updated_at": "2026-01-07T07:02:30Z", "pushed_at": "2026-01-07T06:08:59Z", "git_url": "git://github.com/go-gitea/gitea.git", "ssh_url": "git@github.com:go-gitea/gitea.git", "clone_url": "https://github.com/go-gitea/gitea.git", "svn_url": "https://github.com/go-gitea/gitea", "homepage": "https://gitea.com", "size": 322636, "stargazers_count": 52973, "watchers_count": 52973, "language": "Go", "has_issues": true, "has_projects": false, "has_downloads": true, "has_wiki": false, "has_pages": false, "has_discussions": false, "forks_count": 6304, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 2885, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": true, "topics": ["bitbucket", "cicd", "devops", "docker-registry-v2", "git", "git-gui", "git-server", "gitea", "github", "github-actions", "gitlab", "go", "golang", "hacktoberfest", "maven-server", "npm-registry", "vue"], "visibility": "public", "forks": 6304, "open_issues": 2885, "watchers": 52973, "default_branch": "main"}}, "commits": [{"sha": "9321c271c0a63567c0001276f94614981709ca2f", "parents": ["d725b78824a6e83bc5f6db3c83f742810241d1ee"], "message": "Refactor Attribute"}, {"sha": "44288608a6a2f58ac2c0e5823a2f4dfe9d19172d", "parents": ["9321c271c0a63567c0001276f94614981709ca2f"], "message": "some improvements"}, {"sha": "419b959dfe0791087a129a2b26af08bc93ad1d44", "parents": ["44288608a6a2f58ac2c0e5823a2f4dfe9d19172d"], "message": "Support run attr check on bare repository if git version >= 2.40"}, {"sha": "ed1601a196605b0f20147cc35696b1c3cb5a92a0", "parents": ["419b959dfe0791087a129a2b26af08bc93ad1d44"], "message": "Fix builg gogit"}, {"sha": "0fc7704a301045c797c7dc716224e938da55c8e3", "parents": ["ed1601a196605b0f20147cc35696b1c3cb5a92a0"], "message": "Fix lint"}, {"sha": "6da7152bc1cd5df8bc1954d60462d5fdaf84e6e1", "parents": ["0fc7704a301045c797c7dc716224e938da55c8e3"], "message": "Fix bug"}, {"sha": "2a3571de281b62645e734b88f5774901397e4886", "parents": ["6da7152bc1cd5df8bc1954d60462d5fdaf84e6e1"], "message": "Fix bug"}, {"sha": "33b1cebdd63c66c0077e306a01dcc5760bf683e1", "parents": ["2a3571de281b62645e734b88f5774901397e4886"], "message": "Fix bug"}, {"sha": "1f473af6e1173de266cd0621252a838194f4531a", "parents": ["33b1cebdd63c66c0077e306a01dcc5760bf683e1"], "message": "Fix bug"}, {"sha": "1c4821239564f13c17f7d8f037ce4baef2c06a5b", "parents": ["1f473af6e1173de266cd0621252a838194f4531a"], "message": "Fix test"}, {"sha": "f98e2c7ff2a51e18efb25e197037a08ef9f63a80", "parents": ["1c4821239564f13c17f7d8f037ce4baef2c06a5b"], "message": "Fix lint"}, {"sha": "bdfb061041f94ed9e94aae35bd53fc2ad27d4a99", "parents": ["f98e2c7ff2a51e18efb25e197037a08ef9f63a80"], "message": "Correct tests repository under git"}, {"sha": "2d8c956ddf71ff816fa98600f33f41ab198397be", "parents": ["bdfb061041f94ed9e94aae35bd53fc2ad27d4a99"], "message": "Add tests"}, {"sha": "d17e7ad7697705ab703f5d6af7721d27decfd4ae", "parents": ["2d8c956ddf71ff816fa98600f33f41ab198397be"], "message": "Fix lint"}, {"sha": "a70ef5f701cb464e14779c133e80fa0211384d15", "parents": ["d17e7ad7697705ab703f5d6af7721d27decfd4ae"], "message": "Add trace code back"}, {"sha": "ed37f3a6600e06d63f42d66a3f86c47ab8bd0acc", "parents": ["a70ef5f701cb464e14779c133e80fa0211384d15"], "message": "Remove unnecessary code"}, {"sha": "9db67492f35f01e3e2267e19d7c441828745b2f5", "parents": ["ed37f3a6600e06d63f42d66a3f86c47ab8bd0acc"], "message": "Some improvements"}, {"sha": "d6f138b42b04d2b4aba22497eb0eafe16aa95650", "parents": ["9db67492f35f01e3e2267e19d7c441828745b2f5"], "message": "fine tune"}, {"sha": "2391070f1ef998fc411534f102513846eaf0d3eb", "parents": ["d6f138b42b04d2b4aba22497eb0eafe16aa95650"], "message": "add comment"}, {"sha": "1a522442d9e42491f4908ab640cd82431f35fca6", "parents": ["9db67492f35f01e3e2267e19d7c441828745b2f5"], "message": "merge two functions call as one"}, {"sha": "ad4af2dc03b5330531dc4412c9c1375f8ea32b5a", "parents": ["1a522442d9e42491f4908ab640cd82431f35fca6", "2391070f1ef998fc411534f102513846eaf0d3eb"], "message": "Merge branch 'lunny/attribute' of github.com:lunny/gitea into lunny/attribute"}, {"sha": "1d333296eb7c809eecf55b1c42dce586a044a55b", "parents": ["ad4af2dc03b5330531dc4412c9c1375f8ea32b5a"], "message": "don't make Attributes expose internal map"}, {"sha": "a0cfb36717ccd165e435d2d182152bc62094a89d", "parents": ["1d333296eb7c809eecf55b1c42dce586a044a55b"], "message": "improve test and comment"}, {"sha": "fca3bcf0d9a6adc37d553530bea8a3b0b278b5bf", "parents": ["a0cfb36717ccd165e435d2d182152bc62094a89d"], "message": "Update routers/web/repo/view_file.go\n\nCo-authored-by: yp05327 <576951401@qq.com>"}], "resolved_issues": [{"org": "go-gitea", "repo": "gitea", "number": -1, "state": "unknown", "title": "Refactor Git Attribute & performance optimization", "body": "This PR moved git attributes related code to `modules/git/attribute` sub package and moved language stats related code to `modules/git/languagestats` sub package to make it easier to maintain.\r\n\r\nAnd it also introduced a performance improvement which use the `git check-attr --source` which can be run in a bare git repository so that we don't need to create a git index file. The new parameter need a git version >= 2.40 . If git version less than 2.40, it will fall back to previous implementation.\r\n\r\nBefore:\r\n```\r\ngit-run duration=1.2684s func.caller=git.(*Repository).readTreeToIndex git.command=\"/opt/homebrew/bin/git ...global... read-tree 70187f7727d4ddd8282b576ece93ca233e88b19e\"\r\ngit-run duration=0.1633s func.caller=git.(*CheckAttributeReader).Run git.command=\"/opt/homebrew/bin/git ...global... check-attr --stdin -z --cached linguist-vendored linguist-generated linguist-documentation linguist-detectable linguist-language gitlab-language\"\r\n```\r\n\r\nAfter:\r\n```\r\ngit-run duration=0.1271s func.caller=attribute.(*BatchChecker).run git.command=\"/opt/homebrew/bin/git ...global... check-attr -z --source 70187f7727d4ddd8282b576ece93ca233e88b19e linguist-vendored linguist-generated linguist-language gitlab-language --stdin\"\r\n```"}], "fix_patch": "diff --git a/modules/git/attribute.go b/modules/git/attribute.go\ndeleted file mode 100644\nindex 4dfa510369086..0000000000000\n--- a/modules/git/attribute.go\n+++ /dev/null\n@@ -1,35 +0,0 @@\n-// Copyright 2024 The Gitea Authors. All rights reserved.\n-// SPDX-License-Identifier: MIT\n-\n-package git\n-\n-import (\n-\t\"code.gitea.io/gitea/modules/optional\"\n-)\n-\n-const (\n-\tAttributeLinguistVendored      = \"linguist-vendored\"\n-\tAttributeLinguistGenerated     = \"linguist-generated\"\n-\tAttributeLinguistDocumentation = \"linguist-documentation\"\n-\tAttributeLinguistDetectable    = \"linguist-detectable\"\n-\tAttributeLinguistLanguage      = \"linguist-language\"\n-\tAttributeGitlabLanguage        = \"gitlab-language\"\n-)\n-\n-// true if \"set\"/\"true\", false if \"unset\"/\"false\", none otherwise\n-func AttributeToBool(attr map[string]string, name string) optional.Option[bool] {\n-\tswitch attr[name] {\n-\tcase \"set\", \"true\":\n-\t\treturn optional.Some(true)\n-\tcase \"unset\", \"false\":\n-\t\treturn optional.Some(false)\n-\t}\n-\treturn optional.None[bool]()\n-}\n-\n-func AttributeToString(attr map[string]string, name string) optional.Option[string] {\n-\tif value, has := attr[name]; has && value != \"unspecified\" {\n-\t\treturn optional.Some(value)\n-\t}\n-\treturn optional.None[string]()\n-}\ndiff --git a/modules/git/attribute/attribute.go b/modules/git/attribute/attribute.go\nnew file mode 100644\nindex 0000000000000..adf323ef41c05\n--- /dev/null\n+++ b/modules/git/attribute/attribute.go\n@@ -0,0 +1,114 @@\n+// Copyright 2025 The Gitea Authors. All rights reserved.\n+// SPDX-License-Identifier: MIT\n+\n+package attribute\n+\n+import (\n+\t\"strings\"\n+\n+\t\"code.gitea.io/gitea/modules/optional\"\n+)\n+\n+type Attribute string\n+\n+const (\n+\tLinguistVendored      = \"linguist-vendored\"\n+\tLinguistGenerated     = \"linguist-generated\"\n+\tLinguistDocumentation = \"linguist-documentation\"\n+\tLinguistDetectable    = \"linguist-detectable\"\n+\tLinguistLanguage      = \"linguist-language\"\n+\tGitlabLanguage        = \"gitlab-language\"\n+\tLockable              = \"lockable\"\n+\tFilter                = \"filter\"\n+)\n+\n+var LinguistAttributes = []string{\n+\tLinguistVendored,\n+\tLinguistGenerated,\n+\tLinguistDocumentation,\n+\tLinguistDetectable,\n+\tLinguistLanguage,\n+\tGitlabLanguage,\n+}\n+\n+func (a Attribute) IsUnspecified() bool {\n+\treturn a == \"\" || a == \"unspecified\"\n+}\n+\n+func (a Attribute) ToString() optional.Option[string] {\n+\tif !a.IsUnspecified() {\n+\t\treturn optional.Some(string(a))\n+\t}\n+\treturn optional.None[string]()\n+}\n+\n+// ToBool converts the attribute value to optional boolean: true if \"set\"/\"true\", false if \"unset\"/\"false\", none otherwise\n+func (a Attribute) ToBool() optional.Option[bool] {\n+\tswitch a {\n+\tcase \"set\", \"true\":\n+\t\treturn optional.Some(true)\n+\tcase \"unset\", \"false\":\n+\t\treturn optional.Some(false)\n+\t}\n+\treturn optional.None[bool]()\n+}\n+\n+type Attributes struct {\n+\tm map[string]Attribute\n+}\n+\n+func NewAttributes() *Attributes {\n+\treturn &Attributes{m: make(map[string]Attribute)}\n+}\n+\n+func (attrs *Attributes) Get(name string) Attribute {\n+\tif value, has := attrs.m[name]; has {\n+\t\treturn value\n+\t}\n+\treturn \"\"\n+}\n+\n+func (attrs *Attributes) GetVendored() optional.Option[bool] {\n+\treturn attrs.Get(LinguistVendored).ToBool()\n+}\n+\n+func (attrs *Attributes) GetGenerated() optional.Option[bool] {\n+\treturn attrs.Get(LinguistGenerated).ToBool()\n+}\n+\n+func (attrs *Attributes) GetDocumentation() optional.Option[bool] {\n+\treturn attrs.Get(LinguistDocumentation).ToBool()\n+}\n+\n+func (attrs *Attributes) GetDetectable() optional.Option[bool] {\n+\treturn attrs.Get(LinguistDetectable).ToBool()\n+}\n+\n+func (attrs *Attributes) GetLinguistLanguage() optional.Option[string] {\n+\treturn attrs.Get(LinguistLanguage).ToString()\n+}\n+\n+func (attrs *Attributes) GetGitlabLanguage() optional.Option[string] {\n+\tattrStr := attrs.Get(GitlabLanguage).ToString()\n+\tif attrStr.Has() {\n+\t\traw := attrStr.Value()\n+\t\t// gitlab-language may have additional parameters after the language\n+\t\t// ignore them and just use the main language\n+\t\t// https://docs.gitlab.com/ee/user/project/highlighting.html#override-syntax-highlighting-for-a-file-type\n+\t\tif idx := strings.IndexByte(raw, '?'); idx >= 0 {\n+\t\t\treturn optional.Some(raw[:idx])\n+\t\t}\n+\t}\n+\treturn attrStr\n+}\n+\n+func (attrs *Attributes) GetLanguage() optional.Option[string] {\n+\t// prefer linguist-language over gitlab-language\n+\t// if linguist-language is not set, use gitlab-language\n+\t// if both are not set, return none\n+\tlanguage := attrs.GetLinguistLanguage()\n+\tif language.Value() == \"\" {\n+\t\tlanguage = attrs.GetGitlabLanguage()\n+\t}\n+\treturn language\n+}\ndiff --git a/modules/git/attribute/batch.go b/modules/git/attribute/batch.go\nnew file mode 100644\nindex 0000000000000..4e31fda5753cd\n--- /dev/null\n+++ b/modules/git/attribute/batch.go\n@@ -0,0 +1,216 @@\n+// Copyright 2019 The Gitea Authors. All rights reserved.\n+// SPDX-License-Identifier: MIT\n+\n+package attribute\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\t\"time\"\n+\n+\t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/log\"\n+)\n+\n+// BatchChecker provides a reader for check-attribute content that can be long running\n+type BatchChecker struct {\n+\tattributesNum int\n+\trepo          *git.Repository\n+\tstdinWriter   *os.File\n+\tstdOut        *nulSeparatedAttributeWriter\n+\tctx           context.Context\n+\tcancel        context.CancelFunc\n+\tcmd           *git.Command\n+}\n+\n+// NewBatchChecker creates a check attribute reader for the current repository and provided commit ID\n+// If treeish is empty, then it will use current working directory, otherwise it will use the provided treeish on the bare repo\n+func NewBatchChecker(repo *git.Repository, treeish string, attributes []string) (checker *BatchChecker, returnedErr error) {\n+\tctx, cancel := context.WithCancel(repo.Ctx)\n+\tdefer func() {\n+\t\tif returnedErr != nil {\n+\t\t\tcancel()\n+\t\t}\n+\t}()\n+\n+\tcmd, envs, cleanup, err := checkAttrCommand(repo, treeish, nil, attributes)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdefer func() {\n+\t\tif returnedErr != nil {\n+\t\t\tcleanup()\n+\t\t}\n+\t}()\n+\n+\tcmd.AddArguments(\"--stdin\")\n+\n+\tchecker = &BatchChecker{\n+\t\tattributesNum: len(attributes),\n+\t\trepo:          repo,\n+\t\tctx:           ctx,\n+\t\tcmd:           cmd,\n+\t\tcancel: func() {\n+\t\t\tcancel()\n+\t\t\tcleanup()\n+\t\t},\n+\t}\n+\n+\tstdinReader, stdinWriter, err := os.Pipe()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tchecker.stdinWriter = stdinWriter\n+\n+\tlw := new(nulSeparatedAttributeWriter)\n+\tlw.attributes = make(chan attributeTriple, len(attributes))\n+\tlw.closed = make(chan struct{})\n+\tchecker.stdOut = lw\n+\n+\tgo func() {\n+\t\tdefer func() {\n+\t\t\t_ = stdinReader.Close()\n+\t\t\t_ = lw.Close()\n+\t\t}()\n+\t\tstdErr := new(bytes.Buffer)\n+\t\terr := cmd.Run(ctx, &git.RunOpts{\n+\t\t\tEnv:    envs,\n+\t\t\tDir:    repo.Path,\n+\t\t\tStdin:  stdinReader,\n+\t\t\tStdout: lw,\n+\t\t\tStderr: stdErr,\n+\t\t})\n+\n+\t\tif err != nil && !git.IsErrCanceledOrKilled(err) {\n+\t\t\tlog.Error(\"Attribute checker for commit %s exits with error: %v\", treeish, err)\n+\t\t}\n+\t\tchecker.cancel()\n+\t}()\n+\n+\treturn checker, nil\n+}\n+\n+// CheckPath check attr for given path\n+func (c *BatchChecker) CheckPath(path string) (rs *Attributes, err error) {\n+\tdefer func() {\n+\t\tif err != nil && err != c.ctx.Err() {\n+\t\t\tlog.Error(\"Unexpected error when checking path %s in %s, error: %v\", path, filepath.Base(c.repo.Path), err)\n+\t\t}\n+\t}()\n+\n+\tselect {\n+\tcase <-c.ctx.Done():\n+\t\treturn nil, c.ctx.Err()\n+\tdefault:\n+\t}\n+\n+\tif _, err = c.stdinWriter.Write([]byte(path + \"\\x00\")); err != nil {\n+\t\tdefer c.Close()\n+\t\treturn nil, err\n+\t}\n+\n+\treportTimeout := func() error {\n+\t\tstdOutClosed := false\n+\t\tselect {\n+\t\tcase <-c.stdOut.closed:\n+\t\t\tstdOutClosed = true\n+\t\tdefault:\n+\t\t}\n+\t\tdebugMsg := fmt.Sprintf(\"check path %q in repo %q\", path, filepath.Base(c.repo.Path))\n+\t\tdebugMsg += fmt.Sprintf(\", stdOut: tmp=%q, pos=%d, closed=%v\", string(c.stdOut.tmp), c.stdOut.pos, stdOutClosed)\n+\t\tif c.cmd != nil {\n+\t\t\tdebugMsg += fmt.Sprintf(\", process state: %q\", c.cmd.ProcessState())\n+\t\t}\n+\t\t_ = c.Close()\n+\t\treturn fmt.Errorf(\"CheckPath timeout: %s\", debugMsg)\n+\t}\n+\n+\trs = NewAttributes()\n+\tfor i := 0; i < c.attributesNum; i++ {\n+\t\tselect {\n+\t\tcase <-time.After(5 * time.Second):\n+\t\t\t// there is no \"hang\" problem now. This code is just used to catch other potential problems.\n+\t\t\treturn nil, reportTimeout()\n+\t\tcase attr, ok := <-c.stdOut.ReadAttribute():\n+\t\t\tif !ok {\n+\t\t\t\treturn nil, c.ctx.Err()\n+\t\t\t}\n+\t\t\trs.m[attr.Attribute] = Attribute(attr.Value)\n+\t\tcase <-c.ctx.Done():\n+\t\t\treturn nil, c.ctx.Err()\n+\t\t}\n+\t}\n+\treturn rs, nil\n+}\n+\n+func (c *BatchChecker) Close() error {\n+\tc.cancel()\n+\terr := c.stdinWriter.Close()\n+\treturn err\n+}\n+\n+type attributeTriple struct {\n+\tFilename  string\n+\tAttribute string\n+\tValue     string\n+}\n+\n+type nulSeparatedAttributeWriter struct {\n+\ttmp        []byte\n+\tattributes chan attributeTriple\n+\tclosed     chan struct{}\n+\tworking    attributeTriple\n+\tpos        int\n+}\n+\n+func (wr *nulSeparatedAttributeWriter) Write(p []byte) (n int, err error) {\n+\tl, read := len(p), 0\n+\n+\tnulIdx := bytes.IndexByte(p, '\\x00')\n+\tfor nulIdx >= 0 {\n+\t\twr.tmp = append(wr.tmp, p[:nulIdx]...)\n+\t\tswitch wr.pos {\n+\t\tcase 0:\n+\t\t\twr.working = attributeTriple{\n+\t\t\t\tFilename: string(wr.tmp),\n+\t\t\t}\n+\t\tcase 1:\n+\t\t\twr.working.Attribute = string(wr.tmp)\n+\t\tcase 2:\n+\t\t\twr.working.Value = string(wr.tmp)\n+\t\t}\n+\t\twr.tmp = wr.tmp[:0]\n+\t\twr.pos++\n+\t\tif wr.pos > 2 {\n+\t\t\twr.attributes <- wr.working\n+\t\t\twr.pos = 0\n+\t\t}\n+\t\tread += nulIdx + 1\n+\t\tif l > read {\n+\t\t\tp = p[nulIdx+1:]\n+\t\t\tnulIdx = bytes.IndexByte(p, '\\x00')\n+\t\t} else {\n+\t\t\treturn l, nil\n+\t\t}\n+\t}\n+\twr.tmp = append(wr.tmp, p...)\n+\treturn l, nil\n+}\n+\n+func (wr *nulSeparatedAttributeWriter) ReadAttribute() <-chan attributeTriple {\n+\treturn wr.attributes\n+}\n+\n+func (wr *nulSeparatedAttributeWriter) Close() error {\n+\tselect {\n+\tcase <-wr.closed:\n+\t\treturn nil\n+\tdefault:\n+\t}\n+\tclose(wr.attributes)\n+\tclose(wr.closed)\n+\treturn nil\n+}\ndiff --git a/modules/git/attribute/checker.go b/modules/git/attribute/checker.go\nnew file mode 100644\nindex 0000000000000..c17006a15407b\n--- /dev/null\n+++ b/modules/git/attribute/checker.go\n@@ -0,0 +1,96 @@\n+// Copyright 2025 The Gitea Authors. All rights reserved.\n+// SPDX-License-Identifier: MIT\n+\n+package attribute\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"os\"\n+\n+\t\"code.gitea.io/gitea/modules/git\"\n+)\n+\n+func checkAttrCommand(gitRepo *git.Repository, treeish string, filenames, attributes []string) (*git.Command, []string, func(), error) {\n+\tcancel := func() {}\n+\tenvs := []string{\"GIT_FLUSH=1\"}\n+\tcmd := git.NewCommand(\"check-attr\", \"-z\")\n+\tif len(attributes) == 0 {\n+\t\tcmd.AddArguments(\"--all\")\n+\t}\n+\n+\t// there is treeish, read from bare repo or temp index created by \"read-tree\"\n+\tif treeish != \"\" {\n+\t\tif git.DefaultFeatures().SupportCheckAttrOnBare {\n+\t\t\tcmd.AddArguments(\"--source\")\n+\t\t\tcmd.AddDynamicArguments(treeish)\n+\t\t} else {\n+\t\t\tindexFilename, worktree, deleteTemporaryFile, err := gitRepo.ReadTreeToTemporaryIndex(treeish)\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, nil, nil, err\n+\t\t\t}\n+\n+\t\t\tcmd.AddArguments(\"--cached\")\n+\t\t\tenvs = append(envs,\n+\t\t\t\t\"GIT_INDEX_FILE=\"+indexFilename,\n+\t\t\t\t\"GIT_WORK_TREE=\"+worktree,\n+\t\t\t)\n+\t\t\tcancel = deleteTemporaryFile\n+\t\t}\n+\t} // else: no treeish, assume it is a not a bare repo, read from working directory\n+\n+\tcmd.AddDynamicArguments(attributes...)\n+\tif len(filenames) > 0 {\n+\t\tcmd.AddDashesAndList(filenames...)\n+\t}\n+\treturn cmd, envs, cancel, nil\n+}\n+\n+type CheckAttributeOpts struct {\n+\tFilenames  []string\n+\tAttributes []string\n+}\n+\n+// CheckAttributes return the attributes of the given filenames and attributes in the given treeish.\n+// If treeish is empty, then it will use current working directory, otherwise it will use the provided treeish on the bare repo\n+func CheckAttributes(ctx context.Context, gitRepo *git.Repository, treeish string, opts CheckAttributeOpts) (map[string]*Attributes, error) {\n+\tcmd, envs, cancel, err := checkAttrCommand(gitRepo, treeish, opts.Filenames, opts.Attributes)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdefer cancel()\n+\n+\tstdOut := new(bytes.Buffer)\n+\tstdErr := new(bytes.Buffer)\n+\n+\tif err := cmd.Run(ctx, &git.RunOpts{\n+\t\tEnv:    append(os.Environ(), envs...),\n+\t\tDir:    gitRepo.Path,\n+\t\tStdout: stdOut,\n+\t\tStderr: stdErr,\n+\t}); err != nil {\n+\t\treturn nil, fmt.Errorf(\"failed to run check-attr: %w\\n%s\\n%s\", err, stdOut.String(), stdErr.String())\n+\t}\n+\n+\tfields := bytes.Split(stdOut.Bytes(), []byte{'\\000'})\n+\tif len(fields)%3 != 1 {\n+\t\treturn nil, errors.New(\"wrong number of fields in return from check-attr\")\n+\t}\n+\n+\tattributesMap := make(map[string]*Attributes)\n+\tfor i := 0; i < (len(fields) / 3); i++ {\n+\t\tfilename := string(fields[3*i])\n+\t\tattribute := string(fields[3*i+1])\n+\t\tinfo := string(fields[3*i+2])\n+\t\tattribute2info, ok := attributesMap[filename]\n+\t\tif !ok {\n+\t\t\tattribute2info = NewAttributes()\n+\t\t\tattributesMap[filename] = attribute2info\n+\t\t}\n+\t\tattribute2info.m[attribute] = Attribute(info)\n+\t}\n+\n+\treturn attributesMap, nil\n+}\ndiff --git a/modules/git/command.go b/modules/git/command.go\nindex f449f1ff0e6a8..eaaa4969d0bb1 100644\n--- a/modules/git/command.go\n+++ b/modules/git/command.go\n@@ -80,6 +80,13 @@ func (c *Command) LogString() string {\n \treturn strings.Join(a, \" \")\n }\n \n+func (c *Command) ProcessState() string {\n+\tif c.cmd == nil {\n+\t\treturn \"\"\n+\t}\n+\treturn c.cmd.ProcessState.String()\n+}\n+\n // NewCommand creates and returns a new Git Command based on given command and arguments.\n // Each argument should be safe to be trusted. User-provided arguments should be passed to AddDynamicArguments instead.\n func NewCommand(args ...internal.CmdArg) *Command {\ndiff --git a/modules/git/git.go b/modules/git/git.go\nindex 2b593910a2dc0..a2ffd6d289880 100644\n--- a/modules/git/git.go\n+++ b/modules/git/git.go\n@@ -30,6 +30,7 @@ type Features struct {\n \tSupportProcReceive     bool           // >= 2.29\n \tSupportHashSha256      bool           // >= 2.42, SHA-256 repositories no longer an â€˜experimental curiosityâ€™\n \tSupportedObjectFormats []ObjectFormat // sha1, sha256\n+\tSupportCheckAttrOnBare bool           // >= 2.40\n }\n \n var (\n@@ -77,6 +78,7 @@ func loadGitVersionFeatures() (*Features, error) {\n \tif features.SupportHashSha256 {\n \t\tfeatures.SupportedObjectFormats = append(features.SupportedObjectFormats, Sha256ObjectFormat)\n \t}\n+\tfeatures.SupportCheckAttrOnBare = features.CheckVersionAtLeast(\"2.40\")\n \treturn features, nil\n }\n \ndiff --git a/modules/git/repo_language_stats.go b/modules/git/languagestats/language_stats.go\nsimilarity index 59%\nrename from modules/git/repo_language_stats.go\nrename to modules/git/languagestats/language_stats.go\nindex 8551ea9d24e8b..a71284c3e446f 100644\n--- a/modules/git/repo_language_stats.go\n+++ b/modules/git/languagestats/language_stats.go\n@@ -1,13 +1,15 @@\n // Copyright 2020 The Gitea Authors. All rights reserved.\n // SPDX-License-Identifier: MIT\n \n-package git\n+package languagestats\n \n import (\n+\t\"context\"\n \t\"strings\"\n \t\"unicode\"\n \n-\t\"code.gitea.io/gitea/modules/optional\"\n+\t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n )\n \n const (\n@@ -49,19 +51,15 @@ func mergeLanguageStats(stats map[string]int64) map[string]int64 {\n \treturn res\n }\n \n-func TryReadLanguageAttribute(attrs map[string]string) optional.Option[string] {\n-\tlanguage := AttributeToString(attrs, AttributeLinguistLanguage)\n-\tif language.Value() == \"\" {\n-\t\tlanguage = AttributeToString(attrs, AttributeGitlabLanguage)\n-\t\tif language.Has() {\n-\t\t\traw := language.Value()\n-\t\t\t// gitlab-language may have additional parameters after the language\n-\t\t\t// ignore them and just use the main language\n-\t\t\t// https://docs.gitlab.com/ee/user/project/highlighting.html#override-syntax-highlighting-for-a-file-type\n-\t\t\tif idx := strings.IndexByte(raw, '?'); idx >= 0 {\n-\t\t\t\tlanguage = optional.Some(raw[:idx])\n-\t\t\t}\n-\t\t}\n+// GetFileLanguage tries to get the (linguist) language of the file content\n+func GetFileLanguage(ctx context.Context, gitRepo *git.Repository, treeish, treePath string) (string, error) {\n+\tattributesMap, err := attribute.CheckAttributes(ctx, gitRepo, treeish, attribute.CheckAttributeOpts{\n+\t\tAttributes: []string{attribute.LinguistLanguage, attribute.GitlabLanguage},\n+\t\tFilenames:  []string{treePath},\n+\t})\n+\tif err != nil {\n+\t\treturn \"\", err\n \t}\n-\treturn language\n+\n+\treturn attributesMap[treePath].GetLanguage().Value(), nil\n }\ndiff --git a/modules/git/repo_language_stats_gogit.go b/modules/git/languagestats/language_stats_gogit.go\nsimilarity index 73%\nrename from modules/git/repo_language_stats_gogit.go\nrename to modules/git/languagestats/language_stats_gogit.go\nindex a34c03c781f55..418c05b15789f 100644\n--- a/modules/git/repo_language_stats_gogit.go\n+++ b/modules/git/languagestats/language_stats_gogit.go\n@@ -3,13 +3,15 @@\n \n //go:build gogit\n \n-package git\n+package languagestats\n \n import (\n \t\"bytes\"\n \t\"io\"\n \n \t\"code.gitea.io/gitea/modules/analyze\"\n+\tgit_module \"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n \t\"code.gitea.io/gitea/modules/optional\"\n \n \t\"github.com/go-enry/go-enry/v2\"\n@@ -19,7 +21,7 @@ import (\n )\n \n // GetLanguageStats calculates language stats for git repository at specified commit\n-func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, error) {\n+func GetLanguageStats(repo *git_module.Repository, commitID string) (map[string]int64, error) {\n \tr, err := git.PlainOpen(repo.Path)\n \tif err != nil {\n \t\treturn nil, err\n@@ -40,8 +42,11 @@ func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, err\n \t\treturn nil, err\n \t}\n \n-\tchecker, deferable := repo.CheckAttributeReader(commitID)\n-\tdefer deferable()\n+\tchecker, err := attribute.NewBatchChecker(repo, commitID, attribute.LinguistAttributes)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdefer checker.Close()\n \n \t// sizes contains the current calculated size of all files by language\n \tsizes := make(map[string]int64)\n@@ -62,43 +67,41 @@ func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, err\n \t\tisDocumentation := optional.None[bool]()\n \t\tisDetectable := optional.None[bool]()\n \n-\t\tif checker != nil {\n-\t\t\tattrs, err := checker.CheckPath(f.Name)\n-\t\t\tif err == nil {\n-\t\t\t\tisVendored = AttributeToBool(attrs, AttributeLinguistVendored)\n-\t\t\t\tif isVendored.ValueOrDefault(false) {\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n-\n-\t\t\t\tisGenerated = AttributeToBool(attrs, AttributeLinguistGenerated)\n-\t\t\t\tif isGenerated.ValueOrDefault(false) {\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n+\t\tattrs, err := checker.CheckPath(f.Name)\n+\t\tif err == nil {\n+\t\t\tisVendored = attrs.GetVendored()\n+\t\t\tif isVendored.ValueOrDefault(false) {\n+\t\t\t\treturn nil\n+\t\t\t}\n \n-\t\t\t\tisDocumentation = AttributeToBool(attrs, AttributeLinguistDocumentation)\n-\t\t\t\tif isDocumentation.ValueOrDefault(false) {\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n+\t\t\tisGenerated = attrs.GetGenerated()\n+\t\t\tif isGenerated.ValueOrDefault(false) {\n+\t\t\t\treturn nil\n+\t\t\t}\n \n-\t\t\t\tisDetectable = AttributeToBool(attrs, AttributeLinguistDetectable)\n-\t\t\t\tif !isDetectable.ValueOrDefault(true) {\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n+\t\t\tisDocumentation = attrs.GetDocumentation()\n+\t\t\tif isDocumentation.ValueOrDefault(false) {\n+\t\t\t\treturn nil\n+\t\t\t}\n \n-\t\t\t\thasLanguage := TryReadLanguageAttribute(attrs)\n-\t\t\t\tif hasLanguage.Value() != \"\" {\n-\t\t\t\t\tlanguage := hasLanguage.Value()\n+\t\t\tisDetectable = attrs.GetDetectable()\n+\t\t\tif !isDetectable.ValueOrDefault(true) {\n+\t\t\t\treturn nil\n+\t\t\t}\n \n-\t\t\t\t\t// group languages, such as Pug -> HTML; SCSS -> CSS\n-\t\t\t\t\tgroup := enry.GetLanguageGroup(language)\n-\t\t\t\t\tif len(group) != 0 {\n-\t\t\t\t\t\tlanguage = group\n-\t\t\t\t\t}\n+\t\t\thasLanguage := attrs.GetLanguage()\n+\t\t\tif hasLanguage.Value() != \"\" {\n+\t\t\t\tlanguage := hasLanguage.Value()\n \n-\t\t\t\t\t// this language will always be added to the size\n-\t\t\t\t\tsizes[language] += f.Size\n-\t\t\t\t\treturn nil\n+\t\t\t\t// group languages, such as Pug -> HTML; SCSS -> CSS\n+\t\t\t\tgroup := enry.GetLanguageGroup(language)\n+\t\t\t\tif len(group) != 0 {\n+\t\t\t\t\tlanguage = group\n \t\t\t\t}\n+\n+\t\t\t\t// this language will always be added to the size\n+\t\t\t\tsizes[language] += f.Size\n+\t\t\t\treturn nil\n \t\t\t}\n \t\t}\n \ndiff --git a/modules/git/repo_language_stats_nogogit.go b/modules/git/languagestats/language_stats_nogogit.go\nsimilarity index 73%\nrename from modules/git/repo_language_stats_nogogit.go\nrename to modules/git/languagestats/language_stats_nogogit.go\nindex de7707bd6cd8b..34797263a603a 100644\n--- a/modules/git/repo_language_stats_nogogit.go\n+++ b/modules/git/languagestats/language_stats_nogogit.go\n@@ -3,13 +3,15 @@\n \n //go:build !gogit\n \n-package git\n+package languagestats\n \n import (\n \t\"bytes\"\n \t\"io\"\n \n \t\"code.gitea.io/gitea/modules/analyze\"\n+\t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n \t\"code.gitea.io/gitea/modules/log\"\n \t\"code.gitea.io/gitea/modules/optional\"\n \n@@ -17,7 +19,7 @@ import (\n )\n \n // GetLanguageStats calculates language stats for git repository at specified commit\n-func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, error) {\n+func GetLanguageStats(repo *git.Repository, commitID string) (map[string]int64, error) {\n \t// We will feed the commit IDs in order into cat-file --batch, followed by blobs as necessary.\n \t// so let's create a batch stdin and stdout\n \tbatchStdinWriter, batchReader, cancel, err := repo.CatFileBatch(repo.Ctx)\n@@ -34,19 +36,19 @@ func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, err\n \tif err := writeID(commitID); err != nil {\n \t\treturn nil, err\n \t}\n-\tshaBytes, typ, size, err := ReadBatchLine(batchReader)\n+\tshaBytes, typ, size, err := git.ReadBatchLine(batchReader)\n \tif typ != \"commit\" {\n \t\tlog.Debug(\"Unable to get commit for: %s. Err: %v\", commitID, err)\n-\t\treturn nil, ErrNotExist{commitID, \"\"}\n+\t\treturn nil, git.ErrNotExist{ID: commitID}\n \t}\n \n-\tsha, err := NewIDFromString(string(shaBytes))\n+\tsha, err := git.NewIDFromString(string(shaBytes))\n \tif err != nil {\n \t\tlog.Debug(\"Unable to get commit for: %s. Err: %v\", commitID, err)\n-\t\treturn nil, ErrNotExist{commitID, \"\"}\n+\t\treturn nil, git.ErrNotExist{ID: commitID}\n \t}\n \n-\tcommit, err := CommitFromReader(repo, sha, io.LimitReader(batchReader, size))\n+\tcommit, err := git.CommitFromReader(repo, sha, io.LimitReader(batchReader, size))\n \tif err != nil {\n \t\tlog.Debug(\"Unable to get commit for: %s. Err: %v\", commitID, err)\n \t\treturn nil, err\n@@ -62,8 +64,11 @@ func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, err\n \t\treturn nil, err\n \t}\n \n-\tchecker, deferable := repo.CheckAttributeReader(commitID)\n-\tdefer deferable()\n+\tchecker, err := attribute.NewBatchChecker(repo, commitID, attribute.LinguistAttributes)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdefer checker.Close()\n \n \tcontentBuf := bytes.Buffer{}\n \tvar content []byte\n@@ -96,43 +101,36 @@ func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, err\n \t\tisDocumentation := optional.None[bool]()\n \t\tisDetectable := optional.None[bool]()\n \n-\t\tif checker != nil {\n-\t\t\tattrs, err := checker.CheckPath(f.Name())\n-\t\t\tif err == nil {\n-\t\t\t\tisVendored = AttributeToBool(attrs, AttributeLinguistVendored)\n-\t\t\t\tif isVendored.ValueOrDefault(false) {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\n-\t\t\t\tisGenerated = AttributeToBool(attrs, AttributeLinguistGenerated)\n-\t\t\t\tif isGenerated.ValueOrDefault(false) {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n+\t\tattrs, err := checker.CheckPath(f.Name())\n+\t\tif err == nil {\n+\t\t\tif isVendored = attrs.GetVendored(); isVendored.ValueOrDefault(false) {\n+\t\t\t\tcontinue\n+\t\t\t}\n \n-\t\t\t\tisDocumentation = AttributeToBool(attrs, AttributeLinguistDocumentation)\n-\t\t\t\tif isDocumentation.ValueOrDefault(false) {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n+\t\t\tif isGenerated = attrs.GetGenerated(); isGenerated.ValueOrDefault(false) {\n+\t\t\t\tcontinue\n+\t\t\t}\n \n-\t\t\t\tisDetectable = AttributeToBool(attrs, AttributeLinguistDetectable)\n-\t\t\t\tif !isDetectable.ValueOrDefault(true) {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n+\t\t\tif isDocumentation = attrs.GetDocumentation(); isDocumentation.ValueOrDefault(false) {\n+\t\t\t\tcontinue\n+\t\t\t}\n \n-\t\t\t\thasLanguage := TryReadLanguageAttribute(attrs)\n-\t\t\t\tif hasLanguage.Value() != \"\" {\n-\t\t\t\t\tlanguage := hasLanguage.Value()\n+\t\t\tif isDetectable = attrs.GetDetectable(); !isDetectable.ValueOrDefault(true) {\n+\t\t\t\tcontinue\n+\t\t\t}\n \n-\t\t\t\t\t// group languages, such as Pug -> HTML; SCSS -> CSS\n-\t\t\t\t\tgroup := enry.GetLanguageGroup(language)\n-\t\t\t\t\tif len(group) != 0 {\n-\t\t\t\t\t\tlanguage = group\n-\t\t\t\t\t}\n+\t\t\tif hasLanguage := attrs.GetLanguage(); hasLanguage.Value() != \"\" {\n+\t\t\t\tlanguage := hasLanguage.Value()\n \n-\t\t\t\t\t// this language will always be added to the size\n-\t\t\t\t\tsizes[language] += f.Size()\n-\t\t\t\t\tcontinue\n+\t\t\t\t// group languages, such as Pug -> HTML; SCSS -> CSS\n+\t\t\t\tgroup := enry.GetLanguageGroup(language)\n+\t\t\t\tif len(group) != 0 {\n+\t\t\t\t\tlanguage = group\n \t\t\t\t}\n+\n+\t\t\t\t// this language will always be added to the size\n+\t\t\t\tsizes[language] += f.Size()\n+\t\t\t\tcontinue\n \t\t\t}\n \t\t}\n \n@@ -149,7 +147,7 @@ func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, err\n \t\t\tif err := writeID(f.ID.String()); err != nil {\n \t\t\t\treturn nil, err\n \t\t\t}\n-\t\t\t_, _, size, err := ReadBatchLine(batchReader)\n+\t\t\t_, _, size, err := git.ReadBatchLine(batchReader)\n \t\t\tif err != nil {\n \t\t\t\tlog.Debug(\"Error reading blob: %s Err: %v\", f.ID.String(), err)\n \t\t\t\treturn nil, err\n@@ -167,7 +165,7 @@ func (repo *Repository) GetLanguageStats(commitID string) (map[string]int64, err\n \t\t\t\treturn nil, err\n \t\t\t}\n \t\t\tcontent = contentBuf.Bytes()\n-\t\t\tif err := DiscardFull(batchReader, discard); err != nil {\n+\t\t\tif err := git.DiscardFull(batchReader, discard); err != nil {\n \t\t\t\treturn nil, err\n \t\t\t}\n \t\t}\ndiff --git a/modules/git/repo_attribute.go b/modules/git/repo_attribute.go\ndeleted file mode 100644\nindex fde42d4730c1d..0000000000000\n--- a/modules/git/repo_attribute.go\n+++ /dev/null\n@@ -1,341 +0,0 @@\n-// Copyright 2019 The Gitea Authors. All rights reserved.\n-// SPDX-License-Identifier: MIT\n-\n-package git\n-\n-import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"time\"\n-\n-\t\"code.gitea.io/gitea/modules/log\"\n-)\n-\n-// CheckAttributeOpts represents the possible options to CheckAttribute\n-type CheckAttributeOpts struct {\n-\tCachedOnly    bool\n-\tAllAttributes bool\n-\tAttributes    []string\n-\tFilenames     []string\n-\tIndexFile     string\n-\tWorkTree      string\n-}\n-\n-// CheckAttribute return the Blame object of file\n-func (repo *Repository) CheckAttribute(opts CheckAttributeOpts) (map[string]map[string]string, error) {\n-\tenv := []string{}\n-\n-\tif len(opts.IndexFile) > 0 {\n-\t\tenv = append(env, \"GIT_INDEX_FILE=\"+opts.IndexFile)\n-\t}\n-\tif len(opts.WorkTree) > 0 {\n-\t\tenv = append(env, \"GIT_WORK_TREE=\"+opts.WorkTree)\n-\t}\n-\n-\tif len(env) > 0 {\n-\t\tenv = append(os.Environ(), env...)\n-\t}\n-\n-\tstdOut := new(bytes.Buffer)\n-\tstdErr := new(bytes.Buffer)\n-\n-\tcmd := NewCommand(\"check-attr\", \"-z\")\n-\n-\tif opts.AllAttributes {\n-\t\tcmd.AddArguments(\"-a\")\n-\t} else {\n-\t\tfor _, attribute := range opts.Attributes {\n-\t\t\tif attribute != \"\" {\n-\t\t\t\tcmd.AddDynamicArguments(attribute)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif opts.CachedOnly {\n-\t\tcmd.AddArguments(\"--cached\")\n-\t}\n-\n-\tcmd.AddDashesAndList(opts.Filenames...)\n-\n-\tif err := cmd.Run(repo.Ctx, &RunOpts{\n-\t\tEnv:    env,\n-\t\tDir:    repo.Path,\n-\t\tStdout: stdOut,\n-\t\tStderr: stdErr,\n-\t}); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to run check-attr: %w\\n%s\\n%s\", err, stdOut.String(), stdErr.String())\n-\t}\n-\n-\t// FIXME: This is incorrect on versions < 1.8.5\n-\tfields := bytes.Split(stdOut.Bytes(), []byte{'\\000'})\n-\n-\tif len(fields)%3 != 1 {\n-\t\treturn nil, errors.New(\"wrong number of fields in return from check-attr\")\n-\t}\n-\n-\tname2attribute2info := make(map[string]map[string]string)\n-\n-\tfor i := 0; i < (len(fields) / 3); i++ {\n-\t\tfilename := string(fields[3*i])\n-\t\tattribute := string(fields[3*i+1])\n-\t\tinfo := string(fields[3*i+2])\n-\t\tattribute2info := name2attribute2info[filename]\n-\t\tif attribute2info == nil {\n-\t\t\tattribute2info = make(map[string]string)\n-\t\t}\n-\t\tattribute2info[attribute] = info\n-\t\tname2attribute2info[filename] = attribute2info\n-\t}\n-\n-\treturn name2attribute2info, nil\n-}\n-\n-// CheckAttributeReader provides a reader for check-attribute content that can be long running\n-type CheckAttributeReader struct {\n-\t// params\n-\tAttributes []string\n-\tRepo       *Repository\n-\tIndexFile  string\n-\tWorkTree   string\n-\n-\tstdinReader io.ReadCloser\n-\tstdinWriter *os.File\n-\tstdOut      *nulSeparatedAttributeWriter\n-\tcmd         *Command\n-\tenv         []string\n-\tctx         context.Context\n-\tcancel      context.CancelFunc\n-}\n-\n-// Init initializes the CheckAttributeReader\n-func (c *CheckAttributeReader) Init(ctx context.Context) error {\n-\tif len(c.Attributes) == 0 {\n-\t\tlw := new(nulSeparatedAttributeWriter)\n-\t\tlw.attributes = make(chan attributeTriple)\n-\t\tlw.closed = make(chan struct{})\n-\n-\t\tc.stdOut = lw\n-\t\tc.stdOut.Close()\n-\t\treturn errors.New(\"no provided Attributes to check\")\n-\t}\n-\n-\tc.ctx, c.cancel = context.WithCancel(ctx)\n-\tc.cmd = NewCommand(\"check-attr\", \"--stdin\", \"-z\")\n-\n-\tif len(c.IndexFile) > 0 {\n-\t\tc.cmd.AddArguments(\"--cached\")\n-\t\tc.env = append(c.env, \"GIT_INDEX_FILE=\"+c.IndexFile)\n-\t}\n-\n-\tif len(c.WorkTree) > 0 {\n-\t\tc.env = append(c.env, \"GIT_WORK_TREE=\"+c.WorkTree)\n-\t}\n-\n-\tc.env = append(c.env, \"GIT_FLUSH=1\")\n-\n-\tc.cmd.AddDynamicArguments(c.Attributes...)\n-\n-\tvar err error\n-\n-\tc.stdinReader, c.stdinWriter, err = os.Pipe()\n-\tif err != nil {\n-\t\tc.cancel()\n-\t\treturn err\n-\t}\n-\n-\tlw := new(nulSeparatedAttributeWriter)\n-\tlw.attributes = make(chan attributeTriple, 5)\n-\tlw.closed = make(chan struct{})\n-\tc.stdOut = lw\n-\treturn nil\n-}\n-\n-func (c *CheckAttributeReader) Run() error {\n-\tdefer func() {\n-\t\t_ = c.stdinReader.Close()\n-\t\t_ = c.stdOut.Close()\n-\t}()\n-\tstdErr := new(bytes.Buffer)\n-\terr := c.cmd.Run(c.ctx, &RunOpts{\n-\t\tEnv:    c.env,\n-\t\tDir:    c.Repo.Path,\n-\t\tStdin:  c.stdinReader,\n-\t\tStdout: c.stdOut,\n-\t\tStderr: stdErr,\n-\t})\n-\tif err != nil && !IsErrCanceledOrKilled(err) {\n-\t\treturn fmt.Errorf(\"failed to run attr-check. Error: %w\\nStderr: %s\", err, stdErr.String())\n-\t}\n-\treturn nil\n-}\n-\n-// CheckPath check attr for given path\n-func (c *CheckAttributeReader) CheckPath(path string) (rs map[string]string, err error) {\n-\tdefer func() {\n-\t\tif err != nil && err != c.ctx.Err() {\n-\t\t\tlog.Error(\"Unexpected error when checking path %s in %s, error: %v\", path, filepath.Base(c.Repo.Path), err)\n-\t\t}\n-\t}()\n-\n-\tselect {\n-\tcase <-c.ctx.Done():\n-\t\treturn nil, c.ctx.Err()\n-\tdefault:\n-\t}\n-\n-\tif _, err = c.stdinWriter.Write([]byte(path + \"\\x00\")); err != nil {\n-\t\tdefer c.Close()\n-\t\treturn nil, err\n-\t}\n-\n-\treportTimeout := func() error {\n-\t\tstdOutClosed := false\n-\t\tselect {\n-\t\tcase <-c.stdOut.closed:\n-\t\t\tstdOutClosed = true\n-\t\tdefault:\n-\t\t}\n-\t\tdebugMsg := fmt.Sprintf(\"check path %q in repo %q\", path, filepath.Base(c.Repo.Path))\n-\t\tdebugMsg += fmt.Sprintf(\", stdOut: tmp=%q, pos=%d, closed=%v\", string(c.stdOut.tmp), c.stdOut.pos, stdOutClosed)\n-\t\tif c.cmd.cmd != nil {\n-\t\t\tdebugMsg += fmt.Sprintf(\", process state: %q\", c.cmd.cmd.ProcessState.String())\n-\t\t}\n-\t\t_ = c.Close()\n-\t\treturn fmt.Errorf(\"CheckPath timeout: %s\", debugMsg)\n-\t}\n-\n-\trs = make(map[string]string)\n-\tfor range c.Attributes {\n-\t\tselect {\n-\t\tcase <-time.After(5 * time.Second):\n-\t\t\t// There is a strange \"hang\" problem in gitdiff.GetDiff -> CheckPath\n-\t\t\t// So add a timeout here to mitigate the problem, and output more logs for debug purpose\n-\t\t\t// In real world, if CheckPath runs long than seconds, it blocks the end user's operation,\n-\t\t\t// and at the moment the CheckPath result is not so important, so we can just ignore it.\n-\t\t\treturn nil, reportTimeout()\n-\t\tcase attr, ok := <-c.stdOut.ReadAttribute():\n-\t\t\tif !ok {\n-\t\t\t\treturn nil, c.ctx.Err()\n-\t\t\t}\n-\t\t\trs[attr.Attribute] = attr.Value\n-\t\tcase <-c.ctx.Done():\n-\t\t\treturn nil, c.ctx.Err()\n-\t\t}\n-\t}\n-\treturn rs, nil\n-}\n-\n-func (c *CheckAttributeReader) Close() error {\n-\tc.cancel()\n-\terr := c.stdinWriter.Close()\n-\treturn err\n-}\n-\n-type attributeTriple struct {\n-\tFilename  string\n-\tAttribute string\n-\tValue     string\n-}\n-\n-type nulSeparatedAttributeWriter struct {\n-\ttmp        []byte\n-\tattributes chan attributeTriple\n-\tclosed     chan struct{}\n-\tworking    attributeTriple\n-\tpos        int\n-}\n-\n-func (wr *nulSeparatedAttributeWriter) Write(p []byte) (n int, err error) {\n-\tl, read := len(p), 0\n-\n-\tnulIdx := bytes.IndexByte(p, '\\x00')\n-\tfor nulIdx >= 0 {\n-\t\twr.tmp = append(wr.tmp, p[:nulIdx]...)\n-\t\tswitch wr.pos {\n-\t\tcase 0:\n-\t\t\twr.working = attributeTriple{\n-\t\t\t\tFilename: string(wr.tmp),\n-\t\t\t}\n-\t\tcase 1:\n-\t\t\twr.working.Attribute = string(wr.tmp)\n-\t\tcase 2:\n-\t\t\twr.working.Value = string(wr.tmp)\n-\t\t}\n-\t\twr.tmp = wr.tmp[:0]\n-\t\twr.pos++\n-\t\tif wr.pos > 2 {\n-\t\t\twr.attributes <- wr.working\n-\t\t\twr.pos = 0\n-\t\t}\n-\t\tread += nulIdx + 1\n-\t\tif l > read {\n-\t\t\tp = p[nulIdx+1:]\n-\t\t\tnulIdx = bytes.IndexByte(p, '\\x00')\n-\t\t} else {\n-\t\t\treturn l, nil\n-\t\t}\n-\t}\n-\twr.tmp = append(wr.tmp, p...)\n-\treturn l, nil\n-}\n-\n-func (wr *nulSeparatedAttributeWriter) ReadAttribute() <-chan attributeTriple {\n-\treturn wr.attributes\n-}\n-\n-func (wr *nulSeparatedAttributeWriter) Close() error {\n-\tselect {\n-\tcase <-wr.closed:\n-\t\treturn nil\n-\tdefault:\n-\t}\n-\tclose(wr.attributes)\n-\tclose(wr.closed)\n-\treturn nil\n-}\n-\n-// CheckAttributeReader creates a check attribute reader for the current repository and provided commit ID\n-func (repo *Repository) CheckAttributeReader(commitID string) (*CheckAttributeReader, context.CancelFunc) {\n-\tindexFilename, worktree, deleteTemporaryFile, err := repo.ReadTreeToTemporaryIndex(commitID)\n-\tif err != nil {\n-\t\treturn nil, func() {}\n-\t}\n-\n-\tchecker := &CheckAttributeReader{\n-\t\tAttributes: []string{\n-\t\t\tAttributeLinguistVendored,\n-\t\t\tAttributeLinguistGenerated,\n-\t\t\tAttributeLinguistDocumentation,\n-\t\t\tAttributeLinguistDetectable,\n-\t\t\tAttributeLinguistLanguage,\n-\t\t\tAttributeGitlabLanguage,\n-\t\t},\n-\t\tRepo:      repo,\n-\t\tIndexFile: indexFilename,\n-\t\tWorkTree:  worktree,\n-\t}\n-\tctx, cancel := context.WithCancel(repo.Ctx)\n-\tif err := checker.Init(ctx); err != nil {\n-\t\tlog.Error(\"Unable to open attribute checker for commit %s, error: %v\", commitID, err)\n-\t} else {\n-\t\tgo func() {\n-\t\t\terr := checker.Run()\n-\t\t\tif err != nil && !IsErrCanceledOrKilled(err) {\n-\t\t\t\tlog.Error(\"Attribute checker for commit %s exits with error: %v\", commitID, err)\n-\t\t\t}\n-\t\t\tcancel()\n-\t\t}()\n-\t}\n-\tdeferrable := func() {\n-\t\t_ = checker.Close()\n-\t\tcancel()\n-\t\tdeleteTemporaryFile()\n-\t}\n-\n-\treturn checker, deferrable\n-}\ndiff --git a/modules/indexer/stats/db.go b/modules/indexer/stats/db.go\nindex 067a6f609bdc8..199d493e97d21 100644\n--- a/modules/indexer/stats/db.go\n+++ b/modules/indexer/stats/db.go\n@@ -8,6 +8,7 @@ import (\n \n \trepo_model \"code.gitea.io/gitea/models/repo\"\n \t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/languagestats\"\n \t\"code.gitea.io/gitea/modules/gitrepo\"\n \t\"code.gitea.io/gitea/modules/graceful\"\n \t\"code.gitea.io/gitea/modules/log\"\n@@ -62,7 +63,7 @@ func (db *DBIndexer) Index(id int64) error {\n \t}\n \n \t// Calculate and save language statistics to database\n-\tstats, err := gitRepo.GetLanguageStats(commitID)\n+\tstats, err := languagestats.GetLanguageStats(gitRepo, commitID)\n \tif err != nil {\n \t\tif !setting.IsInTesting {\n \t\t\tlog.Error(\"Unable to get language stats for ID %s for default branch %s in %s. Error: %v\", commitID, repo.DefaultBranch, repo.FullName(), err)\ndiff --git a/routers/web/repo/blame.go b/routers/web/repo/blame.go\nindex efd85b9452798..e125267524c7b 100644\n--- a/routers/web/repo/blame.go\n+++ b/routers/web/repo/blame.go\n@@ -15,13 +15,13 @@ import (\n \tuser_model \"code.gitea.io/gitea/models/user\"\n \t\"code.gitea.io/gitea/modules/charset\"\n \t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/languagestats\"\n \t\"code.gitea.io/gitea/modules/highlight\"\n \t\"code.gitea.io/gitea/modules/log\"\n \t\"code.gitea.io/gitea/modules/setting\"\n \t\"code.gitea.io/gitea/modules/templates\"\n \t\"code.gitea.io/gitea/modules/util\"\n \t\"code.gitea.io/gitea/services/context\"\n-\tfiles_service \"code.gitea.io/gitea/services/repository/files\"\n )\n \n type blameRow struct {\n@@ -234,7 +234,7 @@ func processBlameParts(ctx *context.Context, blameParts []*git.BlamePart) map[st\n func renderBlame(ctx *context.Context, blameParts []*git.BlamePart, commitNames map[string]*user_model.UserCommit) {\n \trepoLink := ctx.Repo.RepoLink\n \n-\tlanguage, err := files_service.TryGetContentLanguage(ctx.Repo.GitRepo, ctx.Repo.CommitID, ctx.Repo.TreePath)\n+\tlanguage, err := languagestats.GetFileLanguage(ctx, ctx.Repo.GitRepo, ctx.Repo.CommitID, ctx.Repo.TreePath)\n \tif err != nil {\n \t\tlog.Error(\"Unable to get file language for %-v:%s. Error: %v\", ctx.Repo.Repository, ctx.Repo.TreePath, err)\n \t}\ndiff --git a/routers/web/repo/setting/lfs.go b/routers/web/repo/setting/lfs.go\nindex efda9bda58bd5..a065620b2b249 100644\n--- a/routers/web/repo/setting/lfs.go\n+++ b/routers/web/repo/setting/lfs.go\n@@ -18,6 +18,7 @@ import (\n \t\"code.gitea.io/gitea/modules/charset\"\n \t\"code.gitea.io/gitea/modules/container\"\n \t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n \t\"code.gitea.io/gitea/modules/git/pipeline\"\n \t\"code.gitea.io/gitea/modules/lfs\"\n \t\"code.gitea.io/gitea/modules/log\"\n@@ -134,39 +135,24 @@ func LFSLocks(ctx *context.Context) {\n \t}\n \tdefer gitRepo.Close()\n \n-\tfilenames := make([]string, len(lfsLocks))\n-\n-\tfor i, lock := range lfsLocks {\n-\t\tfilenames[i] = lock.Path\n-\t}\n-\n-\tif err := gitRepo.ReadTreeToIndex(ctx.Repo.Repository.DefaultBranch); err != nil {\n-\t\tlog.Error(\"Unable to read the default branch to the index: %s (%v)\", ctx.Repo.Repository.DefaultBranch, err)\n-\t\tctx.ServerError(\"LFSLocks\", fmt.Errorf(\"unable to read the default branch to the index: %s (%w)\", ctx.Repo.Repository.DefaultBranch, err))\n-\t\treturn\n-\t}\n-\n-\tname2attribute2info, err := gitRepo.CheckAttribute(git.CheckAttributeOpts{\n-\t\tAttributes: []string{\"lockable\"},\n-\t\tFilenames:  filenames,\n-\t\tCachedOnly: true,\n-\t})\n+\tchecker, err := attribute.NewBatchChecker(gitRepo, ctx.Repo.Repository.DefaultBranch, []string{attribute.Lockable})\n \tif err != nil {\n \t\tlog.Error(\"Unable to check attributes in %s (%v)\", tmpBasePath, err)\n \t\tctx.ServerError(\"LFSLocks\", err)\n \t\treturn\n \t}\n+\tdefer checker.Close()\n \n \tlockables := make([]bool, len(lfsLocks))\n+\tfilenames := make([]string, len(lfsLocks))\n \tfor i, lock := range lfsLocks {\n-\t\tattribute2info, has := name2attribute2info[lock.Path]\n-\t\tif !has {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif attribute2info[\"lockable\"] != \"set\" {\n+\t\tfilenames[i] = lock.Path\n+\t\tattrs, err := checker.CheckPath(lock.Path)\n+\t\tif err != nil {\n+\t\t\tlog.Error(\"Unable to check attributes in %s: %s (%v)\", tmpBasePath, lock.Path, err)\n \t\t\tcontinue\n \t\t}\n-\t\tlockables[i] = true\n+\t\tlockables[i] = attrs.Get(attribute.Lockable).ToBool().Value()\n \t}\n \tctx.Data[\"Lockables\"] = lockables\n \ndiff --git a/routers/web/repo/view_file.go b/routers/web/repo/view_file.go\nindex 12083a1ced188..ff0e1b4d54c72 100644\n--- a/routers/web/repo/view_file.go\n+++ b/routers/web/repo/view_file.go\n@@ -18,6 +18,7 @@ import (\n \t\"code.gitea.io/gitea/modules/actions\"\n \t\"code.gitea.io/gitea/modules/charset\"\n \t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n \t\"code.gitea.io/gitea/modules/highlight\"\n \t\"code.gitea.io/gitea/modules/log\"\n \t\"code.gitea.io/gitea/modules/markup\"\n@@ -25,7 +26,6 @@ import (\n \t\"code.gitea.io/gitea/modules/util\"\n \t\"code.gitea.io/gitea/services/context\"\n \tissue_service \"code.gitea.io/gitea/services/issue\"\n-\tfiles_service \"code.gitea.io/gitea/services/repository/files\"\n \n \t\"github.com/nektos/act/pkg/model\"\n )\n@@ -147,6 +147,23 @@ func prepareToRenderFile(ctx *context.Context, entry *git.TreeEntry) {\n \t\tctx.Data[\"EditFileTooltip\"] = ctx.Tr(\"repo.editor.cannot_edit_non_text_files\")\n \t}\n \n+\t// read all needed attributes which will be used later\n+\t// there should be no performance different between reading 2 or 4 here\n+\tattrsMap, err := attribute.CheckAttributes(ctx, ctx.Repo.GitRepo, ctx.Repo.CommitID, attribute.CheckAttributeOpts{\n+\t\tFilenames:  []string{ctx.Repo.TreePath},\n+\t\tAttributes: []string{attribute.LinguistGenerated, attribute.LinguistVendored, attribute.LinguistLanguage, attribute.GitlabLanguage},\n+\t})\n+\tif err != nil {\n+\t\tctx.ServerError(\"attribute.CheckAttributes\", err)\n+\t\treturn\n+\t}\n+\tattrs := attrsMap[ctx.Repo.TreePath]\n+\tif attrs == nil {\n+\t\t// this case shouldn't happen, just in case.\n+\t\tsetting.PanicInDevOrTesting(\"no attributes found for %s\", ctx.Repo.TreePath)\n+\t\tattrs = attribute.NewAttributes()\n+\t}\n+\n \tswitch {\n \tcase isRepresentableAsText:\n \t\tif fInfo.fileSize >= setting.UI.MaxDisplayFileSize {\n@@ -209,11 +226,7 @@ func prepareToRenderFile(ctx *context.Context, entry *git.TreeEntry) {\n \t\t\t\tctx.Data[\"NumLines\"] = bytes.Count(buf, []byte{'\\n'}) + 1\n \t\t\t}\n \n-\t\t\tlanguage, err := files_service.TryGetContentLanguage(ctx.Repo.GitRepo, ctx.Repo.CommitID, ctx.Repo.TreePath)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Error(\"Unable to get file language for %-v:%s. Error: %v\", ctx.Repo.Repository, ctx.Repo.TreePath, err)\n-\t\t\t}\n-\n+\t\t\tlanguage := attrs.GetLanguage().Value()\n \t\t\tfileContent, lexerName, err := highlight.File(blob.Name(), language, buf)\n \t\t\tctx.Data[\"LexerName\"] = lexerName\n \t\t\tif err != nil {\n@@ -283,17 +296,7 @@ func prepareToRenderFile(ctx *context.Context, entry *git.TreeEntry) {\n \t\t}\n \t}\n \n-\tif ctx.Repo.GitRepo != nil {\n-\t\tchecker, deferable := ctx.Repo.GitRepo.CheckAttributeReader(ctx.Repo.CommitID)\n-\t\tif checker != nil {\n-\t\t\tdefer deferable()\n-\t\t\tattrs, err := checker.CheckPath(ctx.Repo.TreePath)\n-\t\t\tif err == nil {\n-\t\t\t\tctx.Data[\"IsVendored\"] = git.AttributeToBool(attrs, git.AttributeLinguistVendored).Value()\n-\t\t\t\tctx.Data[\"IsGenerated\"] = git.AttributeToBool(attrs, git.AttributeLinguistGenerated).Value()\n-\t\t\t}\n-\t\t}\n-\t}\n+\tctx.Data[\"IsVendored\"], ctx.Data[\"IsGenerated\"] = attrs.GetVendored().Value(), attrs.GetGenerated().Value()\n \n \tif fInfo.st.IsImage() && !fInfo.st.IsSvgImage() {\n \t\timg, _, err := image.DecodeConfig(bytes.NewReader(buf))\ndiff --git a/services/gitdiff/gitdiff.go b/services/gitdiff/gitdiff.go\nindex b9781cf8d067b..9ee86d9dfc0f8 100644\n--- a/services/gitdiff/gitdiff.go\n+++ b/services/gitdiff/gitdiff.go\n@@ -25,6 +25,7 @@ import (\n \t\"code.gitea.io/gitea/modules/analyze\"\n \t\"code.gitea.io/gitea/modules/charset\"\n \t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n \t\"code.gitea.io/gitea/modules/highlight\"\n \t\"code.gitea.io/gitea/modules/lfs\"\n \t\"code.gitea.io/gitea/modules/log\"\n@@ -1237,24 +1238,21 @@ func GetDiffForRender(ctx context.Context, gitRepo *git.Repository, opts *DiffOp\n \t\treturn nil, err\n \t}\n \n-\tchecker, deferrable := gitRepo.CheckAttributeReader(opts.AfterCommitID)\n-\tdefer deferrable()\n+\tchecker, err := attribute.NewBatchChecker(gitRepo, opts.AfterCommitID, []string{attribute.LinguistVendored, attribute.LinguistGenerated, attribute.LinguistLanguage, attribute.GitlabLanguage})\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdefer checker.Close()\n \n \tfor _, diffFile := range diff.Files {\n \t\tisVendored := optional.None[bool]()\n \t\tisGenerated := optional.None[bool]()\n-\t\tif checker != nil {\n-\t\t\tattrs, err := checker.CheckPath(diffFile.Name)\n-\t\t\tif err == nil {\n-\t\t\t\tisVendored = git.AttributeToBool(attrs, git.AttributeLinguistVendored)\n-\t\t\t\tisGenerated = git.AttributeToBool(attrs, git.AttributeLinguistGenerated)\n-\n-\t\t\t\tlanguage := git.TryReadLanguageAttribute(attrs)\n-\t\t\t\tif language.Has() {\n-\t\t\t\t\tdiffFile.Language = language.Value()\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tchecker = nil // CheckPath fails, it's not impossible to \"check\" anymore\n+\t\tattrs, err := checker.CheckPath(diffFile.Name)\n+\t\tif err == nil {\n+\t\t\tisVendored, isGenerated = attrs.GetVendored(), attrs.GetGenerated()\n+\t\t\tlanguage := attrs.GetLanguage()\n+\t\t\tif language.Has() {\n+\t\t\t\tdiffFile.Language = language.Value()\n \t\t\t}\n \t\t}\n \ndiff --git a/services/markup/renderhelper_codepreview.go b/services/markup/renderhelper_codepreview.go\nindex 28d11209846d8..fa1eb824a2f54 100644\n--- a/services/markup/renderhelper_codepreview.go\n+++ b/services/markup/renderhelper_codepreview.go\n@@ -14,13 +14,13 @@ import (\n \t\"code.gitea.io/gitea/models/repo\"\n \t\"code.gitea.io/gitea/models/unit\"\n \t\"code.gitea.io/gitea/modules/charset\"\n+\t\"code.gitea.io/gitea/modules/git/languagestats\"\n \t\"code.gitea.io/gitea/modules/gitrepo\"\n \t\"code.gitea.io/gitea/modules/indexer/code\"\n \t\"code.gitea.io/gitea/modules/markup\"\n \t\"code.gitea.io/gitea/modules/setting\"\n \t\"code.gitea.io/gitea/modules/util\"\n \tgitea_context \"code.gitea.io/gitea/services/context\"\n-\t\"code.gitea.io/gitea/services/repository/files\"\n )\n \n func renderRepoFileCodePreview(ctx context.Context, opts markup.RenderCodePreviewOptions) (template.HTML, error) {\n@@ -61,7 +61,7 @@ func renderRepoFileCodePreview(ctx context.Context, opts markup.RenderCodePrevie\n \t\treturn \"\", err\n \t}\n \n-\tlanguage, _ := files.TryGetContentLanguage(gitRepo, opts.CommitID, opts.FilePath)\n+\tlanguage, _ := languagestats.GetFileLanguage(ctx, gitRepo, opts.CommitID, opts.FilePath)\n \tblob, err := commit.GetBlobByPath(opts.FilePath)\n \tif err != nil {\n \t\treturn \"\", err\ndiff --git a/services/repository/files/content.go b/services/repository/files/content.go\nindex e23cd1abce610..0327e7f2cebeb 100644\n--- a/services/repository/files/content.go\n+++ b/services/repository/files/content.go\n@@ -277,28 +277,3 @@ func GetBlobBySHA(ctx context.Context, repo *repo_model.Repository, gitRepo *git\n \t\tContent:  content,\n \t}, nil\n }\n-\n-// TryGetContentLanguage tries to get the (linguist) language of the file content\n-func TryGetContentLanguage(gitRepo *git.Repository, commitID, treePath string) (string, error) {\n-\tindexFilename, worktree, deleteTemporaryFile, err := gitRepo.ReadTreeToTemporaryIndex(commitID)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\tdefer deleteTemporaryFile()\n-\n-\tfilename2attribute2info, err := gitRepo.CheckAttribute(git.CheckAttributeOpts{\n-\t\tCachedOnly: true,\n-\t\tAttributes: []string{git.AttributeLinguistLanguage, git.AttributeGitlabLanguage},\n-\t\tFilenames:  []string{treePath},\n-\t\tIndexFile:  indexFilename,\n-\t\tWorkTree:   worktree,\n-\t})\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\tlanguage := git.TryReadLanguageAttribute(filename2attribute2info[treePath])\n-\n-\treturn language.Value(), nil\n-}\ndiff --git a/services/repository/files/update.go b/services/repository/files/update.go\nindex 3f6255e77a77c..75ede4976f9a2 100644\n--- a/services/repository/files/update.go\n+++ b/services/repository/files/update.go\n@@ -15,6 +15,7 @@ import (\n \trepo_model \"code.gitea.io/gitea/models/repo\"\n \tuser_model \"code.gitea.io/gitea/models/user\"\n \t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n \t\"code.gitea.io/gitea/modules/gitrepo\"\n \t\"code.gitea.io/gitea/modules/lfs\"\n \t\"code.gitea.io/gitea/modules/log\"\n@@ -488,16 +489,15 @@ func CreateOrUpdateFile(ctx context.Context, t *TemporaryUploadRepository, file\n \tvar lfsMetaObject *git_model.LFSMetaObject\n \tif setting.LFS.StartServer && hasOldBranch {\n \t\t// Check there is no way this can return multiple infos\n-\t\tfilename2attribute2info, err := t.gitRepo.CheckAttribute(git.CheckAttributeOpts{\n-\t\t\tAttributes: []string{\"filter\"},\n+\t\tattributesMap, err := attribute.CheckAttributes(ctx, t.gitRepo, \"\" /* use temp repo's working dir */, attribute.CheckAttributeOpts{\n+\t\t\tAttributes: []string{attribute.Filter},\n \t\t\tFilenames:  []string{file.Options.treePath},\n-\t\t\tCachedOnly: true,\n \t\t})\n \t\tif err != nil {\n \t\t\treturn err\n \t\t}\n \n-\t\tif filename2attribute2info[file.Options.treePath] != nil && filename2attribute2info[file.Options.treePath][\"filter\"] == \"lfs\" {\n+\t\tif attributesMap[file.Options.treePath] != nil && attributesMap[file.Options.treePath].Get(attribute.Filter).ToString().Value() == \"lfs\" {\n \t\t\t// OK so we are supposed to LFS this data!\n \t\t\tpointer, err := lfs.GeneratePointer(treeObjectContentReader)\n \t\t\tif err != nil {\ndiff --git a/services/repository/files/upload.go b/services/repository/files/upload.go\nindex 2e4ed1744ef7c..f348cb68ab543 100644\n--- a/services/repository/files/upload.go\n+++ b/services/repository/files/upload.go\n@@ -14,6 +14,7 @@ import (\n \trepo_model \"code.gitea.io/gitea/models/repo\"\n \tuser_model \"code.gitea.io/gitea/models/user\"\n \t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/git/attribute\"\n \t\"code.gitea.io/gitea/modules/lfs\"\n \t\"code.gitea.io/gitea/modules/setting\"\n )\n@@ -105,12 +106,11 @@ func UploadRepoFiles(ctx context.Context, repo *repo_model.Repository, doer *use\n \t\t}\n \t}\n \n-\tvar filename2attribute2info map[string]map[string]string\n+\tvar attributesMap map[string]*attribute.Attributes\n \tif setting.LFS.StartServer {\n-\t\tfilename2attribute2info, err = t.gitRepo.CheckAttribute(git.CheckAttributeOpts{\n-\t\t\tAttributes: []string{\"filter\"},\n+\t\tattributesMap, err = attribute.CheckAttributes(ctx, t.gitRepo, \"\" /* use temp repo's working dir */, attribute.CheckAttributeOpts{\n+\t\t\tAttributes: []string{attribute.Filter},\n \t\t\tFilenames:  names,\n-\t\t\tCachedOnly: true,\n \t\t})\n \t\tif err != nil {\n \t\t\treturn err\n@@ -119,7 +119,7 @@ func UploadRepoFiles(ctx context.Context, repo *repo_model.Repository, doer *use\n \n \t// Copy uploaded files into repository.\n \tfor i := range infos {\n-\t\tif err := copyUploadedLFSFileIntoRepository(ctx, &infos[i], filename2attribute2info, t, opts.TreePath); err != nil {\n+\t\tif err := copyUploadedLFSFileIntoRepository(ctx, &infos[i], attributesMap, t, opts.TreePath); err != nil {\n \t\t\treturn err\n \t\t}\n \t}\n@@ -176,7 +176,7 @@ func UploadRepoFiles(ctx context.Context, repo *repo_model.Repository, doer *use\n \treturn repo_model.DeleteUploads(ctx, uploads...)\n }\n \n-func copyUploadedLFSFileIntoRepository(ctx context.Context, info *uploadInfo, filename2attribute2info map[string]map[string]string, t *TemporaryUploadRepository, treePath string) error {\n+func copyUploadedLFSFileIntoRepository(ctx context.Context, info *uploadInfo, attributesMap map[string]*attribute.Attributes, t *TemporaryUploadRepository, treePath string) error {\n \tfile, err := os.Open(info.upload.LocalPath())\n \tif err != nil {\n \t\treturn err\n@@ -184,7 +184,7 @@ func copyUploadedLFSFileIntoRepository(ctx context.Context, info *uploadInfo, fi\n \tdefer file.Close()\n \n \tvar objectHash string\n-\tif setting.LFS.StartServer && filename2attribute2info[info.upload.Name] != nil && filename2attribute2info[info.upload.Name][\"filter\"] == \"lfs\" {\n+\tif setting.LFS.StartServer && attributesMap[info.upload.Name] != nil && attributesMap[info.upload.Name].Get(attribute.Filter).ToString().Value() == \"lfs\" {\n \t\t// Handle LFS\n \t\t// FIXME: Inefficient! this should probably happen in models.Upload\n \t\tpointer, err := lfs.GeneratePointer(file)\n", "test_patch": "diff --git a/modules/git/attribute/attribute_test.go b/modules/git/attribute/attribute_test.go\nnew file mode 100644\nindex 0000000000000..dadb5582a3cb5\n--- /dev/null\n+++ b/modules/git/attribute/attribute_test.go\n@@ -0,0 +1,37 @@\n+// Copyright 2025 The Gitea Authors. All rights reserved.\n+// SPDX-License-Identifier: MIT\n+\n+package attribute\n+\n+import (\n+\t\"testing\"\n+\n+\t\"github.com/stretchr/testify/assert\"\n+)\n+\n+func Test_Attribute(t *testing.T) {\n+\tassert.Empty(t, Attribute(\"\").ToString().Value())\n+\tassert.Empty(t, Attribute(\"unspecified\").ToString().Value())\n+\tassert.Equal(t, \"python\", Attribute(\"python\").ToString().Value())\n+\tassert.Equal(t, \"Java\", Attribute(\"Java\").ToString().Value())\n+\n+\tattributes := Attributes{\n+\t\tm: map[string]Attribute{\n+\t\t\tLinguistGenerated:     \"true\",\n+\t\t\tLinguistDocumentation: \"false\",\n+\t\t\tLinguistDetectable:    \"set\",\n+\t\t\tLinguistLanguage:      \"Python\",\n+\t\t\tGitlabLanguage:        \"Java\",\n+\t\t\t\"filter\":              \"unspecified\",\n+\t\t\t\"test\":                \"\",\n+\t\t},\n+\t}\n+\n+\tassert.Empty(t, attributes.Get(\"test\").ToString().Value())\n+\tassert.Empty(t, attributes.Get(\"filter\").ToString().Value())\n+\tassert.Equal(t, \"Python\", attributes.Get(LinguistLanguage).ToString().Value())\n+\tassert.Equal(t, \"Java\", attributes.Get(GitlabLanguage).ToString().Value())\n+\tassert.True(t, attributes.Get(LinguistGenerated).ToBool().Value())\n+\tassert.False(t, attributes.Get(LinguistDocumentation).ToBool().Value())\n+\tassert.True(t, attributes.Get(LinguistDetectable).ToBool().Value())\n+}\ndiff --git a/modules/git/repo_attribute_test.go b/modules/git/attribute/batch_test.go\nsimilarity index 50%\nrename from modules/git/repo_attribute_test.go\nrename to modules/git/attribute/batch_test.go\nindex d8fd9f0e8d857..30a3d805fe983 100644\n--- a/modules/git/repo_attribute_test.go\n+++ b/modules/git/attribute/batch_test.go\n@@ -1,13 +1,19 @@\n // Copyright 2021 The Gitea Authors. All rights reserved.\n // SPDX-License-Identifier: MIT\n \n-package git\n+package attribute\n \n import (\n+\t\"path/filepath\"\n \t\"testing\"\n \t\"time\"\n \n+\t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/setting\"\n+\t\"code.gitea.io/gitea/modules/test\"\n+\n \t\"github.com/stretchr/testify/assert\"\n+\t\"github.com/stretchr/testify/require\"\n )\n \n func Test_nulSeparatedAttributeWriter_ReadAttribute(t *testing.T) {\n@@ -24,7 +30,7 @@ func Test_nulSeparatedAttributeWriter_ReadAttribute(t *testing.T) {\n \tselect {\n \tcase attr := <-wr.ReadAttribute():\n \t\tassert.Equal(t, \".gitignore\\\"\\n\", attr.Filename)\n-\t\tassert.Equal(t, AttributeLinguistVendored, attr.Attribute)\n+\t\tassert.Equal(t, LinguistVendored, attr.Attribute)\n \t\tassert.Equal(t, \"unspecified\", attr.Value)\n \tcase <-time.After(100 * time.Millisecond):\n \t\tassert.FailNow(t, \"took too long to read an attribute from the list\")\n@@ -38,7 +44,7 @@ func Test_nulSeparatedAttributeWriter_ReadAttribute(t *testing.T) {\n \tselect {\n \tcase attr := <-wr.ReadAttribute():\n \t\tassert.Equal(t, \".gitignore\\\"\\n\", attr.Filename)\n-\t\tassert.Equal(t, AttributeLinguistVendored, attr.Attribute)\n+\t\tassert.Equal(t, LinguistVendored, attr.Attribute)\n \t\tassert.Equal(t, \"unspecified\", attr.Value)\n \tcase <-time.After(100 * time.Millisecond):\n \t\tassert.FailNow(t, \"took too long to read an attribute from the list\")\n@@ -77,21 +83,90 @@ func Test_nulSeparatedAttributeWriter_ReadAttribute(t *testing.T) {\n \tassert.NoError(t, err)\n \tassert.Equal(t, attributeTriple{\n \t\tFilename:  \"shouldbe.vendor\",\n-\t\tAttribute: AttributeLinguistVendored,\n+\t\tAttribute: LinguistVendored,\n \t\tValue:     \"set\",\n \t}, attr)\n \tattr = <-wr.ReadAttribute()\n \tassert.NoError(t, err)\n \tassert.Equal(t, attributeTriple{\n \t\tFilename:  \"shouldbe.vendor\",\n-\t\tAttribute: AttributeLinguistGenerated,\n+\t\tAttribute: LinguistGenerated,\n \t\tValue:     \"unspecified\",\n \t}, attr)\n \tattr = <-wr.ReadAttribute()\n \tassert.NoError(t, err)\n \tassert.Equal(t, attributeTriple{\n \t\tFilename:  \"shouldbe.vendor\",\n-\t\tAttribute: AttributeLinguistLanguage,\n+\t\tAttribute: LinguistLanguage,\n \t\tValue:     \"unspecified\",\n \t}, attr)\n }\n+\n+func expectedAttrs() *Attributes {\n+\treturn &Attributes{\n+\t\tm: map[string]Attribute{\n+\t\t\tLinguistGenerated:     \"unspecified\",\n+\t\t\tLinguistDetectable:    \"unspecified\",\n+\t\t\tLinguistDocumentation: \"unspecified\",\n+\t\t\tLinguistVendored:      \"unspecified\",\n+\t\t\tLinguistLanguage:      \"Python\",\n+\t\t\tGitlabLanguage:        \"unspecified\",\n+\t\t},\n+\t}\n+}\n+\n+func Test_BatchChecker(t *testing.T) {\n+\tsetting.AppDataPath = t.TempDir()\n+\trepoPath := \"../tests/repos/language_stats_repo\"\n+\tgitRepo, err := git.OpenRepository(t.Context(), repoPath)\n+\trequire.NoError(t, err)\n+\tdefer gitRepo.Close()\n+\n+\tcommitID := \"8fee858da5796dfb37704761701bb8e800ad9ef3\"\n+\n+\tt.Run(\"Create index file to run git check-attr\", func(t *testing.T) {\n+\t\tdefer test.MockVariableValue(&git.DefaultFeatures().SupportCheckAttrOnBare, false)()\n+\t\tchecker, err := NewBatchChecker(gitRepo, commitID, LinguistAttributes)\n+\t\tassert.NoError(t, err)\n+\t\tdefer checker.Close()\n+\t\tattributes, err := checker.CheckPath(\"i-am-a-python.p\")\n+\t\tassert.NoError(t, err)\n+\t\tassert.Equal(t, expectedAttrs(), attributes)\n+\t})\n+\n+\t// run git check-attr on work tree\n+\tt.Run(\"Run git check-attr on git work tree\", func(t *testing.T) {\n+\t\tdir := filepath.Join(t.TempDir(), \"test-repo\")\n+\t\terr := git.Clone(t.Context(), repoPath, dir, git.CloneRepoOptions{\n+\t\t\tShared: true,\n+\t\t\tBranch: \"master\",\n+\t\t})\n+\t\tassert.NoError(t, err)\n+\n+\t\ttempRepo, err := git.OpenRepository(t.Context(), dir)\n+\t\tassert.NoError(t, err)\n+\t\tdefer tempRepo.Close()\n+\n+\t\tchecker, err := NewBatchChecker(tempRepo, \"\", LinguistAttributes)\n+\t\tassert.NoError(t, err)\n+\t\tdefer checker.Close()\n+\t\tattributes, err := checker.CheckPath(\"i-am-a-python.p\")\n+\t\tassert.NoError(t, err)\n+\t\tassert.Equal(t, expectedAttrs(), attributes)\n+\t})\n+\n+\tif !git.DefaultFeatures().SupportCheckAttrOnBare {\n+\t\tt.Skip(\"git version 2.40 is required to support run check-attr on bare repo\")\n+\t\treturn\n+\t}\n+\n+\tt.Run(\"Run git check-attr in bare repository\", func(t *testing.T) {\n+\t\tchecker, err := NewBatchChecker(gitRepo, commitID, LinguistAttributes)\n+\t\tassert.NoError(t, err)\n+\t\tdefer checker.Close()\n+\n+\t\tattributes, err := checker.CheckPath(\"i-am-a-python.p\")\n+\t\tassert.NoError(t, err)\n+\t\tassert.Equal(t, expectedAttrs(), attributes)\n+\t})\n+}\ndiff --git a/modules/git/attribute/checker_test.go b/modules/git/attribute/checker_test.go\nnew file mode 100644\nindex 0000000000000..97db43460bb81\n--- /dev/null\n+++ b/modules/git/attribute/checker_test.go\n@@ -0,0 +1,74 @@\n+// Copyright 2025 The Gitea Authors. All rights reserved.\n+// SPDX-License-Identifier: MIT\n+\n+package attribute\n+\n+import (\n+\t\"path/filepath\"\n+\t\"testing\"\n+\n+\t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/setting\"\n+\t\"code.gitea.io/gitea/modules/test\"\n+\n+\t\"github.com/stretchr/testify/assert\"\n+\t\"github.com/stretchr/testify/require\"\n+)\n+\n+func Test_Checker(t *testing.T) {\n+\tsetting.AppDataPath = t.TempDir()\n+\trepoPath := \"../tests/repos/language_stats_repo\"\n+\tgitRepo, err := git.OpenRepository(t.Context(), repoPath)\n+\trequire.NoError(t, err)\n+\tdefer gitRepo.Close()\n+\n+\tcommitID := \"8fee858da5796dfb37704761701bb8e800ad9ef3\"\n+\n+\tt.Run(\"Create index file to run git check-attr\", func(t *testing.T) {\n+\t\tdefer test.MockVariableValue(&git.DefaultFeatures().SupportCheckAttrOnBare, false)()\n+\t\tattrs, err := CheckAttributes(t.Context(), gitRepo, commitID, CheckAttributeOpts{\n+\t\t\tFilenames:  []string{\"i-am-a-python.p\"},\n+\t\t\tAttributes: LinguistAttributes,\n+\t\t})\n+\t\tassert.NoError(t, err)\n+\t\tassert.Len(t, attrs, 1)\n+\t\tassert.Equal(t, expectedAttrs(), attrs[\"i-am-a-python.p\"])\n+\t})\n+\n+\t// run git check-attr on work tree\n+\tt.Run(\"Run git check-attr on git work tree\", func(t *testing.T) {\n+\t\tdir := filepath.Join(t.TempDir(), \"test-repo\")\n+\t\terr := git.Clone(t.Context(), repoPath, dir, git.CloneRepoOptions{\n+\t\t\tShared: true,\n+\t\t\tBranch: \"master\",\n+\t\t})\n+\t\tassert.NoError(t, err)\n+\n+\t\ttempRepo, err := git.OpenRepository(t.Context(), dir)\n+\t\tassert.NoError(t, err)\n+\t\tdefer tempRepo.Close()\n+\n+\t\tattrs, err := CheckAttributes(t.Context(), tempRepo, \"\", CheckAttributeOpts{\n+\t\t\tFilenames:  []string{\"i-am-a-python.p\"},\n+\t\t\tAttributes: LinguistAttributes,\n+\t\t})\n+\t\tassert.NoError(t, err)\n+\t\tassert.Len(t, attrs, 1)\n+\t\tassert.Equal(t, expectedAttrs(), attrs[\"i-am-a-python.p\"])\n+\t})\n+\n+\tif !git.DefaultFeatures().SupportCheckAttrOnBare {\n+\t\tt.Skip(\"git version 2.40 is required to support run check-attr on bare repo\")\n+\t\treturn\n+\t}\n+\n+\tt.Run(\"Run git check-attr in bare repository\", func(t *testing.T) {\n+\t\tattrs, err := CheckAttributes(t.Context(), gitRepo, commitID, CheckAttributeOpts{\n+\t\t\tFilenames:  []string{\"i-am-a-python.p\"},\n+\t\t\tAttributes: LinguistAttributes,\n+\t\t})\n+\t\tassert.NoError(t, err)\n+\t\tassert.Len(t, attrs, 1)\n+\t\tassert.Equal(t, expectedAttrs(), attrs[\"i-am-a-python.p\"])\n+\t})\n+}\ndiff --git a/modules/git/attribute/main_test.go b/modules/git/attribute/main_test.go\nnew file mode 100644\nindex 0000000000000..df8241bfb08d4\n--- /dev/null\n+++ b/modules/git/attribute/main_test.go\n@@ -0,0 +1,41 @@\n+// Copyright 2025 The Gitea Authors. All rights reserved.\n+// SPDX-License-Identifier: MIT\n+\n+package attribute\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"testing\"\n+\n+\t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/setting\"\n+\t\"code.gitea.io/gitea/modules/util\"\n+)\n+\n+func testRun(m *testing.M) error {\n+\tgitHomePath, err := os.MkdirTemp(os.TempDir(), \"git-home\")\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"unable to create temp dir: %w\", err)\n+\t}\n+\tdefer util.RemoveAll(gitHomePath)\n+\tsetting.Git.HomePath = gitHomePath\n+\n+\tif err = git.InitFull(context.Background()); err != nil {\n+\t\treturn fmt.Errorf(\"failed to call Init: %w\", err)\n+\t}\n+\n+\texitCode := m.Run()\n+\tif exitCode != 0 {\n+\t\treturn fmt.Errorf(\"run test failed, ExitCode=%d\", exitCode)\n+\t}\n+\treturn nil\n+}\n+\n+func TestMain(m *testing.M) {\n+\tif err := testRun(m); err != nil {\n+\t\t_, _ = fmt.Fprintf(os.Stderr, \"Test failed: %v\", err)\n+\t\tos.Exit(1)\n+\t}\n+}\ndiff --git a/modules/git/repo_language_stats_test.go b/modules/git/languagestats/language_stats_test.go\nsimilarity index 75%\nrename from modules/git/repo_language_stats_test.go\nrename to modules/git/languagestats/language_stats_test.go\nindex 12ce958c6e556..b908ae6413d72 100644\n--- a/modules/git/repo_language_stats_test.go\n+++ b/modules/git/languagestats/language_stats_test.go\n@@ -3,12 +3,12 @@\n \n //go:build !gogit\n \n-package git\n+package languagestats\n \n import (\n-\t\"path/filepath\"\n \t\"testing\"\n \n+\t\"code.gitea.io/gitea/modules/git\"\n \t\"code.gitea.io/gitea/modules/setting\"\n \n \t\"github.com/stretchr/testify/assert\"\n@@ -17,13 +17,12 @@ import (\n \n func TestRepository_GetLanguageStats(t *testing.T) {\n \tsetting.AppDataPath = t.TempDir()\n-\trepoPath := filepath.Join(testReposDir, \"language_stats_repo\")\n-\tgitRepo, err := openRepositoryWithDefaultContext(repoPath)\n+\trepoPath := \"../tests/repos/language_stats_repo\"\n+\tgitRepo, err := git.OpenRepository(t.Context(), repoPath)\n \trequire.NoError(t, err)\n-\n \tdefer gitRepo.Close()\n \n-\tstats, err := gitRepo.GetLanguageStats(\"8fee858da5796dfb37704761701bb8e800ad9ef3\")\n+\tstats, err := GetLanguageStats(gitRepo, \"8fee858da5796dfb37704761701bb8e800ad9ef3\")\n \trequire.NoError(t, err)\n \n \tassert.Equal(t, map[string]int64{\ndiff --git a/modules/git/languagestats/main_test.go b/modules/git/languagestats/main_test.go\nnew file mode 100644\nindex 0000000000000..707d268c818ef\n--- /dev/null\n+++ b/modules/git/languagestats/main_test.go\n@@ -0,0 +1,41 @@\n+// Copyright 2025 The Gitea Authors. All rights reserved.\n+// SPDX-License-Identifier: MIT\n+\n+package languagestats\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"testing\"\n+\n+\t\"code.gitea.io/gitea/modules/git\"\n+\t\"code.gitea.io/gitea/modules/setting\"\n+\t\"code.gitea.io/gitea/modules/util\"\n+)\n+\n+func testRun(m *testing.M) error {\n+\tgitHomePath, err := os.MkdirTemp(os.TempDir(), \"git-home\")\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"unable to create temp dir: %w\", err)\n+\t}\n+\tdefer util.RemoveAll(gitHomePath)\n+\tsetting.Git.HomePath = gitHomePath\n+\n+\tif err = git.InitFull(context.Background()); err != nil {\n+\t\treturn fmt.Errorf(\"failed to call Init: %w\", err)\n+\t}\n+\n+\texitCode := m.Run()\n+\tif exitCode != 0 {\n+\t\treturn fmt.Errorf(\"run test failed, ExitCode=%d\", exitCode)\n+\t}\n+\treturn nil\n+}\n+\n+func TestMain(m *testing.M) {\n+\tif err := testRun(m); err != nil {\n+\t\t_, _ = fmt.Fprintf(os.Stderr, \"Test failed: %v\", err)\n+\t\tos.Exit(1)\n+\t}\n+}\ndiff --git a/modules/git/tests/repos/language_stats_repo/config b/modules/git/tests/repos/language_stats_repo/config\nindex 515f4836297fd..a4ef456cbc233 100644\n--- a/modules/git/tests/repos/language_stats_repo/config\n+++ b/modules/git/tests/repos/language_stats_repo/config\n@@ -1,5 +1,5 @@\n [core]\n \trepositoryformatversion = 0\n \tfilemode = true\n-\tbare = false\n+\tbare = true\n \tlogallrefupdates = true\ndiff --git a/modules/git/tests/repos/repo3_notes/config b/modules/git/tests/repos/repo3_notes/config\nindex d545cdabdbdda..5ed22e23d15d7 100644\n--- a/modules/git/tests/repos/repo3_notes/config\n+++ b/modules/git/tests/repos/repo3_notes/config\n@@ -1,7 +1,7 @@\n [core]\n \trepositoryformatversion = 0\n \tfilemode = false\n-\tbare = false\n+\tbare = true\n \tlogallrefupdates = true\n \tsymlinks = false\n \tignorecase = true\ndiff --git a/modules/git/tests/repos/repo4_commitsbetween/config b/modules/git/tests/repos/repo4_commitsbetween/config\nindex d545cdabdbdda..5ed22e23d15d7 100644\n--- a/modules/git/tests/repos/repo4_commitsbetween/config\n+++ b/modules/git/tests/repos/repo4_commitsbetween/config\n@@ -1,7 +1,7 @@\n [core]\n \trepositoryformatversion = 0\n \tfilemode = false\n-\tbare = false\n+\tbare = true\n \tlogallrefupdates = true\n \tsymlinks = false\n \tignorecase = true\n"}
{"org": "gofiber", "repo": "fiber", "number": 3895, "state": "closed", "title": "âš¡ perf: Improve performance of RebuildTree() by 68%", "body": "## Summary\r\n- This pull request focuses on improving the efficiency and memory management of the routing tree construction within the application. It refactors the buildTree function to simplify bucket allocation and ensure precise preallocation of data structures. Additionally, a new benchmark is added to track the performance of the tree rebuilding operation, providing a baseline for future optimizations.\r\n\r\n### Before\r\n\r\n```console\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/gofiber/fiber/v3\r\ncpu: AMD Ryzen 7 7800X3D 8-Core Processor           \r\nBenchmark_App_RebuildTree\r\nBenchmark_App_RebuildTree-4   \t   32095\t     37059 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32386\t     37005 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   33099\t     36389 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   31860\t     36865 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32568\t     37237 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32446\t     36411 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   33307\t     36342 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32185\t     36923 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   33567\t     36420 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   31746\t     36938 ns/op\t   26768 B/op\t     290 allocs/op\r\nPASS\r\nok  \tgithub.com/gofiber/fiber/v3\t11.972s\r\n```\r\n\r\n### After\r\n\r\n```console\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/gofiber/fiber/v3\r\ncpu: AMD Ryzen 7 7800X3D 8-Core Processor           \r\nBenchmark_App_RebuildTree\r\nBenchmark_App_RebuildTree-4   \t   88698\t     13380 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   87627\t     13122 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   89563\t     13367 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   90487\t     13846 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   83514\t     14268 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   81895\t     13929 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   84560\t     14110 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   81997\t     13364 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   87638\t     13045 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   91272\t     13427 ns/op\t   17952 B/op\t      93 allocs/op\r\nPASS\r\nok  \tgithub.com/gofiber/fiber/v3\t11.795s\r\n```\r\n\r\n### Benchstat\r\n\r\n```console\r\n$ go run golang.org/x/perf/cmd/benchstat@latest old.txt new.txt \r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/gofiber/fiber/v3\r\ncpu: AMD Ryzen 7 7800X3D 8-Core Processor           \r\n                   â”‚   old.txt   â”‚               new.txt               â”‚\r\n                   â”‚   sec/op    â”‚   sec/op     vs base                â”‚\r\n_App_RebuildTree-4   36.89Âµ Â± 1%   13.40Âµ Â± 5%  -63.67% (p=0.000 n=10)\r\n\r\n                   â”‚   old.txt    â”‚               new.txt                â”‚\r\n                   â”‚     B/op     â”‚     B/op      vs base                â”‚\r\n_App_RebuildTree-4   26.14Ki Â± 0%   17.53Ki Â± 0%  -32.93% (p=0.000 n=10)\r\n\r\n                   â”‚   old.txt   â”‚              new.txt               â”‚\r\n                   â”‚  allocs/op  â”‚ allocs/op   vs base                â”‚\r\n_App_RebuildTree-4   290.00 Â± 0%   93.00 Â± 0%  -67.93% (p=0.000 n=10)\r\n```\r\n", "url": "https://api.github.com/repos/gofiber/fiber/pulls/3895", "id": 3038244364, "node_id": "PR_kwDODfYWS861F-4M", "html_url": "https://github.com/gofiber/fiber/pull/3895", "diff_url": "https://github.com/gofiber/fiber/pull/3895.diff", "patch_url": "https://github.com/gofiber/fiber/pull/3895.patch", "issue_url": "https://api.github.com/repos/gofiber/fiber/issues/3895", "created_at": "2025-11-24T02:46:25+00:00", "updated_at": "2025-11-24T12:11:23+00:00", "closed_at": "2025-11-24T12:11:11+00:00", "merged_at": "2025-11-24T12:11:11+00:00", "merge_commit_sha": "a477afdf84310dbc34f1ac9ec72ce2765cb7e004", "labels": ["ðŸ§¹ Updates", "v3", "âš¡ï¸ Performance", "codex"], "draft": false, "commits_url": "https://api.github.com/repos/gofiber/fiber/pulls/3895/commits", "review_comments_url": "https://api.github.com/repos/gofiber/fiber/pulls/3895/comments", "review_comment_url": "https://api.github.com/repos/gofiber/fiber/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/gofiber/fiber/issues/3895/comments", "base": {"label": "gofiber:main", "ref": "main", "sha": "5caa478c02c5eb6cd1c251ddc634ec0600e66a7b", "user": {"login": "gofiber", "id": 59947262, "node_id": "MDEyOk9yZ2FuaXphdGlvbjU5OTQ3MjYy", "avatar_url": "https://avatars.githubusercontent.com/u/59947262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gofiber", "html_url": "https://github.com/gofiber", "followers_url": "https://api.github.com/users/gofiber/followers", "following_url": "https://api.github.com/users/gofiber/following{/other_user}", "gists_url": "https://api.github.com/users/gofiber/gists{/gist_id}", "starred_url": "https://api.github.com/users/gofiber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gofiber/subscriptions", "organizations_url": "https://api.github.com/users/gofiber/orgs", "repos_url": "https://api.github.com/users/gofiber/repos", "events_url": "https://api.github.com/users/gofiber/events{/privacy}", "received_events_url": "https://api.github.com/users/gofiber/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "repo": {"id": 234231371, "node_id": "MDEwOlJlcG9zaXRvcnkyMzQyMzEzNzE=", "name": "fiber", "full_name": "gofiber/fiber", "private": false, "owner": {"login": "gofiber", "id": 59947262, "node_id": "MDEyOk9yZ2FuaXphdGlvbjU5OTQ3MjYy", "avatar_url": "https://avatars.githubusercontent.com/u/59947262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gofiber", "html_url": "https://github.com/gofiber", "followers_url": "https://api.github.com/users/gofiber/followers", "following_url": "https://api.github.com/users/gofiber/following{/other_user}", "gists_url": "https://api.github.com/users/gofiber/gists{/gist_id}", "starred_url": "https://api.github.com/users/gofiber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gofiber/subscriptions", "organizations_url": "https://api.github.com/users/gofiber/orgs", "repos_url": "https://api.github.com/users/gofiber/repos", "events_url": "https://api.github.com/users/gofiber/events{/privacy}", "received_events_url": "https://api.github.com/users/gofiber/received_events", "type": "Organization", "user_view_type": "public", "site_admin": false}, "html_url": "https://github.com/gofiber/fiber", "description": "âš¡ï¸ Express inspired web framework written in Go", "fork": false, "url": "https://api.github.com/repos/gofiber/fiber", "forks_url": "https://api.github.com/repos/gofiber/fiber/forks", "keys_url": "https://api.github.com/repos/gofiber/fiber/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/gofiber/fiber/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/gofiber/fiber/teams", "hooks_url": "https://api.github.com/repos/gofiber/fiber/hooks", "issue_events_url": "https://api.github.com/repos/gofiber/fiber/issues/events{/number}", "events_url": "https://api.github.com/repos/gofiber/fiber/events", "assignees_url": "https://api.github.com/repos/gofiber/fiber/assignees{/user}", "branches_url": "https://api.github.com/repos/gofiber/fiber/branches{/branch}", "tags_url": "https://api.github.com/repos/gofiber/fiber/tags", "blobs_url": "https://api.github.com/repos/gofiber/fiber/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/gofiber/fiber/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/gofiber/fiber/git/refs{/sha}", "trees_url": "https://api.github.com/repos/gofiber/fiber/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/gofiber/fiber/statuses/{sha}", "languages_url": "https://api.github.com/repos/gofiber/fiber/languages", "stargazers_url": "https://api.github.com/repos/gofiber/fiber/stargazers", "contributors_url": "https://api.github.com/repos/gofiber/fiber/contributors", "subscribers_url": "https://api.github.com/repos/gofiber/fiber/subscribers", "subscription_url": "https://api.github.com/repos/gofiber/fiber/subscription", "commits_url": "https://api.github.com/repos/gofiber/fiber/commits{/sha}", "git_commits_url": "https://api.github.com/repos/gofiber/fiber/git/commits{/sha}", "comments_url": "https://api.github.com/repos/gofiber/fiber/comments{/number}", "issue_comment_url": "https://api.github.com/repos/gofiber/fiber/issues/comments{/number}", "contents_url": "https://api.github.com/repos/gofiber/fiber/contents/{+path}", "compare_url": "https://api.github.com/repos/gofiber/fiber/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/gofiber/fiber/merges", "archive_url": "https://api.github.com/repos/gofiber/fiber/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/gofiber/fiber/downloads", "issues_url": "https://api.github.com/repos/gofiber/fiber/issues{/number}", "pulls_url": "https://api.github.com/repos/gofiber/fiber/pulls{/number}", "milestones_url": "https://api.github.com/repos/gofiber/fiber/milestones{/number}", "notifications_url": "https://api.github.com/repos/gofiber/fiber/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/gofiber/fiber/labels{/name}", "releases_url": "https://api.github.com/repos/gofiber/fiber/releases{/id}", "deployments_url": "https://api.github.com/repos/gofiber/fiber/deployments", "created_at": "2020-01-16T03:59:20Z", "updated_at": "2026-01-07T16:33:00Z", "pushed_at": "2026-01-07T14:22:10Z", "git_url": "git://github.com/gofiber/fiber.git", "ssh_url": "git@github.com:gofiber/fiber.git", "clone_url": "https://github.com/gofiber/fiber.git", "svn_url": "https://github.com/gofiber/fiber", "homepage": "https://gofiber.io", "size": 235074, "stargazers_count": 38919, "watchers_count": 38919, "language": "Go", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": true, "has_discussions": false, "forks_count": 1937, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 73, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "topics": ["express", "expressjs", "fast", "fiber", "flexible", "framework", "friendly", "go", "golang", "hacktoberfest", "hacktoberfest2020", "nodejs", "performance", "rest-api", "web"], "visibility": "public", "forks": 1937, "open_issues": 73, "watchers": 38919, "default_branch": "main"}}, "commits": [{"sha": "ee1466ccc04f6df90b1f39d780a24227ec7c81fc", "parents": ["46651b0011ad28bde4b2f2ae54118fc08e64e8cc"], "message": "Add RebuildTree benchmark"}], "resolved_issues": [{"org": "gofiber", "repo": "fiber", "number": -1, "state": "unknown", "title": "âš¡ perf: Improve performance of RebuildTree() by 68%", "body": "## Summary\r\n- This pull request focuses on improving the efficiency and memory management of the routing tree construction within the application. It refactors the buildTree function to simplify bucket allocation and ensure precise preallocation of data structures. Additionally, a new benchmark is added to track the performance of the tree rebuilding operation, providing a baseline for future optimizations.\r\n\r\n### Before\r\n\r\n```console\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/gofiber/fiber/v3\r\ncpu: AMD Ryzen 7 7800X3D 8-Core Processor           \r\nBenchmark_App_RebuildTree\r\nBenchmark_App_RebuildTree-4   \t   32095\t     37059 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32386\t     37005 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   33099\t     36389 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   31860\t     36865 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32568\t     37237 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32446\t     36411 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   33307\t     36342 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   32185\t     36923 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   33567\t     36420 ns/op\t   26768 B/op\t     290 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   31746\t     36938 ns/op\t   26768 B/op\t     290 allocs/op\r\nPASS\r\nok  \tgithub.com/gofiber/fiber/v3\t11.972s\r\n```\r\n\r\n### After\r\n\r\n```console\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/gofiber/fiber/v3\r\ncpu: AMD Ryzen 7 7800X3D 8-Core Processor           \r\nBenchmark_App_RebuildTree\r\nBenchmark_App_RebuildTree-4   \t   88698\t     13380 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   87627\t     13122 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   89563\t     13367 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   90487\t     13846 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   83514\t     14268 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   81895\t     13929 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   84560\t     14110 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   81997\t     13364 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   87638\t     13045 ns/op\t   17952 B/op\t      93 allocs/op\r\nBenchmark_App_RebuildTree-4   \t   91272\t     13427 ns/op\t   17952 B/op\t      93 allocs/op\r\nPASS\r\nok  \tgithub.com/gofiber/fiber/v3\t11.795s\r\n```\r\n\r\n### Benchstat\r\n\r\n```console\r\n$ go run golang.org/x/perf/cmd/benchstat@latest old.txt new.txt \r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/gofiber/fiber/v3\r\ncpu: AMD Ryzen 7 7800X3D 8-Core Processor           \r\n                   â”‚   old.txt   â”‚               new.txt               â”‚\r\n                   â”‚   sec/op    â”‚   sec/op     vs base                â”‚\r\n_App_RebuildTree-4   36.89Âµ Â± 1%   13.40Âµ Â± 5%  -63.67% (p=0.000 n=10)\r\n\r\n                   â”‚   old.txt    â”‚               new.txt                â”‚\r\n                   â”‚     B/op     â”‚     B/op      vs base                â”‚\r\n_App_RebuildTree-4   26.14Ki Â± 0%   17.53Ki Â± 0%  -32.93% (p=0.000 n=10)\r\n\r\n                   â”‚   old.txt   â”‚              new.txt               â”‚\r\n                   â”‚  allocs/op  â”‚ allocs/op   vs base                â”‚\r\n_App_RebuildTree-4   290.00 Â± 0%   93.00 Â± 0%  -67.93% (p=0.000 n=10)\r\n```\r\n"}], "fix_patch": "diff --git a/router.go b/router.go\nindex 917b7b1f33..a333b4a0ab 100644\n--- a/router.go\n+++ b/router.go\n@@ -710,52 +710,47 @@ func (app *App) buildTree() *App {\n \n \t// 1) First loop: determine all possible 3-char prefixes (\"treePaths\") for each method\n \tfor method := range app.config.RequestMethods {\n-\t\tprefixSet := map[int]struct{}{\n-\t\t\t0: {},\n-\t\t}\n-\t\tfor _, route := range app.stack[method] {\n+\t\troutes := app.stack[method]\n+\t\ttreePaths := make([]int, len(routes))\n+\n+\t\tglobalCount := 0\n+\t\tprefixCounts := make(map[int]int, len(routes))\n+\n+\t\tfor i, route := range routes {\n \t\t\tif len(route.routeParser.segs) > 0 && len(route.routeParser.segs[0].Const) >= maxDetectionPaths {\n-\t\t\t\tprefix := int(route.routeParser.segs[0].Const[0])<<16 |\n+\t\t\t\ttreePaths[i] = int(route.routeParser.segs[0].Const[0])<<16 |\n \t\t\t\t\tint(route.routeParser.segs[0].Const[1])<<8 |\n \t\t\t\t\tint(route.routeParser.segs[0].Const[2])\n-\t\t\t\tprefixSet[prefix] = struct{}{}\n \t\t\t}\n+\n+\t\t\tif treePaths[i] == 0 {\n+\t\t\t\tglobalCount++\n+\t\t\t\tcontinue\n+\t\t\t}\n+\n+\t\t\tprefixCounts[treePaths[i]]++\n \t\t}\n-\t\ttsMap := make(map[int][]*Route, len(prefixSet))\n-\t\tfor prefix := range prefixSet {\n-\t\t\ttsMap[prefix] = nil\n+\n+\t\ttsMap := make(map[int][]*Route, len(prefixCounts)+1)\n+\t\ttsMap[0] = make([]*Route, 0, globalCount)\n+\t\tfor treePath, count := range prefixCounts {\n+\t\t\ttsMap[treePath] = make([]*Route, 0, count+globalCount)\n \t\t}\n-\t\tapp.treeStack[method] = tsMap\n-\t}\n \n-\t// 2) Second loop: for each method and each discovered treePath, assign matching routes\n-\tfor method := range app.config.RequestMethods {\n-\t\t// get the map of buckets for this method\n-\t\ttsMap := app.treeStack[method]\n-\n-\t\t// for every treePath key (including the empty one)\n-\t\tfor treePath := range tsMap {\n-\t\t\t// iterate all routes of this method\n-\t\t\tfor _, route := range app.stack[method] {\n-\t\t\t\t// compute this route's own prefix (\"\" or first 3 chars)\n-\t\t\t\troutePath := 0\n-\t\t\t\tif len(route.routeParser.segs) > 0 && len(route.routeParser.segs[0].Const) >= 3 {\n-\t\t\t\t\troutePath = int(route.routeParser.segs[0].Const[0])<<16 |\n-\t\t\t\t\t\tint(route.routeParser.segs[0].Const[1])<<8 |\n-\t\t\t\t\t\tint(route.routeParser.segs[0].Const[2])\n-\t\t\t\t}\n+\t\tfor i, route := range routes {\n+\t\t\ttreePath := treePaths[i]\n \n-\t\t\t\t// if it's a global route, assign to every bucket\n-\t\t\t\t// If the route path is 0 (global route) or matches the current tree path,\n-\t\t\t\t// append this route to the current bucket\n-\t\t\t\tif routePath == 0 || routePath == treePath {\n-\t\t\t\t\ttsMap[treePath] = append(tsMap[treePath], route)\n+\t\t\tif treePath == 0 {\n+\t\t\t\tfor bucket := range tsMap {\n+\t\t\t\t\ttsMap[bucket] = append(tsMap[bucket], route)\n \t\t\t\t}\n+\t\t\t\tcontinue\n \t\t\t}\n \n-\t\t\t// after collecting, dedupe the bucket if it's not the global one\n-\t\t\ttsMap[treePath] = uniqueRouteStack(tsMap[treePath])\n+\t\t\ttsMap[treePath] = append(tsMap[treePath], route)\n \t\t}\n+\n+\t\tapp.treeStack[method] = tsMap\n \t}\n \n \t// reset the flag and return\n", "test_patch": "diff --git a/router_test.go b/router_test.go\nindex 6ad70ccbc7..9cbaa31ef0 100644\n--- a/router_test.go\n+++ b/router_test.go\n@@ -1454,6 +1454,20 @@ func registerDummyRoutes(app *App) {\n \t}\n }\n \n+// go test -v -run=^$ -bench=Benchmark_App_RebuildTree -benchmem -count=4\n+func Benchmark_App_RebuildTree(b *testing.B) {\n+\tapp := New()\n+\tregisterDummyRoutes(app)\n+\n+\tb.ReportAllocs()\n+\tb.ResetTimer()\n+\n+\tfor b.Loop() {\n+\t\tapp.routesRefreshed = true\n+\t\tapp.RebuildTree()\n+\t}\n+}\n+\n // go test -v -run=^$ -bench=Benchmark_App_MethodNotAllowed -benchmem -count=4\n func Benchmark_App_MethodNotAllowed(b *testing.B) {\n \tapp := New()\n"}
