# Multi-SWE-Bench: A Multilingual Benchmark for Issue Resolving

Multi-SWE-Bench is a comprehensive framework for evaluating and training Large Language Models (LLMs) on real-world software issue resolution across multiple programming languages. Unlike original Python-centric benchmarks, Multi-SWE-Bench supports **8+ languages** with high-quality curated instances.

## üåç Supported Languages

- **Java** - Enterprise-scale applications (`java_ds/`)
- **TypeScript** - Modern web development (`ts_ds/`)
- **JavaScript** - Frontend and Node.js projects (`js_ds/`)
- **Go** - Cloud-native and microservices (`go_ds/`)
- **Rust** - Systems programming (`rust_ds/`)
- **C++** - Performance-critical applications (`cpp_ds/`)
- **Python** - Data science, ML, and automation (`python_ds/`)

Each language has a dedicated dataset directory with thousands of real-world PRs from popular GitHub repositories. The framework uses GraphQL API to fetch high-quality instances with metadata including:
- Pull request details and changes
- Linked issues and discussions
- Commit messages and reviews
- Code patches and diff files

## üöÄ Key Features

- **Multilingual Support**: 8+ programming languages with dedicated datasets
  - Java, TypeScript, JavaScript, Go, Rust, C++, Python
  - Each language has specialized instance generation scripts
  
- **GraphQL-Powered Data Collection**
  - High-performance GitHub PR fetching via GraphQL API
  - Custom query support for flexible filtering
  - Language/star/date filters for targeted collection
  - Automatic issue-PR relationship resolution

- **Interactive CLI Workflow**
  - `entry.sh` provides menu-driven interface for common tasks
  - No need to memorize complex command-line arguments
  - Streamlined 4-step pipeline execution

- **Filtering & Data Processing**
  - Keyword-based filtering (supports comma-separated lists)
  - Category filtering (Bug Fix, Feature, Refactor, etc.)
  - Match mode options (any/all)
  - Batch processing for multiple datasets

- **Docker-Based Evaluation**
  - Isolated test environments for reproducible results
  - Automatic Dockerfile generation from repository configurations
  - Parallel execution support for faster evaluation

- **Training Data Extraction**
  - Format raw datasets for LLM fine-tuning
  - Structured JSON output with metadata
  - Support for patch generation training

- **Comprehensive Test Harness**
  - Automated test execution
  - Report generation with detailed metrics
  - Patch analysis with Semgrep integration

---

## üõ†Ô∏è Installation & Setup

### System Requirements

**Minimum Requirements:**
- **Docker**: Must be installed and running (required for evaluation)
  - Verify: `docker ps`
- **Python 3.10+**: Recommended version 3.11 or 3.12
  - Verify: `python --version`
- **GitHub Token**: Required for PR fetching
  - Create at: https://github.com/settings/tokens
  - Select scopes: `repo`, `read:org`

**Recommended:**
- 8GB+ RAM memory
- 50GB+ free disk space
- Stable internet connection for GitHub API access

### Installation Steps

Clone the repository and install dependencies:
```bash
git clone https://github.com/ontology-tech/multi-swe-bench.git
cd multi-swe-bench

# Install the package (creates virtual environment if needed)
make install

# (Optional) Install development dependencies
make install-dev
```

**Python Dependencies:**
- `dataclasses_json` - Data serialization
- `docker` - Docker API integration
- `tqdm` - Progress bars
- `gitpython` - Git operations
- `toml` - Configuration parsing
- `pyyaml` - YAML processing
- `PyGithub` - GitHub API client
- `unidiff` - Diff parsing
- `swe-rex` - Regular expressions for SWE tasks

**Dev Dependencies:**
- `ruff` - Fast Python linter and formatter
- `typos` - Source code spell checker
- `prettier` - Code formatter

### üéØ Quick Start
Use the interactive entry script to start common tasks:
```bash
bash entry.sh
```

The interactive menu provides options for:
1. **Fetch PRs from GitHub** using GraphQL API
2. **Filter Raw Dataset** by keywords and categories
3. **Build Dataset by PRs** for environment setup
4. **Extract Training Data** for fine-tuning

---

## üìã Full Execution Pipeline

The pipeline follows a structured 5-step process. Detailed instructions can be found in [all_process.md](all_process.md).

### 1. Generate Raw Dataset (GraphQL-based)

Fetch PRs from GitHub using the high-performance GraphQL API:
```bash
# Basic fetch with language filter
./scripts/new_gen_raw_dataset_graphql.sh -l Rust -s 10000 -n 20 -o ./data/raw_datasets/rust_data

# With additional filters
./scripts/new_gen_raw_dataset_graphql.sh \
  -l TypeScript \
  -s 5000 \
  -n 50 \
  -o ./data/raw_datasets/ts_data \
  -m 2025-01-01 \
  -k "bug fix"

# Use custom query for advanced filtering
./scripts/new_gen_raw_dataset_graphql.sh \
  -q "language:go stars:>1000" \
  -o ./data/raw_datasets/go_custom \
  -n 100

# Collect and categorize data
./scripts/collect_raw_dataset.sh
```

**Parameters:**
- `-l`: Programming language (Python, Rust, Java, TypeScript, etc.)
- `-s`: Minimum star count (default: 10000)
- `-n`: Maximum number of repos to fetch
- `-o`: Output directory (required)
- `-m`: "Merged after" date in ISO format (e.g., 2025-01-01)
- `-k`: Keywords to append to search query
- `-q`: Custom GraphQL search query (overrides -l, -s, -k)
- `-t`: GitHub token path (default: ./tokens.txt)

### 2. (Optional) Filter & Refine Data

Filter datasets by keywords, categories, and match modes:
```bash
# Filter by keywords
./scripts/filter_raw_dataset.sh \
  -i ./data/raw_datasets/rust_data \
  -o ./data/filtered/rust_bugfix \
  -k "bug,fix,repair"

# Filter by categories (Bug Fix, Feature, Refactor, etc.)
./scripts/filter_raw_dataset.sh \
  -i ./data/raw_datasets \
  -o ./data/filtered/features \
  -c "Feature,Enhancement"

# Combine filters with match mode
./scripts/filter_raw_dataset.sh \
  -i ./data/raw_datasets \
  -o ./data/filtered/strict \
  -k "memory,performance" \
  -m all
```

**Filter Modes:**
- `any`: Match any of the criteria (default)
- `all`: Match all criteria

### 3. Build Dataset & Environment

Generate Dockerfiles and environment scripts for evaluation:
```bash
# Process single dataset file
./scripts/unify_repo_scripts.sh ./data/raw_datasets/rust_raw_dataset.jsonl

# Process entire directory
./scripts/unify_repo_scripts.sh ./data/raw_datasets/

# Test mode (dry run)
./scripts/unify_repo_scripts_test.sh ./data/raw_datasets/test_raw_dataset.jsonl
```

This step creates:
- Repositories in `multi-swe-bench/harness/repos/`
- Test instances and evaluation configs
- Dockerfiles for isolated testing

### 4. Generate Repair Patches

Use agents like **SWE-Agent** or tools like **Massgen** to generate fixes:
```bash
# Generate patches using SWE-Agent
./scripts/run_patch.sh ./data/raw_datasets/rust_raw_dataset.jsonl

# Or use Massgen for bulk generation
./scripts/run_massgen_for_jsonl.sh ./data/raw_datasets/ts_data.jsonl

# Batch patch analysis
./scripts/batch_patch_analysis.sh ./data/patches/
```

### 5. Run Evaluation

Execute the final benchmark evaluation:
```bash
# Full pipeline with evaluation
./scripts/run_full_pipeline.sh ./data/raw_datasets/rust_raw_dataset.jsonl

# Run evaluation only
./scripts/run_evaluation.sh ./data/evaluation_results/

# Generate reports
./scripts/analyze_patch.sh ./data/evaluation_results/semgrep_output.jsonl
```

---

## üß≠ Common Commands Summary

| Task | Command | Description |
|------|---------|-------------|
| **Fetch PRs** | `./scripts/new_gen_raw_dataset_graphql.sh -l [Lang] -s [Stars] -n [N] -o [Dir]` | GraphQL API for fetching PRs |
| **Custom Query** | `./scripts/new_gen_raw_dataset_graphql.sh -q "[query]" -o [Dir] -n [N]` | Custom GitHub search query |
| **Filter Data** | `./scripts/filter_raw_dataset.sh -i [Input] -o [Output] -k [Keywords]` | Filter by keywords/categories |
| **Build Dataset** | `./scripts/unify_repo_scripts.sh [Raw_Dataset]` | Generate test environments |
| **Collect Data** | `./scripts/collect_raw_dataset.sh` | Complete and categorize raw data |
| **Copy Datasets** | `./scripts/copy_raw_dataset.sh [Source] [Target]` | Merge datasets |
| **Extract Training** | `./scripts/extract_training_data.sh [Input] [Output_JSON]` | Format for fine-tuning |
| **Run Patches** | `./scripts/run_patch.sh [Raw_Dataset]` | Generate repair patches |
| **Evaluate** | `./scripts/run_full_pipeline.sh [Raw_Dataset]` | Full evaluation pipeline |
| **Analyze Patches** | `./scripts/analyze_patch.sh [Semgrep_Output]` | Quality check on patches |
| **Format Code** | `make format` | Format Python files |
| **Lint Code** | `make lint` | Run ruff linter |
| **Fix Linting** | `make fix` | Auto-fix linting issues |
| **Clean Cache** | `make clean` | Remove Python cache files |

---

## üìÇ Project Structure

### Core Components

```
multi-swe-bench/
‚îú‚îÄ‚îÄ entry.sh                         # Interactive menu for common operations
‚îú‚îÄ‚îÄ Makefile                         # Build and development commands
‚îú‚îÄ‚îÄ setup.py                         # Package configuration
‚îú‚îÄ‚îÄ config.json                      # Main configuration file
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # High-level entry scripts (29+ files)
‚îÇ   ‚îú‚îÄ‚îÄ new_gen_raw_dataset_graphql.sh      # GraphQL API PR fetching
‚îÇ   ‚îú‚îÄ‚îÄ filter_raw_dataset.sh                 # Dataset filtering
‚îÇ   ‚îú‚îÄ‚îÄ unify_repo_scripts.sh                 # Build evaluation environments
‚îÇ   ‚îú‚îÄ‚îÄ run_full_pipeline.sh                  # Complete pipeline runner
‚îÇ   ‚îú‚îÄ‚îÄ extract_training_data.sh              # Training data extraction
‚îÇ   ‚îú‚îÄ‚îÄ run_evaluation.sh                     # Evaluation runner
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ data_pipeline/                   # Data processing workers
‚îÇ   ‚îú‚îÄ‚îÄ Python Scripts:
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ new_fetch_prs_graphql.py         # GraphQL PR fetching logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetch_github_repo_gql.py         # GitHub repository fetching
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filter_prs.py                     # PR filtering utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filter_repo.py                    # Repository filtering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_dataset.py                  # Dataset building (in harness/)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_related_issues.py             # Issue-PR relationship
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ merge_prs_with_issues.py          # Data merging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ util.py                          # General utilities
‚îÇ   ‚îî‚îÄ‚îÄ Shell Scripts:
‚îÇ       ‚îú‚îÄ‚îÄ gen_instance_from_dataset_*.sh   # Language-specific instance generation
‚îÇ       ‚îú‚îÄ‚îÄ gen_all_raw_datasets_new.sh      # Batch dataset generation
‚îÇ       ‚îú‚îÄ‚îÄ create_org_dir.sh                # Organization directory setup
‚îÇ       ‚îî‚îÄ‚îÄ run_*.sh                         # Various runner scripts
‚îÇ
‚îú‚îÄ‚îÄ multi_swe_bench/                  # Core Python package
‚îÇ   ‚îú‚îÄ‚îÄ collect/                         # Data collection components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ new_fetch_prs_graphql.py     # Main GraphQL fetcher
‚îÇ   ‚îú‚îÄ‚îÄ harness/                         # Evaluation harness
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_dataset.py             # Build benchmark instances
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gen_report.py                # Generate evaluation reports
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_evaluation.py            # Execute tests in Docker
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.py                   # Dataset models and utilities
‚îÇ   ‚îî‚îÄ‚îÄ utils/                           # Shared utilities
‚îÇ       ‚îú‚îÄ‚îÄ args_util.py                 # Argument parsing
‚îÇ       ‚îú‚îÄ‚îÄ docker_util.py               # Docker operations
‚îÇ       ‚îú‚îÄ‚îÄ env_to_dockerfile.py         # Environment to Dockerfile conversion
‚îÇ       ‚îú‚îÄ‚îÄ git_util.py                  # Git operations
‚îÇ       ‚îú‚îÄ‚îÄ session_util.py              # Session management
‚îÇ       ‚îî‚îÄ‚îÄ logger.py                    # Logging utilities
‚îÇ
‚îú‚îÄ‚îÄ data/                            # Generated data directory
‚îÇ   ‚îú‚îÄ‚îÄ raw_datasets/                # Collected GitHub PR data (JSONL format)
‚îÇ   ‚îú‚îÄ‚îÄ datasets/                    # Processed benchmark instances
‚îÇ   ‚îú‚îÄ‚îÄ repos/                       # Cloned GitHub repositories
‚îÇ   ‚îú‚îÄ‚îÄ patches/                     # Generated repair patches
‚îÇ   ‚îú‚îÄ‚îÄ logs/                        # Execution and evaluation logs
‚îÇ   ‚îî‚îÄ‚îÄ workdir/                     # Temporary working directories
‚îÇ
‚îú‚îÄ‚îÄ tests/                           # Unit and integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_pr_details.py
‚îÇ   ‚îú‚îÄ‚îÄ test_graphql_query.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ docs/                            # Additional documentation
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ Documentation Files:
    ‚îú‚îÄ‚îÄ all_process.md               # Complete pipeline workflow
    ‚îú‚îÄ‚îÄ Multi-SWE-Bench_Full_Guide.md # Comprehensive user guide
    ‚îú‚îÄ‚îÄ gen_dataset.md               # Dataset generation instructions
    ‚îú‚îÄ‚îÄ run_all_pipeline.md         # Pipeline execution guide
    ‚îî‚îÄ‚îÄ sh_functions.md             # Shell function reference
```


## üìú License

This project is licensed under the Apache License 2.0.

## üìñ Additional Documentation

| Document | Description |
|----------|-------------|
| [all_process.md](all_process.md) | Detailed step-by-step pipeline documentation |
| [Multi-SWE-Bench_Full_Guide.md](Multi-SWE-Bench_Full_Guide.md) | Comprehensive user guide |
| [gen_dataset.md](gen_dataset.md) | Dataset generation guide |
| [Makefile commands](#common-commands-summary) | Development and build commands |

## ‚öôÔ∏è Configuration

### GitHub Token

Create a GitHub personal access token and save it to `tokens.txt`:
```bash
echo "your_github_token_here" > tokens.txt
chmod 600 tokens.txt
```

### config.json

The main configuration file (`config.json`) contains settings for:
- Model configs (API endpoints, model names)
- Evaluation parameters
- Data collection settings

## üêõ Troubleshooting

### Docker Issues
- Ensure Docker daemon is running: `docker ps`
- Check disk space: `docker system df`

### GitHub Rate Limits
- For large fetches (>100 repos), consider using multiple tokens
- GraphQL has higher limits than REST API

### Memory Issues
- Limit concurrent jobs in `run_full_pipeline.sh`
- Use `filter_raw_dataset.sh` to reduce dataset size

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:
1. Format code: `make format`
2. Run linter: `make lint`
3. Add tests for new features
4. Update documentation
