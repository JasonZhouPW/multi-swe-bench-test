# Multi-SWE-Bench å…¨æµç¨‹æ‰§è¡Œæ¸…å•

ä¸‹é¢ä¸ºç»è¿‡æ•´ç†ä¸éªŒè¯çš„"æ­£ç¡®ã€å¯å¤ç°"çš„å…¨æµç¨‹ï¼ˆå…­æ­¥ï¼‰ï¼ŒåŒ…å«å¿…è¦å‰ç½®æ¡ä»¶ã€æ¯æ­¥å‘½ä»¤ã€è¾“å‡ºæ ¡éªŒä¸å¸¸è§æ’æŸ¥å»ºè®®ã€‚

---

## å‰ç½®æ¡ä»¶ ğŸ”§

- å®‰è£… Dockerï¼ˆå¹¶ç¡®ä¿ Docker å¯è¿è¡Œï¼‰
- Python ç¯å¢ƒï¼šå»ºè®®æ‰§è¡Œ `make install`ï¼ˆæˆ– `make install-dev`ï¼‰ä»¥å®‰è£…ä¾èµ–ï¼š
  - `make install` â†’ å®‰è£… package
  - `make install-dev` â†’ å®‰è£…å¼€å‘ä¾èµ–ï¼ˆå¯é€‰ï¼‰
- å¯é€‰ï¼šé¢„ä¸‹è½½é•œåƒï¼ˆåŠ é€Ÿè¯„æµ‹ï¼‰ï¼š
  - macOS / Linux: `bash scripts/download_images.sh scripts/images_mini.txt`
- è‹¥éœ€è¦ä» GitHub æŠ“å–æ•°æ®ï¼šå‡†å¤‡ GitHub tokenï¼ˆç”¨äº `gen_raw_dataset.sh` / collect è„šæœ¬ï¼‰

---

## å…¨æµç¨‹é¡ºåºï¼ˆæŒ‰åºæ‰§è¡Œï¼‰ ğŸ“‹

> [!TIP]
> **æ–°åŠŸèƒ½**ï¼šç°åœ¨å¯ä»¥ä½¿ç”¨æ ¹ç›®å½•ä¸‹çš„ `./entry.sh` è„šæœ¬é€šè¿‡äº¤äº’å¼èœå•å¿«é€Ÿæ‰§è¡Œä»¥ä¸‹å¸¸ç”¨æ­¥éª¤ã€‚

---

### Step 1 - ç”Ÿæˆ Raw Datasetï¼ˆå¿…é¡»æœ€å…ˆæ‰§è¡Œï¼‰

**ç›®çš„**ï¼šä» GitHub æ‹‰å– PR å¹¶æ•´åˆä¸º `*_raw_dataset.jsonl`

**ä¸»è¦è„šæœ¬**ï¼š
- ç”Ÿæˆ PR æ•°æ®ï¼š
  - GraphQL è„šæœ¬ (æ›´ç¨³å®š/é«˜æ•ˆ)ï¼š`./scripts/new_gen_raw_dataset_graphql.sh -l <lang> -s <min_stars>`
  - æ‰¹é‡ç”Ÿæˆï¼š`./data_pipeline/gen_all_raw_datasets_new.sh`
- è¡¥å…¨å¹¶åˆ†ç±»ï¼š`./scripts/collect_raw_dataset.sh`

**è¾“å‡º**ï¼š`data/raw_datasets/<owner__repo>_raw_dataset.jsonl`

**æ ¡éªŒ**ï¼šç¡®è®¤ `data/raw_datasets` ä¸‹å­˜åœ¨ `*_raw_dataset.jsonl`

**ç¤ºä¾‹**ï¼š
```bash
# å•ä¸ªè¯­è¨€
./scripts/new_gen_raw_dataset_graphql.sh -l Python -s 10000 -n 20 -o ./data/raw_datasets

# æ‰¹é‡ç”Ÿæˆæ‰€æœ‰è¯­è¨€
./data_pipeline/gen_all_raw_datasets_new.sh

# å®Œæ•´æµç¨‹
./scripts/collect_raw_dataset.sh
```

---

### Step 1.1 - æ•°æ®è¿‡æ»¤ä¸ç²¾ç‚¼ï¼ˆå¯é€‰ï¼‰

**ç›®çš„**ï¼šç­›é€‰ç‰¹å®šç±»åˆ«ï¼ˆå¦‚ Bug Fix, Performanceï¼‰æˆ–é™åˆ¶ patch å¤§å°

**è„šæœ¬**ï¼š`./scripts/filter_raw_dataset.sh -i <input_dir> -o <output_dir> [options]`

**ç‰¹è‰²**ï¼š
- æ”¯æŒäº¤äº’å¼èœå•ï¼ˆä¸å¸¦å‚æ•°è¿è¡Œï¼‰ã€‚
- é¢„è®¾æ¨¡å¼ï¼šNew Feature, Bug Fix, Edge Case, Performanceã€‚
- Patch è¿‡æ»¤ï¼šä½¿ç”¨ `-p <bytes>` æŒ‡å®šæœ€å°è¡¥ä¸å¤§å°ï¼ˆä»…è®¡ç®—ä»£ç éƒ¨åˆ†ï¼‰ã€‚

**è¾“å‡º**ï¼šè¿‡æ»¤åçš„ JSONL æ–‡ä»¶

**ç¤ºä¾‹**ï¼š
```bash
# äº¤äº’å¼
./scripts/filter_raw_dataset.sh

# è¿‡æ»¤ Bug Fix ç±»åˆ«
./scripts/filter_raw_dataset.sh -i ./raw_datasets -o ./filtered -c "bug,bugfix"

# è¿‡æ»¤å¹¶æœ‰æœ€å° patch å¤§å°é™åˆ¶
./scripts/filter_raw_dataset.sh -i ./raw_datasets -o ./filtered -p 1024 -pt 512
```

---

### Step 2 - Merge JSONL Files by Categoryï¼ˆæ–°å¢ï¼‰

**ç›®çš„**ï¼šå°†æŒ‰ç±»åˆ«åˆ†ç±»çš„ JSONL æ–‡ä»¶åˆå¹¶åˆ°å•ä¸ªæ–‡ä»¶ä¸­

**è„šæœ¬**ï¼š`./scripts/merge_jsonl_by_subdir.sh <directory>`

**è¾“å‡ºæ–‡ä»¶å‘½åæ ¼å¼**ï¼š`filtered_YYYYMMDD_<category>_raw_dataset.jsonl`

**åŠŸèƒ½ç‰¹æ€§**ï¼š
- è‡ªåŠ¨é€’å½’å¤„ç†å­ç›®å½•
- è¿‡æ»¤äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆmacOS extended attributesï¼‰
- éªŒè¯ JSON æ ¼å¼
- ç§»é™¤ null bytes

**ç¤ºä¾‹**ï¼š
```bash
./scripts/merge_jsonl_by_subdir.sh ./raw_datasets/filtered
# è¾“å‡ºï¼š
#   - filtered_20260204_bug-fix_raw_dataset.jsonl
#   - filtered_20260204_edge_raw_dataset.jsonl
#   - filtered_20260204_performance_raw_dataset.jsonl
#   - filtered_20260204_refactor_raw_dataset.jsonl
```

---

### Step 3 - ç”Ÿæˆ Repo Docker ä¸è„šæœ¬å¹¶æ„å»º Dataset

**ç›®çš„**ï¼šä¸ºæ¯ repo ç”Ÿæˆ `Dockerfile`ã€`prepare.sh`ã€`test.sh`ï¼Œå¹¶ç”Ÿæˆ `*_dataset.jsonl`

**ä¸»è¦è„šæœ¬**ï¼š
- è‡ªåŠ¨ç”Ÿæˆä»“åº“è„šæœ¬å¹¶æ„å»º datasetï¼ˆå•æ–‡ä»¶æˆ–ç›®å½•ï¼‰ï¼š
  - å•æ–‡ä»¶ï¼š`./scripts/unify_repo_scripts.sh data/raw_datasets/*_raw_dataset.jsonl`
  - ç›®å½•æ¨¡å¼ï¼š`./scripts/unify_repo_scripts.sh ./data/raw_datasets/filtered/bug-fix`
- æ‰¹é‡å¤„ç†ï¼š`./scripts/batch_unify_repos.sh ./data/raw_datasets/filtered`

**è¾“å‡º**ï¼š
- `multi_swe_bench/harness/repos/<lang>/<org>/<repo>/` - è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•ç¯å¢ƒ
- `data/datasets/<base>_dataset.jsonl` - æœ€ç»ˆ dataset JSONL
- `multi_swe_bench/harness/repos/<lang>/__init__.py` - è¯­è¨€æ ¹ç›®å½•è‡ªåŠ¨ç”Ÿæˆ
- `multi_swe_bench/harness/repos/<lang>/<org>/__init__.py` - org ç›®å½•è‡ªåŠ¨ç”Ÿæˆ
- `multi_swe_bench/harness/repos/__init__.py` - repos æ ¹ç›®å½•è‡ªåŠ¨æ›´æ–°

**æ ¡éªŒ**ï¼š
- ç¡®è®¤ `data/datasets/<base>_dataset.jsonl` å­˜åœ¨
- ç¡®è®¤ç›®å½•ç»“æ„å®Œæ•´

**æ–°åŠŸèƒ½**ï¼š
- æ”¯æŒ JSONL æ–‡é€’å½’æŸ¥æ‰¾ï¼ˆå¤„ç†å­ç›®å½•ä¸­çš„ `*_raw_dataset.jsonl``ï¼‰
- è‡ªåŠ¨ç”Ÿæˆå’Œç»´æŠ¤æ‰€æœ‰å±‚çº§çš„ `__init__.py` æ–‡ä»¶
- è·³è¿‡ binary æ–‡ä»¶ï¼Œé˜²æ­¢æ•°æ®æ±¡æŸ“

**ç¤ºä¾‹**ï¼š
```bash
# å¤„ç†å•ä¸ª raw dataset
./scripts/unify_repo_scripts.sh data/raw_datasets/OpenAPITools__openapi-generator_raw_dataset.jsonl

# å¤„ç†ç›®å½•ï¼ˆåŒ…å«å¤šä¸ª jsonlï¼‰
./scripts/unify_repo_scripts.sh ./data/raw_datasets/filtered/bug-fix

# æ‰¹é‡å¤„ç†å¤šä¸ªå­ç›®å½•
./scripts/batch_unify_repos.sh ./data/raw_datasets/filtered
```

---

### Step 4 - ç”Ÿæˆ Patchï¼ˆå¯ä¸ Step3 å¹¶è¡Œï¼‰

**ç›®çš„**ï¼šä½¿ç”¨ LLM æˆ– Agent å·¥å…· from raw dataset æå–å¹¶ç”Ÿæˆæœ€ç»ˆ patches JSONL

**è„šæœ¬ A (SWE-Agent)**ï¼š
```bash
./scripts/run_patch.sh data/raw_datasets/rust_raw_dataset.jsonl
```

**è„šæœ¬ B (Massgen)**ï¼š
```bash
./scripts/run_massgen_for_jsonl.sh <work-dir> <jsonl_path>
```

**è¾“å‡º**ï¼š`data/patches/<base>_patch.jsonl` æˆ–æŒ‡å®šç›®å½•ä¸‹çš„ `.patch` æ–‡ä»¶

**æ ¡éªŒ**ï¼šå»ºè®®æ‰§è¡Œ Patch Quality Checkï¼ˆè§ä¸‹æ–‡ï¼‰

---

### Step 5 - Patch Quality Checkï¼ˆè´¨é‡æ ¡éªŒï¼Œå¯é€‰ä½†å»ºè®®ï¼‰

**ç›®çš„**ï¼šåœ¨æ‰§è¡Œå®Œæ•´ Evaluation å‰ï¼Œå…ˆå¯¹ç”Ÿæˆçš„ patch è¿›è¡Œé™æ€æ‰«æä¸è¯„åˆ†

**è„šæœ¬**ï¼š
- æ‰«ææ–‡ä»¶ï¼š`./scripts/semgrep_scan.sh <patch_file> <output_json>`
- è¯„åˆ†åˆ†æï¼š`./scripts/analyze_patch.sh <semgrep_result.json>`
- æ‰¹é‡åˆ†æï¼š`./scripts/batch_patch_analysis.sh <patch_dir> [result_file]`
- æ‰¹é‡åˆ†æ (CSV)ï¼š`./scripts/batch_patch_analysis.sh <patch_dir> [result_csv]`

**è¾“å‡º**ï¼šSemgrep æ‰«æç»“æœä¸è´¨é‡è¯„åˆ†æŠ¥å‘Šï¼ˆS/A/B/C/F çº§ï¼‰

---

### Step 6 - æ‰§è¡Œ Evaluationï¼ˆå¿…é¡»ç­‰å¾… Step3 + Step4 å®Œæˆï¼‰

**è„šæœ¬**ï¼š`./scripts/run_full_pipeline.sh data/raw_datasets/<base>_raw_dataset.jsonl`

**è¦æ±‚**ï¼š
- `data/patches/<base>_patch.jsonl` å·²å­˜åœ¨
- `data/datasets/<base>_dataset.jsonl` å·²å­˜åœ¨

**å†…éƒ¨è°ƒç”¨**ï¼š
- `./data_pipeline/run_evaluation.sh`
- Python: `python -m multi_swe_bench.harness.run_evaluation --config <config.json>`

**è¾“å‡º**ï¼š`data/output/`ï¼ˆä¸­é—´ï¼‰ä¸ `data/final_output/`ï¼ˆæœ€ç»ˆæŠ¥å‘Šï¼‰

**æ ¡éªŒ**ï¼šæŸ¥çœ‹ `final_report.json` ä¸ `data/final_output/` ä¸­çš„æŠ¥å‘Šä¸æ—¥å¿—

---

### Step 7 - è®­ç»ƒæ•°æ®æå–ï¼ˆç”¨äºæ¨¡å‹å¾®è°ƒï¼‰

**ç›®çš„**ï¼šå°†å¤„ç†å¥½çš„æ•°æ®é›†è½¬æ¢ä¸º LLM è®­ç»ƒæ ¼å¼ï¼ˆJSONï¼‰

**è„šæœ¬**ï¼š`./scripts/extract_training_data.sh <input_path> <output_file>`

**åŠŸèƒ½**ï¼šæ”¯æŒå¤„ç†å•ä¸ªæ–‡ä»¶æˆ–æ•´ä¸ªç›®å½•ï¼Œè‡ªåŠ¨åˆå¹¶å¹¶è¿›è¡ŒåŒå‘è½¬æ¢ï¼ˆPR->Patch, Patch->PRï¼‰ã€‚

**è¾“å‡º**ï¼šæ ¼å¼åŒ–çš„ JSON æ–‡ä»¶ï¼Œé€‚åˆ LLM å¾®è°ƒ

---

## å¹¶è¡Œç­–ç•¥ä¸å¿«é€Ÿè¿è¡Œå»ºè®® âš¡

**Step 3 ä¸ Step 4 å¯å¹¶è¡Œ**ï¼ˆä¸¤è€…ä»…ä¾èµ– Step1 å’Œ Step 2ï¼‰ï¼š

```bash
./scripts/unify_repo_scripts.sh data/raw_datasets/*_raw_dataset.jsonl &   # Step 3
./scripts/run_patch.sh data/raw_datasets/*_raw_dataset.jsonl &           # Step 4
wait
./scripts/run_full_pipeline.sh data/raw_datasets/*_raw_dataset.jsonl      # Step 6
```

**æ‰¹å¤„ç†ç­–ç•¥**ï¼š
- ä½¿ç”¨ `./scripts/batch_unify_repos.sh` æ‰¹é‡å¤„ç†ç›®å½•
- ä½¿ç”¨ `./scripts/run_all_pipeline.sh` éå†å¤šä¸ª raw_dataset å¹¶ä¾æ¬¡æ‰§è¡Œ

---

## å¸¸è§é”™è¯¯ & è§£å†³è¦ç‚¹ âš ï¸

- **æ‰¾ä¸åˆ° `*_raw_dataset.jsonl`** â†’ æ£€æŸ¥ `./scripts/new_gen_raw_dataset_graphql.sh` / `./scripts/collect_raw_dataset.sh` æ˜¯å¦æˆåŠŸæ‰§è¡Œå¹¶å†™å…¥
- **`patch JSONL not found` æˆ– `dataset JSONL not found`** â†’ æŒ‰é¡ºåºå…ˆç”Ÿæˆ Step 3/Step 4 çš„äº§ç‰©
- **Docker build failed (code 127)** â†’ æ£€æŸ¥ `prepare.sh` æƒé™ä¸ Dockerfile æ˜¯å¦å®‰è£… `bash`; å»ºè®®åœ¨ Dockerfile ä¸­æ·»åŠ ï¼š
  ```dockerfile
    RUN chmod +x /home/prepare.sh
    RUN apk add --no-cache bash
    ```
- **JSONDecodeError / Invalid control character** â†’
  - ä½¿ç”¨ `jq -c` æ¸…æ´—/æ ¡éªŒ JSONL
  - ä½¿ç”¨ `merge_jsonl_by_subdir.sh` è‡ªåŠ¨è¿‡æ»¤ binary æ–‡ä»¶
- **Import Error: æŸä¸ªæ¨¡å—æ‰¾ä¸åˆ°** â†’
  - æ£€æŸ¥ `__init__.py` æ˜¯å¦æ­£ç¡®ç”Ÿæˆ
  - é‡æ–°è¿è¡Œ Step 3 é‡å»º `__init__.py`
- **Terminal backspace ä¸å·¥ä½œ** â†’
  - å·²åœ¨ `entry.sh` ä¸­ä¿®å¤ï¼ˆä½¿ç”¨ `read -rep` æ›¿ä»£ `read -rp`ï¼‰
  - é‡æ–°è¿è¡Œè„šæœ¬æˆ–ä½¿ç”¨äº¤äº’å¼èœå•
- **ç”Ÿæˆçš„ __init__.py ä¸­ import è¯­å¥è¢«è¿æ¥** â†’
  - å·²ä¿®å¤æ‰€æœ‰ gen è„šæœ¬ï¼Œç°åœ¨æ¯ä¸ª import åä¼šæ·»åŠ æ¢è¡Œ
  - é‡æ–°è¿è¡Œè„šæœ¬

---

## å¸¸ç”¨å¿«é€Ÿå‘½ä»¤æ±‡æ€» ğŸ§­

### äº¤äº’å¼èœå•
```bash
./entry.sh  # å¯åŠ¨äº¤äº’å¼èœå•
```

### Step 1: Fetch PRs
```bash
./scripts/new_gen_raw_dataset_graphql.sh -l Python -s 10000 -n 20 -o ./data/raw_datasets
./data_pipeline/gen_all_raw_datasets_new.sh  # æ‰¹é‡ç”Ÿæˆ
./scripts/collect_raw_dataset.sh  # å®Œæ•´æµç¨‹
```

### Step 2: Filter Data
```bash
./scripts/filter_raw_dataset.sh -i ./raw_datasets -o ./filtered -p 1024
./scripts/merge_jsonl_by_subdir.sh ./data/raw_datasets/filtered
```

### Step 3: Build Dataset
```bash
# å•æ–‡ä»¶
./scripts/unify_repo_scripts.sh data/raw_datasets/<file>_raw_dataset.jsonl

# ç›®å½•ï¼ˆé€’å½’ï¼‰
./scripts/batch_unify_repos.sh ./data/raw_datasets/filtered

# æ‰¹é‡
./data_pipeline/gen_instance_from_dataset_java.sh <temp_file>
```

### Step 4: Generate Patches
```bash
./scripts/run_patch.sh data/raw_datasets/<base>_raw_dataset.jsonl
./scripts/run_massgen_for_jsonl.sh <work-dir> <jsonl_path>
```

### Step 5: Quality Check
```bash
./scripts/semgrep_scan.sh <patch_file> <output_json>
./scripts/analyze_patch.sh <output_json>
./scripts/batch_analyze_patches.sh <patch_dir> [result_file]
```

### Step 6: Evaluation
```bash
./scripts/run_full_pipeline.sh data/raw_datasets/<base>_raw_dataset.jsonl
./scripts/run_all_pipeline.sh  # æ‰¹é‡è¿è¡Œ
```

### Step 7: Training Data
```bash
./scripts/extract_training_data.sh data/datasets output.json
```

---

## æµ‹è¯•ä¸éªŒè¯ ğŸ§ª

### éªŒè¯ Raw Data
```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -lh data/raw_datasets/*_raw_dataset.jsonl

# æ£€æŸ¥ JSON æ ¼å¼
jq -c . data/raw_datasets/*.jsonl 2>&1 | head -5
```

### éªŒè¯ Directory Structure
```bash
# æ£€æŸ¥è¯­è¨€æ ¹ç›®å½•
ls -la multi_swe_bench/harness/repos/

# æ£€æŸ¥ org ç›®å½•
ls -la multi_swe_bench/harness/repos/java/apache/

# æ£€æŸ¥ org/__init__.py
cat multi_swe_bench/harness/repos/java/openapitools/openapi-generator/__init__.py

# æ£€æŸ¥è¯­è¨€æ ¹ __init__.py
cat multi_swe_bench/harness/repos/java/__init__.py
```

### éªŒè¯ Dataset
```bash
# æ£€æŸ¥ dataset æ–‡ä»¶
ls -lh data/datasets/*.jsonl

# ç»Ÿè®¡è¡Œæ•°
wc -l data/datasets/*.jsonl
```

---

## æ–°å¢åŠŸèƒ½äº®ç‚¹ âœ¨

### 1. äº¤äº’å¼èœå•ï¼ˆentry.shï¼‰
- ç»Ÿä¸€çš„å…¥å£ç‚¹ï¼Œä¸éœ€è¦è®°å¿†å¤æ‚å‘½ä»¤
- 6 ä¸ªä¸»è¦é€‰é¡¹ï¼Œæ¶µç›–æ‰€æœ‰å¸¸ç”¨æµç¨‹
- æ”¯æŒ backspace é”®ä¿®å¤ï¼ˆLinux/macOS å…¼å®¹ï¼‰

### 2. æ‰¹é‡å¤„ç†ï¼ˆbatch_unify_repos.shï¼‰
- æ‰¹é‡å¤„ç†å¤šä¸ªå­ç›®å½•
- è‡ªåŠ¨å¤„ç†æ¯ä¸ªå­ç›®å½•çš„ `*_raw_dataset.jsonl` æ–‡ä»¶

### 3. JSONL åˆå¹¶ï¼ˆmerge_jsonl_by_subdir.shï¼‰
- æŒ‰å­ç›®å½•åˆå¹¶ JSONL æ–‡ä»¶
- è‡ªåŠ¨è¿‡æ»¤äºŒè¿›åˆ¶æ–‡ä»¶
- è‡ªåŠ¨æ·»åŠ  `_raw_dataset` åç¼€

### 4. é€’å½’å¤„ç†ï¼ˆunify_repo_scripts.shï¼‰
- æ”¯æŒé€’å½’æŸ¥æ‰¾å­ç›®å½•ä¸­çš„æ–‡ä»¶
- è‡ªåŠ¨å¤„ç†æ·±å±‚åµŒå¥—ç»“æ„

### 5. è‡ªåŠ¨ __init__.py ç®¡ç†
- è‡ªåŠ¨ç”Ÿæˆ org/__init__.py
- è‡ªåŠ¨é‡å»ºè¯­è¨€æ ¹ __init__.py
- è‡ªåŠ¨æ·»åŠ æ¢è¡Œç¬¦ï¼Œé˜²æ­¢ import è¯­å¥åˆå¹¶

### 6. å¤šæ ¼å¼ JSON æ”¯æŒï¼ˆcreate_org_dir.shï¼‰
- æ”¯æŒæ‰å¹³ç»“æ„å’ŒåµŒå¥—ç»“æ„çš„ JSON
- å‘åå…¼å®¹æ—§çš„ JSON æ ¼å¼

---

## å·¥ä½œæµå»ºè®® ğŸ’¡

**å°å‹é¡¹ç›®ï¼ˆå•ä¸ª datasetï¼‰**ï¼š
```bash
./entry.sh
# é€‰æ‹©é€‰é¡¹ 1 â†’ Fetch PRs
# é€‰æ‹©é€‰é¡¹ 2 â†’ Filter (å¯é€‰)
# é€‰æ‹©é€‰é¡¹ 3 â†’ Build Dataset
# é€‰æ‹©é€‰é¡¹ 4 â†’ Generate Patches
```

**ä¸­å‹é¡¹ç›®ï¼ˆå¤šä¸ª datasetï¼‰**ï¼š
```bash
./entry.sh
# é€‰æ‹©é€‰é¡¹ 5 â†’ Fetch All Datasets (ä¸€æ¬¡æ€§è·å–æ‰€æœ‰è¯­è¨€)
# é€‰æ‹©é€‰é¡¹ 2 â†’ Filter Data
# é€‰æ‹©é€‰é¡¹ 3 â†’ Build Dataset
# é€‰æ‹©é€‰é¡¹ 4 â†’ Generate Patches
```

**å¤§å‹é¡¹ç›®ï¼ˆåˆ†ç±»å¤„ç†ï¼‰**ï¼š
```bash
./entry.sh
# é€‰æ‹©é€‰é¡¹ 5 â†’ Fetch All Datasets
# é€‰æ‹©é€‰é¡¹ 2 â†’ Filter Data (ç”Ÿæˆå¤šä¸ª filtered ç›®å½•)
./scripts/merge_jsonl_by_subdir.sh ./data/raw_datasets/filtered
./scripts/batch_unify_repos.sh ./data/raw_datasets/filtered
# é€‰æ‹©é€‰é¡¹ 4 â†’ Generate Patches
```

---

## æ•…éšœæ’æŸ¥ ğŸ”§

### Docker ç›¸å…³
```bash
# æ£€æŸ¥ Docker çŠ¶æ€
docker ps

# æŸ¥çœ‹ Docker æ—¥å¿—
docker logs <container_id>

# æ¸…ç†æ— ç”¨çš„ Docker èµ„æº
docker system prune -a
```

### Python ç¯å¢ƒ
```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬
python --version

# é‡æ–°å®‰è£…åŒ…
make install

# æ£€æŸ¥ä¾èµ–
pip list | grep -E "docker|jq|gitpython"
```

### GitHub API
```bash
# éªŒè¯ token
curl -H "Authorization: token YOUR_TOKEN" https://api.github.com/user

# æŸ¥è¯¢é€Ÿç‡é™åˆ¶
curl -H "Authorization: token YOUR_TOKEN" https://api.github.com/rate_limit
```

---

## æ€§èƒ½ä¼˜åŒ– âš¡

- **é¢„ä¸‹è½½ Docker é•œåƒ**ï¼š`bash scripts/download_images.sh scripts/images_mini.txt`
- **å¹¶è¡Œæ‰§è¡Œ**ï¼šStep 3 å’Œ Step 4 å¯ä»¥å¹¶è¡Œ
- **æ‰¹é‡å¤„ç†**ï¼šä½¿ç”¨ batch è„šæœ¬å‡å°‘é‡å¤åŠ³åŠ¨
- **å¢é‡æ›´æ–°**ï¼šunify_repo_scripts.sh æ”¯æŒå•ç‹¬å¤„ç†æ–°çš„ raw_dataset æ–‡ä»¶
- **æ•°æ®è¿‡æ»¤**ï¼šStep 1.1 æå‰è¿‡æ»¤å¯ä»¥å‡å°‘åç»­æ­¥éª¤çš„å¤„ç†é‡

---

## æ€»ç»“ ğŸ“

å®Œæ•´æµç¨‹å…±7 æ­¥ï¼š
1. Fetch PRs from GitHub (Step 1)
2. Filter & Refine Data (Step 1.1, å¯é€‰)
3. Merge JSONL by Category (Step 2, æ–°å¢)
4. Build Dataset (Step 3)
5. Generate Patches (Step 4)
6. Patch Quality Check (Step 5, å¯é€‰)
7. Run Evaluation (Step 6)
8. Extract Training Data (Step 7, å¯é€‰)

æ¯ä¸ªè„šæœ¬éƒ½æœ‰è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œè¿›åº¦æç¤ºï¼Œé‡åˆ°é—®é¢˜æ—¶ä¼šç»™å‡ºæ˜ç¡®çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®çš„è§£å†³æ–¹æ³•ã€‚

å¼€å§‹ä½¿ç”¨å‰ï¼Œå»ºè®®å…ˆè¿è¡Œ `./entry.sh` æ¢ç´¢äº¤äº’å¼èœå•åŠŸèƒ½ï¼
